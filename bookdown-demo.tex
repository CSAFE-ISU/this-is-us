\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={This is us: making CSAFE stronger each week},
            pdfauthor={CSAFE},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{This is us: making CSAFE stronger each week}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{CSAFE}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-09-19}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{prerequisites}{%
\chapter{Prerequisites}\label{prerequisites}}

This is a \emph{sample} book written in \textbf{Markdown}. You can use anything that Pandoc's Markdown supports, e.g., a math equation \(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{# or the development version}
\CommentTok{# devtools::install_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading \texttt{\#}.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): \url{https://yihui.name/tinytex/}.

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

This section will become the section for the administrative updates/organization once we have figured out how to use all of the bookdown features for our purposes.

You can label chapter and section titles using \texttt{\{\#label\}} after them, e.g., we can reference Chapter \ref{intro}. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \ref{glass}.

Figures and tables with captions will be placed in \texttt{figure} and \texttt{table} environments, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\FloatTok{.1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(pressure, }\DataTypeTok{type =} \StringTok{'b'}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/nice-fig-1} 

}

\caption{Here is a nice figure!}\label{fig:nice-fig}
\end{figure}

Reference a figure by its code chunk label with the \texttt{fig:} prefix, e.g., see Figure \ref{fig:nice-fig}. Similarly, you can reference tables generated from \texttt{knitr::kable()}, e.g., see Table \ref{tab:nice-tab}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}
  \KeywordTok{head}\NormalTok{(iris, }\DecValTok{20}\NormalTok{), }\DataTypeTok{caption =} \StringTok{'Here is a nice table!'}\NormalTok{,}
  \DataTypeTok{booktabs =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:nice-tab}Here is a nice table!}
\centering
\begin{tabular}{rrrrl}
\toprule
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\midrule
5.1 & 3.5 & 1.4 & 0.2 & setosa\\
4.9 & 3.0 & 1.4 & 0.2 & setosa\\
4.7 & 3.2 & 1.3 & 0.2 & setosa\\
4.6 & 3.1 & 1.5 & 0.2 & setosa\\
5.0 & 3.6 & 1.4 & 0.2 & setosa\\
\addlinespace
5.4 & 3.9 & 1.7 & 0.4 & setosa\\
4.6 & 3.4 & 1.4 & 0.3 & setosa\\
5.0 & 3.4 & 1.5 & 0.2 & setosa\\
4.4 & 2.9 & 1.4 & 0.2 & setosa\\
4.9 & 3.1 & 1.5 & 0.1 & setosa\\
\addlinespace
5.4 & 3.7 & 1.5 & 0.2 & setosa\\
4.8 & 3.4 & 1.6 & 0.2 & setosa\\
4.8 & 3.0 & 1.4 & 0.1 & setosa\\
4.3 & 3.0 & 1.1 & 0.1 & setosa\\
5.8 & 4.0 & 1.2 & 0.2 & setosa\\
\addlinespace
5.7 & 4.4 & 1.5 & 0.4 & setosa\\
5.4 & 3.9 & 1.3 & 0.4 & setosa\\
5.1 & 3.5 & 1.4 & 0.3 & setosa\\
5.7 & 3.8 & 1.7 & 0.3 & setosa\\
5.1 & 3.8 & 1.5 & 0.3 & setosa\\
\bottomrule
\end{tabular}
\end{table}

You can write citations, too. For example, we are using the \textbf{bookdown} package \citep{R-bookdown} in this sample book, which was built on top of R Markdown and \textbf{knitr} \citep{xie2015}.

\hypertarget{bullets}{%
\chapter{Project CC: Bullets and Cartridge Cases}\label{bullets}}

For both bullets and cartridge cases we are dealing with several inter-related aspects, that we want to address independently.

Those are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  data collection
\item
  computational tools
\item
  similarity scores

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    for bullet lands:

    \begin{enumerate}
    \def\labelenumiii{\alph{enumiii}.}
    \tightlist
    \item
      crosscut identification
    \item
      groove location
    \item
      curvature removal
    \item
      alignment of signatures
    \item
      feature extraction
    \item
      matching with trained Random Forest
    \end{enumerate}
  \item
    for breech faces
  \end{enumerate}
\item
  analysis of results
\item
  communication of results and methods
\end{enumerate}

\hypertarget{data-collection}{%
\section{Data Collection}\label{data-collection}}

\hypertarget{lapd}{%
\subsection{LAPD}\label{lapd}}

All bullets are collected by Srinivasan Rathinam, LAPD.

\hypertarget{main-study}{%
\subsubsection{Main study}\label{main-study}}

4 bullets per barrel for 626 Beretta 92 F/FS firearms , ammunition used are 9 mm Luger Winchester 115 grain with a Copper surface.

scans are on Raven.

evaluation: Yawei is going to work through all 626 barrels of knowns to assess similarity scores

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/yawei/results-FAU-1} 

}

\caption{Results from assessing scans of barrel FAU 1 similarity.}\label{fig:unnamed-chunk-3}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/yawei/results-FAU-2} 

}

\caption{Results from assessing scans of barrel FAU 2 similarity.}\label{fig:unnamed-chunk-4}
\end{figure}

Why some of the cases failed? (181/626 = 30\%)

\texttt{x3p\_crosscut\_optimize()} failed to find the positions to get cross cut for some lands.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/yawei/lapd-FAU-3-Bullet-A-land-6} 

}

\caption{Land scan for barrel FAU 3 bullet A land 6.}\label{fig:unnamed-chunk-5}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/yawei/lapd-FAU-4-Bullet-C-land-5} 

}

\caption{Land scan for barrel FAU 4 bullet C land 5.}\label{fig:unnamed-chunk-6}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/yawei/lapd-FAU-5-Bullet-B-land-5} 

}

\caption{Land scan for barrel FAU 5 bullet B land 5.}\label{fig:unnamed-chunk-7}
\end{figure}

Manual indentification of grooves now\ldots{}

\hypertarget{follow-up-study}{%
\subsubsection{follow-up study}\label{follow-up-study}}

4 bullets per barrel for 96 of the original 626 Beretta firearms using different ammunition

bullets are being scanned

\hypertarget{hamby-sets}{%
\subsection{Hamby Sets}\label{hamby-sets}}

Scans for Hamby Sets 10, 36, 44, and 224

Scans for 3 replicates of clones for Hamby 224

\hypertarget{houston-tests}{%
\subsection{Houston Tests}\label{houston-tests}}

contact: Melissa Nally, Houston FSI

\hypertarget{pre-study}{%
\subsubsection{Pre-study}\label{pre-study}}

3 kits with 23 bullets each

\begin{figure}

{\centering \includegraphics[width=58.11in]{images/bullets/houston-pre-set3} 

}

\caption{Bullet-to-bullet similarity scores for questioned bullets (y-axis) compared to all other bullets of the test set (x-axis).}\label{fig:unnamed-chunk-8}
\end{figure}

evaluation included in submission to JFI

\hypertarget{study}{%
\subsubsection{Study}\label{study}}

4 kits with 20 bullets each

scans done, evaluation finished, some scans of doubtful quality

\hypertarget{houston-persistence}{%
\subsection{Houston Persistence}\label{houston-persistence}}

contact: Melissa Nally, Houston FSI

8 barrels with 40 fired bullets each

\hypertarget{st-louis-persistence}{%
\subsection{St Louis persistence}\label{st-louis-persistence}}

contact: Steve Kramer, St Louis PD

2 barrels with 192 fired bullets each (2 bullets collected every 25 shots)

\hypertarget{dfsc-cartridge-cases}{%
\subsection{DFSC Cartridge cases}\label{dfsc-cartridge-cases}}

Breech face data for knowns are scanned and available on a private github repository

evaluation

\hypertarget{computational-tools}{%
\section{Computational Tools}\label{computational-tools}}

\hypertarget{x3ptools}{%
\subsection{x3ptools}\label{x3ptools}}

\texttt{x3ptools} is an R package for working with files in x3p format. x3p is an ISO standard for describing 3d topographic surface measurements.
\texttt{x3ptools} is available on CRAN, i.e.~can be installed with the command \texttt{install.packages("x3ptools")}. The development version is available from github. Installation instructions and basic usage can be found at \url{https://heike.github.io/x3ptools/}

\hypertarget{bulletxtrctr}{%
\subsection{bulletxtrctr}\label{bulletxtrctr}}

\texttt{bulletxtrctr} is a developmental R package available from github (see \url{https://heike.github.io/bulletxtrctr/}) that allows an assessment of similarity scores using the data extraction pipeline described in \citet{aoas}.

\hypertarget{groovefinder}{%
\subsection{grooveFinder}\label{groovefinder}}

\texttt{grooveFinder} is a developmental R package providing different methods for identifying the location of grooves in scans of bullets.
Installation instructions and some basic usage can be found at \url{https://heike.github.io/grooveFinder/}

\hypertarget{similarity-scores}{%
\section{Similarity Scores}\label{similarity-scores}}

\hypertarget{bullet-lands}{%
\subsection{Bullet Lands}\label{bullet-lands}}

\hypertarget{approaches-to-identify-groove-locations}{%
\subsubsection{Approaches to identify groove locations}\label{approaches-to-identify-groove-locations}}

\hypertarget{hough-transform-method-for-identifying-grooves}{%
\paragraph{Hough Transform Method for Identifying Grooves}\label{hough-transform-method-for-identifying-grooves}}

Charlotte 9/5/19 Update:

Will fill in with more detail later

\textbf{Current Goals}:
- Iron-out issues with consistency of units with \texttt{get\_hough\_grooves}. I believe there are some issues translating from the 2-d visualization to the 3-d visualization that might have to do with inconsistent unit inputs? For Example

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/Hough_project/br411_2d} 

}

\caption{2-dimensional visualization of example bullet br411 with .999 strength threshold}\label{fig:unnamed-chunk-9}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/Hough_project/br411_3d} 

}

\caption{3-dimensional visualization of example bullet br411 with .999 strength threshold}\label{fig:unnamed-chunk-10}
\end{figure}

So either somethin is wrong with \texttt{get\_mask\_hough} or something is funky with the units.

\begin{itemize}
\item
  Also need to think of including a sort of rounding component where lines with slopes that are practically infinite can be viewed as a vertical line
\item
  Compare Hough results with manual identification using score calculations from Kiegan.
\item
  Write up results in Hough Groove Paper (It's coming I promise)

  \begin{itemize}
  \tightlist
  \item
    Create graphical images to explain line selection method
  \item
    Include 2-d and 3-d visualizations of Hough groove area identifications
  \item
    Include crosscut visualization and comparison in results
  \end{itemize}
\end{itemize}

Charlotte update 09/12/19:
This week I have been working on obtaining some results for the Phoenix set on Sunny.
As a minor update the unit issues in \texttt{get\_mask\_hough()} are resolved ( I think). Below
is an example of a nice image that has been generated using masks.

\begin{figure}

{\centering \includegraphics[width=21.53in]{images/bullets/Hough_project/mask_phoenix_nice} 

}

\caption{ Phoenix Gun1 A-9 B1 Land 4 generated at strength threshold of 0.99, initially did not generate estimates at the 0.999 or 0.995 level}\label{fig:unnamed-chunk-11}
\end{figure}

However the mask is only as good as the Hough estimates that supports it as shown here (less nice).

\begin{figure}

{\centering \includegraphics[width=17.03in]{images/bullets/Hough_project/mask_phonix_lessnice} 

}

\caption{ Phoenix Gun1 F-6 B2 Land 5 generated at strength threshold of 0.9, initially did not generate estimates at the 0.999 or 0.995, or 0.99 level}\label{fig:unnamed-chunk-12}
\end{figure}

Hough crosscut predictions for the Phoenix dataset are now uploaded to the bulletQuality Github in the``results'' folder and contains Hough groove estimates at the following five strength levels: 0.999, 0.995, 0.99, 0.95, 0.9. The source and the crosscut estimate are also included in the dataset.

Here are some preliminary results of using Kiegan's area of misidentification method
(thanks Kiegan!) on Hough groove estimates at the strength threshold of 0.999
in comparison to the BCP and Lasso method.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/Hough_project/preliminary_phoenix_score_results_left} 

}

\caption{Left-hand groove area of misidentification log-transformed scores for BCP, Lasso, and Hough}\label{fig:unnamed-chunk-13}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/Hough_project/preliminary_phoenix_score_results_right} 

}

\caption{Right-hand groove area of misidentification log-transformed scores for BCP, Lasso, and Hough}\label{fig:unnamed-chunk-14}
\end{figure}

These scoresare log transformed to show better separation but it's very clear that for the
left groove both Lasso and BCP are out performing the Hough method in correctly identifying grooves. For the righthand side, scores tend to be more similar however once again,
the Lasso method seems to bo the best job since it has a larger density of low scores
and minimizes high score misidenfitications.

For improvement before next week, I will investigate why there are 47 missing Hough
predictions resulting in a score of 0 in these results and change the parameters in the
\texttt{get\_grooves\_hough()} function to try and generate estimates for some of those missing values.

Charlotte update 09/19/2019:

This week we are trying to think of a new way for selecting Hough lines for bullet estimates. The previous method for selecting Hough lines was to find lines with x-intercepts at the top and bottom of the lands closest to the lower and upper one sixth of the bullet lands. However this process was highly dependent on score thresholding from the Hough transform which is frustrating when running a large number of bullets since if the right score threshold was not achieved, no result would be produced. So right now I'm working on a way of selecting Hough lines from the normalized Hough scores.

To obtain a normalized Hough score I take the x-intercepts of each estimated Hough line generate and find the distance between the x-intercept at the top and the bottom of the land. This should give me the max possible score for each Hough line, rather than calculating based on theta. Then I take the Hough score and divide by this maximum to normalize scores between 0 and 1. Right now I am working on visualizing some of these results but my code is buggy because I'm getting negative values when I try to visualize the process using masks when I shouldn't. Here is an example of a bullet land using the old and new method. Really similar results although it would appear that the new resut places the Hough transform lines further in to interior of the land than the old results. So that's promising?

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/Hough_project/phoenix_current_hough_land1} 

}

\caption{Phoenix Gun 1-A9 Bullet 3 Land 1 visualized using current Hough process message}\label{fig:unnamed-chunk-15}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/Hough_project/phoenix_new_hough_land1} 

}

\caption{Phoenix Gun 1-A9 Bullet 3 Land 1 visualized using new Hough process message}\label{fig:unnamed-chunk-16}
\end{figure}

\hypertarget{lasso-method}{%
\paragraph{LASSO Method}\label{lasso-method}}

A paper is in preparation for submission to Forensic Science International describing this method (\texttt{get\_grooves\_lassofull} in \texttt{grooveFinder}), as well as the Bayesian changepoint method (\texttt{get\_grooves\_bcp}).

\hypertarget{robust-loess-method}{%
\paragraph{Robust LOESS Method}\label{robust-loess-method}}

A paper submitted to the Journal of Forensic Science is waiting for peer review response to the first round of revisions.

\hypertarget{bullet-land-comparisons-pipeline}{%
\subsubsection{Bullet Land Comparisons Pipeline}\label{bullet-land-comparisons-pipeline}}

Most data analysis processes can be thought of as a data analysis ``pipeline''. This process can involve data collection, decisions about data cleaning, data transformation or reduction, and feature engineering. For example, consider the general process below:

In the case of the bullet project, we have a pipeline which starts with having two physical bullet LEAs and ends with a quantitative result, a random forest similarity score. Our pipeline could be described (roughly) as something like this:

To make this a little easier to see, we can look at how a 3D scan is processed into a 2D signature:

Now, something important to consider is whether each of these ``data decisions'' has an impact on the quantitative result (here, a similarity score between two LEA signatures). Consider a simple set of decisions we could make in our bullet pipeline:

If we have a pair of signatures, we could theoretically end up with 16 different similarity scores depending on the decisions we make at each point. That is also assuming that both signatures were processed in the same way at each point.

This year, I'll be studying our bullet land ``pipeline'' here at CSAFE, as well as pipelines that are a little different than ours (e.g., \citet{chu_jfs}). There are a few major goals I am working towards:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Quantifying the uncertainty of our RF similarity scores based on data decisions\\
\item
  Comparing reproducibility/robustness of differing bullet analysis approaches

  \begin{itemize}
  \tightlist
  \item
    \citet{aoas} vs. \citet{chu_jfs}, for example
  \item
    Crosscuts: method 1 vs.~alternate? Crosscut parameter tuning?
  \item
    Groove methods\\
  \item
    Original RF vs.~updated/retrained/re-engineering
  \end{itemize}
\item
  Reproducibility/robustness of different approaches when we consider data COLLECTION.
\end{enumerate}

Goal 3 is a major part of this pipeline process which I have been working on since the spring! We designed and collected a bullet scanning variability study of 9 bullets. I'm working on formally modeling the variability at the signature level, taking two major approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Subsampling and assuming independence;
\item
  Directly modeling out the mean structure

  \begin{itemize}
  \tightlist
  \item
    Ignoring peak/valley dependence
  \item
    Using time series/spatial dependence modeling
  \item
    Using a Bayesian shrinkage prior (w/help from Amy!)
  \end{itemize}
\end{enumerate}

Results for Method 1, the subsampling, looks something like this:

I will be updating with more on the ``direct'' modeling in my next Spotlight!

\hypertarget{cartridge-cases}{%
\subsection{Cartridge Cases}\label{cartridge-cases}}

\hypertarget{congruent-matching-cells-cmc-algorithm-for-comparing-cartridge-case-breech-face-impressions}{%
\subsubsection{Congruent Matching Cells (CMC) algorithm for comparing cartridge case breech face impressions}\label{congruent-matching-cells-cmc-algorithm-for-comparing-cartridge-case-breech-face-impressions}}

Joe 9/5/19 Update: Dealing with missing values in the x3p scans continues to be an issue. The Fast Fourier Transform method for calculating cross-correlation can't handle missing data in an image, so we've attempted a few ``fixes'' that haven't necessarily turned out as well as expected. One idea we had was to replace the NA values in a cell with the average pixel value. However, this is artificially introducing a signal where before there was none. This can (and demonstrably has) led to inflated/incorrect correlations between cells that shouldn't have much at all in common. Unfortunately, this may be the only solution if we still wish to adhere to the CMC algorithm as described in Song et al. (2015). One improvement that I've implemented is to ``crop out'' the rows and columns of an image that only contain NAs. This at least means that we've weakened the strength of the artificial signal relative to the breechface's signal.

Below is a series of images that illustrate how we might compare a cell in one image to a region of another image.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_im2_cellComparison} 

}

\caption{Comparing a cell in image 1 to a larger region in image 2. We wish to find the translations of the image 1 cell that yield the highest correlation within the image 2 region.}\label{fig:unnamed-chunk-17}
\end{figure}

For the sake of an example, let's focus on the blue outlined cell in image 1. Our goal is to use the image 1 cell to ``search'' a corresponding larger region in image 2 for the horizontal/vertical translations needed to produce the highest correlation. Below is a zoomed-in version of the blue outlined image 1 cell on the left and the larger image 2 region (approximately: I made the gridded image above by-hand outside of R while the images below are from R). The image 1 cell may look larger than the image 2 region, but we can see from the axes that the image 2 region is indeed larger. Any white pixels in the two images are NA values that need to be dealt with in some way before we can use FFTs to calculate the cross-correlation.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_split} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im2_split} 

}

\caption{(Left) A cell from image 1. (Right) A region from image 2 centered in the same location as the image 1 cell, yet quadruple the area.}\label{fig:unnamed-chunk-18}
\end{figure}

As already discussed above, one ``solution'' is to replace the NA values with the average pixel value of each image. However, to avoid creating a stronger artificial signal than necessary, we can crop-out the NA rows and columns from the two images above. Below is the cropped version of the two images. The cropping doesn't produce signficantly different images in this case, but you could imagine other examples in which a cell has captured only small amount of breechface in the corner. Such examples are fairly common and cropping signficantly changes the resulting correlation values.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_splitFilteredCropped} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im2_splitFilteredCropped} 

}

\caption{The same images as above after cropping NA rows/columns.}\label{fig:unnamed-chunk-19}
\end{figure}

The last step before calculating correlation for these cells is to replace the remaining NAs with the average pixel value. This is shown below.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_splitShifted} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im2_splitShifted} 

}

\caption{The NA-cropped images with remaining NAs replaced with the image's average pixel values.}\label{fig:unnamed-chunk-20}
\end{figure}

The cross-correlation is then calculated between these two images via a standard fast fourier transform process (see \href{http://mathworld.wolfram.com/Cross-CorrelationTheorem.html}{Cross-Correlation Theorem}). The benefit of using such a process is that (as the name suggests) it's faster than calculating the raw correlation between the two images. Also, the translations that produce the highest correlation between the image 1 cell and the image 2 region fall out of the calculation for free.

This pre-processing/cross-correlation calculation procedure is repeated for every cell in image 1 that contains breech face impression. Because it is not valid to assume that the two images are rotationally aligned by default, we perform the same procedure repeatedly while rotating image 2. Currently, we perform a ``rough'' grid search of \(\theta \in [-177.5,180]\) by increments of \(2.5^{\circ}\). Theoretically, the final results tell us how we need to horizontally/vertically translate and rotate the two images to be correctly aligned.

\hypertarget{congruent-matching-tori-a-promising-solution-to-the-missing-value-problem}{%
\subsubsection{Congruent Matching Tori: a promising solution to the missing value problem}\label{congruent-matching-tori-a-promising-solution-to-the-missing-value-problem}}

As discussed above, dealing with missing values is provign to be a pain. The good news is that the currently-implemented CMC as described above yields results very similar to those published in Song et al. (2015) that originally describes that CMC algorithm. While our results seem to agree with currently published results, it would be nice if we could avoid needing to artifically replace missing values. We can do so if, rather than breaking up the circular breech face impression scans into disjoint squares, we break up the breech face impression into donut-shaped regions containing only breech face impression. Below is an example of such a toroidal region.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_original} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_toroidalRegion} 

}

\caption{(Left) The original breech face impression scan image. (Right) A donut-shaped region cut out of the original image.}\label{fig:unnamed-chunk-21}
\end{figure}

By comparing such regions instead of the square cells, we would presumably only need to fill in a few missing value ``holes'' in the breech face impression scan rather than completely replacing a non-existent signal with an artificial one. In the near-future, I hope to finish up the pre-processing needed for this Congruent Matching Tori method by performing a polar transformation on these images to make them into strips that can easily be compared via an FFT.

Joe 9/12/19 Update: Before carving out toroidal regions from the two images we wish to compare, a fair amount of pre-processing needs to be completed. For example, the scans we work with begin with a considerable amount of auxiliary information, for example the firing pin impression, that we don't want to use in our comparisons. This isn't to say that firing pin impressions aren't useful to determine a match between two cartridge cases. In fact there is quite a lot of published research on how to compare two firing pin impressions. Rather, it is common practice to compare breech face impressions and firing pin impressions separately since it is difficult to scan both simultaneously. Thus, there are regions of a breech face impression scan that we want to remove so that the breech face impressions are more easily comparable. Below is an example of two breech face impression scans before processing.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_fullScan} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im2_fullScan} 

}

\caption{Two cartridge case scans before pre-processing.}\label{fig:unnamed-chunk-22}
\end{figure}

There are a variety of techniques to segment an image into various parts. In image processing, common techniques are the Canny edge detector, which identifies edges of shapes in an image using image gradient techniques, and the Hough Transform, which can detect a variety of geometrical shapes in an image. The Hough Transform is what is used to segment the cartridge case images used in the previous section. However, we've found that the use of a Hough Transform doesn't extract the ``breech face signal'' from an image as other techniques. Namely, the breech face can be effectively extracted using the RANSAC (Random sample consensus) method that iteratively fits a plane to a set of data until it settles upon a consensus-based ``bulk'' of the data. In the case of these cartridge case scans, the bulk of the data should predominantely be distributed around the mode height value. That is, the breech face impression. Once we've fit this plane to the breech face impression, we can extract the residuals of the fit to better accentuate the markings left in the cartridge case base by a firearm's breech face. Below is an example of the residuals left after fitting a RANSAC plane to two cartridge case scans above. In the example below, we grab any residuals less than 20 microns in magnitude.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_ransacResiduals} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im2_ransacResiduals} 

}

\caption{Residual values of a RANSAC plane fit to the two cartridge case scans shown above.}\label{fig:unnamed-chunk-23}
\end{figure}

Although these two images are of two different cartridge cases, you can hopefully see that one looks very much like a rotated version of the other. These two cartridge case scans are in fact fired from the same gun (known matches), so it's a good thing that they look so similar. We've now removed quite a bit of the unwanted regions of the original scans. However, there are still some areas of the image (e.g., the faint circular region of pixels in the center of the breech face scan) that just so happened to be close to the fitted plane and thus were brought along in the residual extraction. There are a few ways that we can clean up these last few areas. One is to use two Hough Transforms to detect the inner and outer circles of the breech face impression and filter out any pixels outside of the region between these two circles. The biggest issue with using a Hough Transform is that it must be given the radius of the circle that it is to search for in the image as an argument. That is, we need to know the radius of the breech face impression that we haven't yet identified in order to identify the breech face impression. Instead, we can dilate/erode (or vice-versa) the pixels in the image to remove the remaining ``speckle'' in the image. Below is an example of of the breech face impressions cleaned via a dilation/erosion procedure.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im1_maskFiltered} \includegraphics[width=0.5\linewidth]{images/cartridge_cases/im2_maskFiltered} 

}

\caption{The selected breech face impressions based on dilation and erosion.}\label{fig:unnamed-chunk-24}
\end{figure}

The final step in the pre-processing is to align the two images in some consistent fashion. Luckily, the firing pin impression ring that's left after performing the above dilation/erosion provides us with some idea of how to align the breech face impressions. The location of the firing ring impression in the breech face impression provides us with an indicator of where the cartridge case was located relative to the firing pin when it was sitting in the barrel. So aligning two cartridge cases so that their firing pin impression rings align will ensure that, at the very least, the breech face impression left on the cartridge case is horizontally/vertically aligned if not rotationally aligned.

Joe 9/18/19 Update: To automatically detect the radius of a given breech face impression, we can count the number of non-NA pixels in each row. If we were to imagine scanning down an image and counting the number of non-NA pixels in each row, then this count would obviously start to increase the moment we hit the top of the breech face impression. Because the breech face impressions are circular, the count would continue to increase the further down the image we scan. That is, until we hit the firing pin impression circle. At this point, because the firing pin impression circle consists of NAs, we would expect the non-NA pixel count to dip. This increasing followed by decreasing behavior in the non-NA pixel count constitutes a local maximum. We can use this local maximum of the non-NA pixel count to identify the beginning of the firing pin impression circle. Similarly, we would expect the non-NA pixel count to reach another local maximum once we hit the end of the firing pin impression circle. It's then a simple subtraction of the two row indices containing these local maxima to determine an estimate for the diameter of the firing pin impression circle.

We can see below an example of the non-NA pixel row sums plotted against the row indices (starting from the top of the image and moving down). You can hopefully see that the raw row sums are rather ``noisy''. As such, we can pass a moving average smoother over the row sum values so that the local maxima are easier to identify. This may not be the most robust way to determine the local maxima. I hope to investigate the use of b-splines fit over the row sum values to see if these would be more effective at finding local maxima

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/nonNA_rowSums} 

}

\caption{Non-NA pixel row counts and moving average-smoothed row count values plotted against row index.}\label{fig:unnamed-chunk-25}
\end{figure}

However, because firing pin impression circles have somewhat perforated edges, performing one pass through the image may not yield a particularly accurate estimate. As such, we can repeat the process of finding the distance between local maxima for both the row and column non-NA pixel counts. We can also rotate the image by a few degrees and perform the same process. I am currently rotating the image 0, 15, 30, 45, 60, and 75 degrees and calculating row and column diameter estimates per rotation. Obviously we can apply whatever aggregation function we desire to these estimates to determine a final estimate. Below we see what the Hough Transform selects as the breech face for 4 different radii values. In particular, for circles of radius 210, 213, 216, and 219.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cartridge_cases/houghTransformGridSearch} 

}

\caption{Hough Transform selected circles (red) of radius (1) 210, (2) 213, (3) 216, and (4) 219.}\label{fig:unnamed-chunk-26}
\end{figure}

\hypertarget{modified-chumbley-non-random}{%
\subsection{Modified Chumbley non-random}\label{modified-chumbley-non-random}}

\hypertarget{land-to-land-scores}{%
\subsubsection{Land-to-land scores}\label{land-to-land-scores}}

\hypertarget{bullet-to-bullet-scores}{%
\subsubsection{Bullet-to-bullet scores}\label{bullet-to-bullet-scores}}

Ganesh: In this method we extend the modified chumbley non-random method from land-to-land scoring to bullet-to-bullet scoring.

\hypertarget{analysis-of-results}{%
\section{Analysis of Results}\label{analysis-of-results}}

\hypertarget{communication-of-results-and-methods}{%
\section{Communication of Results and Methods}\label{communication-of-results-and-methods}}

The results are communicated through an interactive user interface. The first part of this interface lets you add all the bullets, barrels and lands for which the random forest and other scores are to be computed. A preliminary diagnostic of the orientations and dimensions of the lands tell us, if we can proceed safely to extraction of markings and then to cross-comparisons.

After this step, we can apply any sampling or interpolation needed on the land images, all these operations can be batched to the entire set of comparisons under consideration. Then we can make transformations like rotation, transpose etc on a sample image, visualize the results, and since we are dealing with conforming orientation and dimensions of lands present in the entire set, we can batch the transformations.

We extract markings, locate grooves, align signatures, and generate cross-comparison results. Each step is notified in UI and all steps are logged.

The scores and results are then communicated through an interactive visualization. We first interact at the top most level where we have bullet-to-bullet scores for all the cross-comparisons presented in a grid. We can select one comparison at a time which would generate a second level of grid visualization that shows the land-to-land scores for all 36 comparisons within a bullet. Interacting with this visualization, we can now pull up score tables, profiles, location of grooves, aligned signatures and raw images.

The framework of interactions, allows for validation of classification recommended by the RF model as well as gives an opportunity to critically asses, identify the cause and diagnose any problems encountered in the bullet matching pipeline.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/bullets/gan-app2_consolidated} 

}

\caption{An instance of the interactive visualizations for communicating results}\label{fig:unnamed-chunk-27}
\end{figure}

\hypertarget{conference-presentations}{%
\subsection{Conference Presentations}\label{conference-presentations}}

\hypertarget{american-academy-of-forensic-sciences}{%
\subsubsection{American Academy of Forensic Sciences}\label{american-academy-of-forensic-sciences}}

\begin{itemize}
\tightlist
\item
  ``Validation Study on Automated Groove Detection Methods in 3D Bullet Land Scans''

  \begin{itemize}
  \tightlist
  \item
    February 2019\\
  \item
    Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann
  \item
    Presentation given by Kiegan Rice
  \end{itemize}
\end{itemize}

\hypertarget{association-of-firearms-and-toolmark-examiners-annual-training-seminar}{%
\subsubsection{Association of Firearms and Toolmark Examiners Annual Training Seminar}\label{association-of-firearms-and-toolmark-examiners-annual-training-seminar}}

\begin{itemize}
\tightlist
\item
  Heike's talk
\item
  ``Reproducibility of Automated Bullet Matching Scores Using High-Resolution 3D LEA Scans''

  \begin{itemize}
  \tightlist
  \item
    May 2019
  \item
    Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann
  \item
    Presentation given by Kiegan Rice
  \end{itemize}
\end{itemize}

\hypertarget{joint-statistical-meetings}{%
\subsubsection{Joint Statistical Meetings}\label{joint-statistical-meetings}}

\begin{itemize}
\tightlist
\item
  ``A non-parametric test for matching bullet striations: extending the chumbley score for bullet-to-bullet matching''

  \begin{itemize}
  \tightlist
  \item
    July 2019
  \item
    Authors:Ganesh Krishnan, Heike Hofmann
  \item
    Talk given by Ganesh Krishnan
  \end{itemize}
\item
  ``Repeatability and reproducibility of automated bullet comparisons using high-resolution 3D scans''

  \begin{itemize}
  \tightlist
  \item
    July 2019
  \item
    Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann
  \item
    Poster presented by Kiegan Rice
  \end{itemize}
\end{itemize}

\hypertarget{miscellaneous}{%
\subsubsection{Miscellaneous}\label{miscellaneous}}

\begin{itemize}
\tightlist
\item
  10th International Workshop on Statistics and Simulation in Salzburg, Austria, September 2019

  \begin{itemize}
  \tightlist
  \item
    ``Reproducibility of High-Resolution 3D Bullet Scans and Automated Bullet Matching Scores''

    \begin{itemize}
    \tightlist
    \item
      Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann
    \item
      Poster presented by Kiegan Rice, won 2nd Springer Poster Award\\
    \end{itemize}
  \item
    ``Case Study Validations of Automatic Bullet Matching''

    \begin{itemize}
    \tightlist
    \item
      Authors: Heike Hofmann, Susan VanderPlas
    \item
      Presentation given by Alicia Carriquiry
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{people-involved}{%
\section{People involved}\label{people-involved}}

\hypertarget{faculty}{%
\subsection{Faculty}\label{faculty}}

\begin{itemize}
\tightlist
\item
  Heike Hofmann
\item
  Susan VanderPlas
\end{itemize}

\hypertarget{graduate-students}{%
\subsection{Graduate Students}\label{graduate-students}}

\begin{itemize}
\tightlist
\item
  Ganesh Krishnan
\item
  Kiegan Rice
\item
  Nate Garton
\item
  Charlotte Roiger
\item
  Joe Zemmels
\item
  Yawei Ge
\end{itemize}

\hypertarget{undergraduates}{%
\subsection{Undergraduates}\label{undergraduates}}

\begin{itemize}
\tightlist
\item
  Talen Fisher (fix3p)
\item
  Andrew Maloney
\item
  Mya Fisher, Allison Mark, Connor Hergenreter, Carley McConnell, Anyesha Ray (scanner)
\end{itemize}

\hypertarget{project-g-handwriting-signatures}{%
\chapter{Project G: Handwriting (\& Signatures)}\label{project-g-handwriting-signatures}}

The handwriting project has four major focuses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  data collection
\item
  computational tools
\item
  statistical analysis

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    glyph clustering
  \item
    closed set modeling for writer identification
  \end{enumerate}
\item
  communication of results
\end{enumerate}

\hypertarget{data-collection-1}{%
\section{Data Collection}\label{data-collection-1}}

We are conducting a large data collection study to gather handwriting samples from a variety of participants across the world (most in the Midwest). Each participant provides handwriting samples at three sessions. Session packets are prepared, mailed to participants, completed, and mailed back. Once recieved, we scan all surveys and writing samples. Scans are loaded, cropped, and saved using a Shiny app. The app also facilitates survey data entry, saving that participant data to lines in an excel spreadsheet.

Data collection is underway with the most recent update (9/1) at 106 participants enrolled:

\begin{itemize}
\tightlist
\item
  44 complete through session \#3
\item
  52 complete through session \#2
\item
  10 complete through session \#1
\end{itemize}

As of September 2019, Marc and Anyesha are the primary contacts for the study.

\hypertarget{computational-tools-1}{%
\section{Computational Tools}\label{computational-tools-1}}

\texttt{handwriter} is a developmental R package hosted at \url{https://github.com/CSAFE-ISU/handwriter}. It is our major computational tool for the project. The package takes in scanned handwritten documents and the following are performed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
   Binarize. Turn the image to pure black and white.
\item
   Skeletonize. Reduce writing to a 1 pixel wide skeleton.
\item
   Break. Connected writing is decomposed into small manageable pieces called glyphs . Glyphs are graphical structures with nodes and edges that often, but not always, correspond to Roman letters, and are the smallest unit of observation we consider for statistcal modelling.
\item
   Measure. A variety of measurements are taken on each glyph.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/handwriting/amy/handwriter_csafe} 

}

\caption{Connected text processed by `handwriter`. The grey background is the original pen stroke. Colored lines represent the single pixel skeleton with color changes marking glyph decomposition. Red dots mark endpoints and intersections of each glyph.}\label{fig:unnamed-chunk-28}
\end{figure}

For an input document, functions in the package give back a list of glyphs with path and node location information, adjacency grouping assignment, slope (pictured below), and centroid locations, among other things.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/handwriting/amy/handwriter_slopecalc} 

}

\caption{A visual of the ``slope'' calculation for two glyphs.}\label{fig:unnamed-chunk-29}
\end{figure}

We are currently working to incorporate the cluster grouping assignments into the package. This will be complete pending creation of a template.

\hypertarget{statistical-analysis}{%
\section{Statistical Analysis}\label{statistical-analysis}}

\hypertarget{clustering}{%
\subsection{Clustering}\label{clustering}}

Background to be added here.

Paper submitted!

\ldots{}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/handwriting/amy/clusterpaper_header} 

}

\end{figure}

\hypertarget{closed-set-modelling}{%
\subsection{Closed set modelling}\label{closed-set-modelling}}

The following will be the jumping off point for purposes of this book with respect to modelling and discussion of results.

\begin{figure}
\centering
\includegraphics{images/handwriting/amy/Crawford_SIMSTAT_2019.png}
\caption{Poster given at the 10th International Workshop on Simulation and Statistics}
\end{figure}

\hypertarget{communication-of-results}{%
\section{Communication of Results}\label{communication-of-results}}

Presenting/corresponding author is in bold.

\hypertarget{papers}{%
\subsection{Papers}\label{papers}}

\begin{itemize}
\tightlist
\item
  ``A Clustering Method for Graphical Handwriting Components and Statistical Writership Analysis''

  \begin{itemize}
  \tightlist
  \item
    Authors: Nick Berry, Amy Crawford
  \item
    Submitted to The Annals of Applied Statistics in September 2019.
  \end{itemize}
\item
  ``Handwriting 2''

  \begin{itemize}
  \tightlist
  \item
    Authors: Amy Crawford, Alicia Carriquiry, and Danica Ommen
  \item
    In preparation for submission to PNAS
  \end{itemize}
\end{itemize}

\hypertarget{talks}{%
\subsection{Talks}\label{talks}}

\begin{itemize}
\tightlist
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    August 2019
  \item
    Authors: Amy Crawford, Alicia Carriquiry, Danica Ommen
  \item
    American Society of Questioned Document Examiners (ASQDE) Annual Meeting in Cary, NC.
  \item
    Talk, 80 minutes.
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    July 2019
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry Danica Ommen
  \item
    Joint Statistical Meetings (JSM) in Denver, CO.
  \item
    Talk, 15 minutes.
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    July 2019
  \item
    Authors: Alicia Carriquiry, Amy Crawford, Nick Berry, Danica Ommen
  \item
    Lima, Peru.
  \item
    Talk
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    February 2019
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry, Danica Ommen
  \item
    American Academy of Forensic Sciences (AAFS) Annual Meeting in Baltimore, MD.
  \item
    Talk, 20 minutes
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    August 2018
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry, Danica Ommen
  \item
    American Society of Questioned Document Examiners (ASQDE) Annual Meeting in Park City, UT.
  \item
    Talk, 20 minutes.
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    July 2018
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry, Danica Ommen
  \item
    Joint Statistical Meetings (JSM) in Vancouver, BC, Canada.
  \item
    Talk, 15 minutes.
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    May 2018
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry
  \item
    American Bar Association, 9th Annual Prescription for Criminal Justice Forensics Program in New York, NY.
  \item
    Talk, 15 minutes.
  \end{itemize}
\end{itemize}

\hypertarget{posters}{%
\subsection{Posters}\label{posters}}

\begin{itemize}
\tightlist
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    August 2019
  \item
    Authors: Amy Crawford, Alicia Carriquiry, Danica Ommen
  \item
    10th International Workshop on Statistics and Simulation in Salzburg, Austria
  \item
    1st Springer Poster Award
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    February 2018
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry
  \item
    American Academy of Forensic Sciences in Seattle, WA
  \item
    YFSF Best Poster Award
  \end{itemize}
\item
  ``TITLE''

  \begin{itemize}
  \tightlist
  \item
    May 2018 and 2019
  \item
    Authors: Amy Crawford, Nick Berry, Alicia Carriquiry, Danica Ommen
  \item
    CSAFE Annual All-Hands Meetin in Ames, IA
  \end{itemize}
\end{itemize}

\hypertarget{people-involved-1}{%
\section{People involved}\label{people-involved-1}}

\hypertarget{faculty-1}{%
\subsection{Faculty}\label{faculty-1}}

\begin{itemize}
\tightlist
\item
  Alicia Carriquiry
\item
  Danica Ommen
\item
  Hal Stern (UCI, Project G PI)
\end{itemize}

\hypertarget{graduate-students-1}{%
\subsection{Graduate Students}\label{graduate-students-1}}

\begin{itemize}
\tightlist
\item
  Amy Crawford
\end{itemize}

\hypertarget{undergraduates-1}{%
\subsection{Undergraduates}\label{undergraduates-1}}

\begin{itemize}
\tightlist
\item
  Anyesha Rey (data collection)
\end{itemize}

\hypertarget{glass}{%
\chapter{Glass}\label{glass}}

\hypertarget{shoes}{%
\chapter{Shoes}\label{shoes}}

\hypertarget{longitudinal}{%
\section{Longitudinal Shoe Study}\label{longitudinal}}

\href{https://github.com/CSAFE-ISU/Longitudinal_Shoe_Study}{Github repository}

\hypertarget{paper-describing-the-database}{%
\subsection{Paper describing the database}\label{paper-describing-the-database}}

\href{https://github.com/CSAFE-ISU/Longitudinal_Shoe_Study/tree/master/Paper}{Paper subdirectory of Github repository}

Goal:

\begin{itemize}
\tightlist
\item
  Describe experiment
\item
  Describe database function
\item
  Publicize data for analysis by others in the community
\end{itemize}

\hypertarget{lss-paper-methods}{%
\subsubsection*{Methods and Data Description}\label{lss-paper-methods}}
\addcontentsline{toc}{subsubsection}{Methods and Data Description}

Methods and data description handed off to Alicia for editing

\hypertarget{lss-paper-analysis}{%
\subsubsection*{Data Analysis Tools}\label{lss-paper-analysis}}
\addcontentsline{toc}{subsubsection}{Data Analysis Tools}

\begin{itemize}
\tightlist
\item
  Working with the \texttt{EBImage} package - very fast processing of images
\end{itemize}

\hypertarget{lss-paper-analysis-film}{%
\paragraph{Film and Powder Images}\label{lss-paper-analysis-film}}
\addcontentsline{toc}{paragraph}{Film and Powder Images}

Analysis Summary: Create a mask via thresholding, clean it up, fill in mask holes, creating a shoe ``region'' mask. Apply this mask to the image, replacing any pixels outside the mask with the median background pixel. Additional thresholding and normalization can be applied if a binary image is more desireable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create threshold mask

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Blur image (circular/gaussian blur, diameter 15)\\
  \item
    Invert the image\\
  \item
    Threshold image (adaptive threshold, 10 x 10 region, keep anything with an average higher than 0.025 from the mean)\\
  \item
    Create mask\\
    Default parameters selected by visually screening several shoes:
    (default parameters rad1 = 5, rad2 = {91}, proportion = 1.5*area of rad2 in px/area of image in px)\\

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      erode mask image (circle, diameter rad1)
    \item
      dilate mask image (circle, diameter rad2)
    \item
      label disjoint regions of the image
    \item
      prune small image regions (area \textless{} proportion parameter)
    \end{enumerate}
  \item
    Fill in mask holes\\
  \item
    Expand mask to capture entire shoe region

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      set background color
    \item
      create dataframe of useful (non-background) pixels
    \item
      fill in holes and concave regions in mask, then expand by expand\_rad vertically and horizontally (similar to ``convex hull'', but with additional expansion radius)
    \end{enumerate}
  \end{enumerate}
\item
  Mask image to remove extra variability unrelated to the shoe\\
\item
  Threshold masked image?\\
  Con: Lose grey information; Pro: fully remove background\\
\item
  Compromise: Keep grey pixels from thresholded, masked image (e.g.~use 3. as a mask), then renormalize\\
\end{enumerate}

I've added the functions from last week to the \texttt{ShoeScrubR} package, which will hopefully contain methods for handling all of the different 2D shoe data from the longitudinal study.

Using that package, I tried the method out on a sequence of shoes over time to see what methods might best show wear. Each column shows a single left shoe over four timepoints. The shoes are the first 9 shoeIDs (e.g.~1 - 9).

Original

Cleaned\\

Cleaned and Thresholded

Even with the cleaning methods\ldots{} there is a lot of extra noise.

Next step: templating!

Basic framework:

Create a template for each size and model combination
(using GIMP - if I could automate this, I wouldn't need the template)

Intelligently brute force angle and position of template\\
Goal: Maximize the number of black pixels in the image within the template region

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with an image and a template mask
\item
  Blur, normalize, invert, and threshold the image\\
\item
  Naively align the ``centers'' of the two images (avg of white pixel row/cols). To make this calculation comparable, do some very crude dilation/erosion (that may or may not generalize that well) to fill in the image a bit.\\
  ~\\
  Then make the aligned center the actual center of the image via padding. (This is the 1st time we have modified the actual image beyond thresholding and color changes).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textasciitilde{}\textasciitilde{}Create a new mask to sample the image (and the mask) radially. \textasciitilde{}\textasciitilde{} This doesn't work when the object isn't a solid entity :(
\end{enumerate}

New Option: Use image pyramids and brute-force alignment, starting off with an estimated rotation angle of \(\theta\) from principal components\\
~\\
~\\

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Brute force full-size image to get finer alignment.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Remove anything not in the mask region.
\end{enumerate}

\hypertarget{lss-paper-analysis-wear}{%
\paragraph{Wear Characterization}\label{lss-paper-analysis-wear}}
\addcontentsline{toc}{paragraph}{Wear Characterization}

Ideas:

\begin{itemize}
\tightlist
\item
  average intensity of cleaned image
\item
  length of border/edges detected
\end{itemize}

\hypertarget{connor}{%
\section{Passive Shoe Recognition}\label{connor}}

\hypertarget{nij-grant}{%
\subsection{NIJ Grant}\label{nij-grant}}

Grant scope: Build the shoe scanner, develop an automatic recognition algorithm for geometric design elements, test the scanner in locations around Ames.

Status: Funded! Next challenge: Figuring out how to transfer it to UNL.

\hypertarget{connor-convolutional-neural-network-for-outsole-recognition}{%
\subsection{CoNNOR: Convolutional Neural Network for Outsole Recognition}\label{connor-convolutional-neural-network-for-outsole-recognition}}

\textbf{Project Overview}

\begin{itemize}
\tightlist
\item
  Label images of shoes according to geometric classification scheme
\item
  Use convolutional base of pretrained CNN VGG16 and train a new classifier on labeled features
\item
  Eventually, acquire real data passively and use CoNNOR to assess feature similarities and frequencies
\end{itemize}

\href{https://lib.dr.iastate.edu/creativecomponents/264/}{Link to submitted Creative Component on CoNNOR}

\href{https://github.com/srvanderplas/CoNNORFSI}{Github repository for paper submitted to Forensic Science International}

\textbf{Exploring new directions:}

\begin{itemize}
\tightlist
\item
  Truncate convolutional base and train random forest on features

  \begin{itemize}
  \tightlist
  \item
    Could replace fully connected layers of neural net as classifier
  \item
    Importance score can filter/reduce the number of features
  \item
    \emph{Block 4 random forest training terminated after one week :( }
  \item
    \emph{Block 5 currently training for two different random forest packages (randomForest and ranger)}
  \item
    \emph{If new models take more than 1-2 weeks, will look into subsampling techniques.}
  \end{itemize}
\item
  \emph{Spatial integration}

  \begin{itemize}
  \tightlist
  \item
    \emph{Model is currently set up to take in 256x256 pixels}
  \item
    \emph{Try taking in full shoe using a sliding window of size 256x256}
  \item
    \emph{View class predictions spatially}
  \end{itemize}
\item
  Fully convolutional networks (FCNs)

  \begin{itemize}
  \tightlist
  \item
    Unsupervised segmentation to assess current classification scheme
  \item
    Handle whole shoe image of any size (instead of only 256x256 pixel images)
  \end{itemize}
\end{itemize}

\textbf{References for CNNs and FCNs}

\href{https://stats.stackexchange.com/questions/266075/patch-wise-training-and-fully-convolutional-training-in-fully-convolutional-neur}{Stack Exchange post explaining patchwise training}

\href{https://ieeexplore.ieee.org/abstract/document/6338939}{``Learning Hierarchical Features for Scene Labeling''}: describes an application of multi-scale CNNs and image pyramids

\href{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.8646}{``Pyramid methods in image processing''}: classic paper from 1984 explaining pyramid methods

\href{https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf}{``Fully Convolutional Networks for Semantic Segmentation''}

\href{https://arxiv.org/pdf/1711.08506.pdf}{``W-Net: A Deep Model for Fully Unsupervised Image Segmentation''}

\hypertarget{spatial-integration}{%
\subsection{Spatial integration}\label{spatial-integration}}

The overhead costs of going fully convolutional are high; CNN papers are opaque, and many supervised techniques require fully labeled data for semantic segmentation (i.e., label every pixel). Moreover, complex models (for both supervised and unsupervised tequniques) are often only available in Python, and there are a large number of GitHub repositories of mixed quality and reliability. Filtering for quality, understanding code structures, and implementing them on HPC are all enormous tasks on their own.

In the meantime, it is much easier (relatively speaking) to use our existing framework of 256x256 square pixel images, for which we have generated thousands of labeled images and have already trained and improved domain-specific models. Currently, I have code working to automatically crop image borders, chop the image into 256x256 pixels (padding the image when appropriate) and correct the contrast on the individual images.

\\

I hoped to have some cool visualizations to show today. Unfortunately, model predictions are behaving very strangely, and I haven't been able to figure out why\ldots{} All classes are predicting to zero, except quadrilaterals.

Possible issues:

\begin{itemize}
\item
  The specific shoe is behaving strangely

  The issue persists for multiple shoes
\item
  Using a different package to read in the image changes the extracted features

  The issue persists when I try using the original image reading function
\item
  Contrast correction changes predictions

  The issue exists for images both with and without contrast correction
\item
  Model trained incorrectly

  The \href{https://bigfoot.csafe.iastate.edu:442/tiltonm/NNPreview/}{Shiny app} shows predictions on the test set are behaving as expected
\item
  Bigfoot is angry?
\end{itemize}

Bottom line: CNN features are NOT interpretable, which makes them VERY hard to debug.

\hypertarget{maxclique}{%
\section{Maximum Clique Matching}\label{maxclique}}

\hypertarget{cocoa}{%
\section{Project Tread (formerly Cocoa Powder Citizen Science)}\label{cocoa}}

Project Tread, modified from \href{https://www.dundee.ac.uk/leverhulme/citizenscience/details/sole-searching.php}{Leverhulme Institute's Sole Searching}, is a developing CSAFE project with the goals of engaging community participation in forensic research and acquiring shoe print data that may be useful in future analyses.

In progress:

\begin{itemize}
\tightlist
\item
  Review \href{https://forensicstats.org/project-tread/}{procedures} and IRB documents written by James
\item
  Perhaps modify procedures, then bribe some friends into helping me test them :)

  \begin{itemize}
  \tightlist
  \item
    Test for length, clarity, ease, etc.
  \end{itemize}
\item
  Be involved in set up of data collection site (through CSSM)
\end{itemize}

\hypertarget{comparing-the-procedures}{%
\subsubsection{Comparing the procedures}\label{comparing-the-procedures}}

\begin{longtable}[]{@{}lll@{}}
\toprule
Procedure & Leverhulme & CSAFE\tabularnewline
\midrule
\endhead
`Before' Pictures & 4 per shoe & 15 per shoe\tabularnewline
Paper & Letter (larger) & Tape printer paper\tabularnewline
& &\tabularnewline
Actions & Run, jump, walk & Step, hop\tabularnewline
Replicates & 6 per shoe & 9 per shoe\tabularnewline
`After' Pictures & 1 pic per print & 3 pics per print\tabularnewline
& &\tabularnewline
Total number prints & 18 per shoe & 18 per shoe\tabularnewline
Total number images & 18 per shoe & 54 per shoe\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{d-shoe-recognition}{%
\section{3d Shoe Recognition}\label{d-shoe-recognition}}

The set up

What we have tried

What we are doing now
- Transforming the mesh objects to points aligned by the center of mass to overlay them detecting difference.
- Problems
- Isnt aligning properly as you can see
- Next
- angle transformations

\hypertarget{shoe-outsole-matching-using-image-descriptors}{%
\section{Shoe outsole matching using image descriptors}\label{shoe-outsole-matching-using-image-descriptors}}

Previously, features such as edge, corner, SURF were extracted to match shoeprints. The goal of this project is to find other image descriptors as image features for shoe print matching.

\textbf{Image descriptors}

\begin{itemize}
\tightlist
\item
  SURF(Speeded Up Robust Features)- blobs
\item
  KAZE - blobs
\item
  ORB(Oriented FAST and Rotated BRIEF)- corners
\end{itemize}

\hypertarget{theoretical-foundations}{%
\chapter{Theoretical foundations}\label{theoretical-foundations}}

\hypertarget{common-source-vs-specific-source-comparison-via-information-theory}{%
\section{Common Source vs Specific Source Comparison via Information Theory}\label{common-source-vs-specific-source-comparison-via-information-theory}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

\textbf{Central Goals}

\begin{itemize}
\tightlist
\item
  continue work started by Danica and Peter Vergeer on the analysis of likelihood ratios
\item
  study the differences between specific source (SS) and common source (CS) likelihood ratios (LRs) in an information theoretic way
\item
  does the CS or SS LR have more ``information''?
\item
  can be the CS or SS hypotheses (prosecution or defense) be formally compared in terms of being easier to ``prove'' or ``disprove''?
\end{itemize}

\textbf{General Notation}
Let \(X \in \mathbb{R}^{q_x}\) and \(Y \in \mathbb{R}^{q_y}\) be two random vectors with joint distribution \(P\) and corresponding density \(p\).

\begin{itemize}
\tightlist
\item
  \textbf{Entropy}: \(\mathbb{H}(X) = -\int{p(x) \log p(x) dx}\)
\item
  \textbf{Conditional Entropy}: \(\mathbb{H}(X|Y) = \mathbb{E}_{Y}\left[-\int{p(x|y) \log p(x|y) dx}\right]\)
\item
  \textbf{Type II Conditional Entropy}: \(\mathbb{H}_{2}(X|Y) = -\int{p(x|y) \log p(x|y) dx}\)
\item
  \textbf{Mutual Information}: \(\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X|Y)\)
\end{itemize}

Proof that Mutual Information is always positive:

\begin{align*}
\mathbb{I}(X;Y) &= \mathbb{H}(X) - \mathbb{H}(X|Y) \\
&= -\int{p(x) \log p(x) dx} + \int{\int{p(x|y)p(y) \log p(x|y) dx} dy} \\
&= -\int{\int{p(x,y) \log p(x) dx}dy} + \int{\int{p(x,y) \log p(x|y) dx} dy} \\
&= -\int{\int{p(x,y) \log p(x) dx}dy} + \int{\int{p(x,y) \log \frac{p(x,y)}{p(y)} dx} dy} \\
&= \int{\int{p(x,y) \log \frac{p(x,y)}{p(x)p(y)} dx}dy} \\
&= KL(P||P_{X} \times P_{Y}) \\
&\geq 0
\end{align*}

\hypertarget{common-source-vs-specific-source-lr}{%
\subsection{Common Source vs Specific Source LR}\label{common-source-vs-specific-source-lr}}

The ``common source'' problem is to determine whether two pieces of evidence, both with unknown origin, have the same origin. One might be interested in this problem if two crimes were suspected to be linked, but no suspect has yet been identified. Alternatively, the ``specific source'' problem is to determine whether a fragment of evidence coming from an unknown source, such as evidence at a crime scene, has the same origin as a fragment of evidence of known origin, such as evidence collected directly from a suspect.

\textbf{Basic Setup}

\begin{itemize}
\tightlist
\item
  \(H \in \{ H_p, H_d \}\) as the random variable associated with the CS hypothesis.
\item
  \(A\) and \(B\) are discrete r.v.'s representing two ``sources'' of evidence
\item
  distributions for \(A\) and \(B\) defined conditionally based on the hypothesis
\item
  SS hypothesis is represented by the conditional random variable \(H_p|A\)
\item
  \(X\) is data coming from \(A\), \(Y\) is data coming from \(B\)
\item
  compare information contained in \((X,Y)\) about \(H_p\) and \(H_p|A\)
\item
  join density can be written as \(p(X,Y,A,B,H) = p(X,Y|A,B)p(B|A,H)p(A|H)p(H)\)
\end{itemize}

\textbf{Is there more information in a CS or SS LR?}

Let us examine this question in two different ways.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is the posterior entropy (given \((X,Y)\)) in the common source hypothesis smaller than that of the specific source hypothesis?

  \begin{itemize}
  \tightlist
  \item
    In other words, would observing the specific value of \(A\) as well as the data make you \emph{more} certain about \(H\) than just observing the data?
  \end{itemize}
\item
  Is the posterior entropy (given \((X,Y)\)) in the common source hypothesis smaller than the average (over possible values for \(A\)) posterior entropy of the specific source hypothesis?

  \begin{itemize}
  \tightlist
  \item
    In other words, do you expect that, on average, observing the value of \(A\) as well as the data make you \emph{more} certain about \(H\) than just observing the data?
  \end{itemize}
\end{enumerate}

Answering the first question/interpretation, to me, requires proving that

\[ \mathbb{H}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A) \geq 0 \].

Answering the second question requires proving that

\[ \mathbb{H}(H|X,Y) - \mathbb{H}(H|X,Y, A) \geq 0 \].

\noindent Luckily, the second question is true due to the fact that

\begin{align*}
\mathbb{H}(H|X,Y) - \mathbb{H}(H|X,Y,A) &= \mathbb{E}_{(X,Y)} \left[ - \int{p(h,a|x,y) \log p(h|x,y) d(h,a)} + \int{p(h,a|x,y) \log p(h|x,y,a) d(h,a)} \right] \\
&= - \int{p(h,a|x,y)p(x,y) \log \frac{p(h,a|x,y)}{p(a|x,y)p(h|x,y)} d(h,x,y,a)} \\
&= \mathbb{E}_{(X,Y)} \left[ KL(P_{(H,A)|(X,Y)}||P_{H|(X,Y)} \times P_{A|(X,Y)}) \right] \geq 0
\end{align*}

\hypertarget{score-based-likelihood-ratios-are-not-fundamentally-incoherent}{%
\section{Score-based Likelihood Ratios are not Fundamentally ``Incoherent''}\label{score-based-likelihood-ratios-are-not-fundamentally-incoherent}}

Concern has been raised in the literature on LRs about a desirable property supposedly inherently absent from specific-source SLRs. The property, dubbed ``coherence'', intuitively says that given two mutually exhaustive hypotheses, \(H_A\) and \(H_B\), the likelihood ratio used to compare hypothesis A to hypothesis B should be the reciprocal of that used to compare hypothesis B to hypothesis A. I will argue that the claims about the inherent incoherency of SLRs is a result of thinking about SLRs too narrowly. Specifically, I will show that the arguments as to why SLRs are incoherent arise through the inappropriate comparison of SLRs based on different score functions. When one appropriately considers a single score function, incoherency is impossible.

\hypertarget{coherence}{%
\subsection{Coherence}\label{coherence}}

Denote by \(E \in \mathbb{R}^{n}\) the vector of random variables describing \emph{all} of the observed evidence or data which will be used to evaluate the relative likelihood of the two hypotheses. Define by \(LR_{i,j} \equiv \frac{p(E|H_i)}{p(E|H_j)}\) the likelihood ratio of hypothesis \(i\) to hypothesis \(j\). The coherency principal is satisfied if

\[ LR_{i,j} = \frac{1}{LR_{j,i}} \].

Likelihood ratios are fundamentally coherent, but what about score-based likelihood ratios? Denote by \(s: \mathbb{R}^n \rightarrow \mathbb{R}^{q}\) a score function mapping the original data to Euclidean space of dimension \(q\) (typically \(q = 1\)). Similar to LRs, denote by \(SLR_{i,j} \equiv \frac{p(s(E)|H_i)}{p(s(E)|H_j)}\) the score-based likelihood ratio comparing hypothesis \(i\) to hypothesis \(j\). Clearly, in this general context SLRs are also coherent.

\hypertarget{problems-with-arguments-showing-slrs-are-incoherent}{%
\subsection{Problems with arguments showing SLRs are incoherent}\label{problems-with-arguments-showing-slrs-are-incoherent}}

Let us examine the arguments presented in {[}REFS{]} to the incoherence of SLRs. These arguments stem from an example where there are two known sources of evidence say, source \(A\) and source \(B\), each producing data \(e_A\) and \(e_B\), respectively. Furthermore, assume that we have a third piece of evidence of unknown origin, \(e_u\), which must have come from either \(A\) or \(B\). We then wish to evaluate the support of the data for \(H_A\) or \(H_B\) defined as follows

\textbackslash{}begin\{array\}\{cc\}
H\_A: \& e\_u \text{ was generated from source } A \textbackslash{}
H\_B: \& e\_u \text{ was generated from source } B.
\textbackslash{}end\{array\}

In this case, we have \(LR_{A,B} = \frac{p(e_A, e_B, e_u|H_A)}{p(e_A, e_B, e_u|H_B)}\). We make use of \emph{all} available data in the formulation of the numerator and denominator densities. Under the assumptions that each fragment of evidence is independent under both hypothesis \(A\) and \(B\) as well as that \(p(e_A,e_B|H_A) = p(e_A,e_B|H_B)\), the LR reduces to \(LR_{A,B} = \frac{p(e_u|H_A)}{p(e_u|H_B)}\). The second assumption is generally acceptable as the source of \(e_u\) ought to have no impact on the distribution of the evidence with known source.

{[}REFS{]} then consider possible SLRs for this example. However, they make an assumption that the score is explicitly a function only of two fragments of evidence. That is, assuming the dimension of \(e_i\), \(dim(e_i) = k\), is constant for \(i = A,B,u\), their score maps \(s:\mathbb{R}^k \times \mathbb{R}^k \rightarrow \mathbb{R}\). An common example of such a score is Euclidean distance, i.e. \(s(x,y) = \left[ \sum_{i = 1}^{k}(x_i - y_i)^2 \right]^{1/2}\). Such a score makes perfect sense in a typical specific-source problem context in which only two fragments of evidence are considered: one from the known source and one from the unknown source.

However, when one desires to create an SLR based on this score in this particular example, it is tempting to suggest that the natural SLR is \(SLR_{A,B} = \frac{p(s(e_A,e_u)|H_A)}{p(s(e_A,e_u)|H_B)}\). Yet, the natural SLR if the hypotheses were reversed is \(SLR_{B,A} = \frac{p(s(e_B,e_u)|H_B)}{p(s(e_B,e_u)|H_A)}\). Neither of these SLRs is the reciprocal of the other, and so the specific source SLR appears to be ``incoherent''.

This approach, however, should raise a red flag immediately. Why, in the full LR case, do we require that (simplifying model assumptions aside) the numerator and denominator densities be functions of all available data, but the score is not? Furthermore, if we consider these SLRs in the more general context of scores depending on all available data, we see that, in fact, what {[}REFS{]} define to be \(SLR_{A,B}\) and \(SLR_{B,A}\) turn out to be two different SLRs depending on two different scores.

For clarity, we will use \(s(\cdot)\) to denote scores which are explicitly functions of \emph{all} observed data, and we will use \(\delta (\cdot)\) to denote score functions which are only a function of two fragments of evidence/data. Specifically, the score in \(SLR_{A,B}\) is \(s_1(e_u,e_A,e_B) = \delta(e_u,e_A)\) and the score in \(SLR_{B,A}\) is \(s_2(e_u,e_A,e_B) = \delta(e_u,e_B)\). While the functional form of the score in the two SLRs \emph{appears} to be the same, clearly \(s_1(e_u,e_A,e_B) \neq s_2(e_u,e_A,e_B)\). Thus, the two SLRs are simply two distinct quantities whose relationship needn't be expected to be related anymore than if one had decided to use two different function forms of \(\delta(\cdot,\cdot)\) in the two separate SLRs.

One might ask how to reasonably construct an SLR which utilizes a score other than a similarity metric for two fragments of evidence. One such example in this case would be \(s(e_u, e_A, e_B) = \frac{\delta(e_u,e_A)}{\delta(e_u,e_B)}\). Intuitively, under \(H_A\), the numerator should be larger than the denominator, while under \(H_B\), the opposite should be true.

\hypertarget{example-of-a-coherent-slr-in-the-two-source-problem}{%
\subsection{Example of a coherent SLR in the two source problem}\label{example-of-a-coherent-slr-in-the-two-source-problem}}

Suppose that our hypotheses are defined such that

\[
\begin{array}{cc}
H_A: & e_u \sim N(\mu_A, \sigma^2), e_A \sim N(\mu_A, \sigma^2), e_B \sim N(\mu_B, \sigma^2) \\
H_B: & e_u \sim N(\mu_B, \sigma^2), e_A \sim N(\mu_A, \sigma^2), e_B \sim N(\mu_B, \sigma^2),
\end{array}
\]

where \(e_u\), \(e_A\), \(e_B\) are mutual independent under both \(H_A\) and \(H_B\).

It is true that the LR depends only on the evidence of the unknown source \emph{in this specific scenario}, but that is a consequence of modeling assumptions and not of LR paradigmatic principals.

\hypertarget{outreach-activities}{%
\chapter{Outreach activities}\label{outreach-activities}}

CSAFE has several ongoing outreach projects.

\hypertarget{book-on-forensic-science-and-statistics}{%
\section{Book on Forensic Science and Statistics}\label{book-on-forensic-science-and-statistics}}

Project members:

\begin{itemize}
\tightlist
\item
  Kiegan Rice\\
\item
  Alicia Carriquiry\\
\item
  Hal Stern (UCI)
\end{itemize}

General book outline:

\begin{itemize}
\tightlist
\item
  Chapter 1: Introduction\\
\item
  Chapter 2: Analysis of Forensic Evidence

  \begin{itemize}
  \tightlist
  \item
    Drafted. In the editing stage.
  \end{itemize}
\item
  Chapter 3: The Path to the Witness Stand

  \begin{itemize}
  \tightlist
  \item
    Drafted. In the editing stage.
  \end{itemize}
\item
  Chapter 4: Communicating Evidence in the Courtroom

  \begin{itemize}
  \tightlist
  \item
    Not drafted. In the writing stage.
  \end{itemize}
\item
  Chapter 5: Conclusions
\end{itemize}

\bibliography{book.bib,packages.bib}


\end{document}
