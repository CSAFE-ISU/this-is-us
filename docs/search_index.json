[
["index.html", "This is us: making CSAFE stronger each week Chapter 1 Prerequisites 1.1 Setting up the repository 1.2 Contributions to the repository", " This is us: making CSAFE stronger each week CSAFE 2020-10-26 Chapter 1 Prerequisites This project is a hugely collaborative effort across a large number of individuals. In order to avoid (technical) conflicts between contributions and contributors, we all need to follow a set of guidelines. 1.1 Setting up the repository Git and Github: git has to be installed on your machine (instructions). Create a free Github account at https://github.com/. Most likely you qualify for one of their educational discounts. You can always upgrade your account at a later date, when you have a better idea of what the benefits mean. Become a member of the csafe-isu github organization. For that send email with your github handle (the login you have just created) to hofmann at iastate dot edu. Dr Hofmann will then give you access to the organization. Create a local copy of the this-is-us repository on your machine by cloning it (the green button on the right hand side). Open the RStudio project in the this-is-us folder. I am assuming that you have set up git in Rstudio. Run the command usethis::git_vaccinate in the console. (In case you don’t have the package usethis installed, run the command install.packages(\"usethis\") beforehand.) The git_vaccinate() command creates a file called .gitignore that prevents you from accidentally sharing personal credentials stored in files .DS, .Rproj.user, and .Rhistory. 1.2 Contributions to the repository Everybody is expected to add a summary of their work since the last show-and tell meeting. Think of this write-up as our joint lab book, that is organized by topic rather than chronologically. The mechanism to work with any github repository is pull - commit - pull - push. When you are working collaboratively make sure that you commit and push often to avoid conflicts with your collaborators. pull: once RStudio has been set up properly, you will have an additional tab in your environment pane called Git. The bluish downwards arrow is a shortcut for a pull. Click it and make sure that you don’t get an error message. build: Once everything is pulled, build the book by first selecting the tab Build and then hitting the Build Book hammer. make changes: work in your changes summarizing the work you have done since the last show and tell. Abstain from writing a novel - go for succinct and precise :) We are using Rmarkdown to write. There are plenty of resources for learning/brushing up on your Rmarkdown skills, e.g.: cheatsheet with Rmarkdown commands Rstudio’s tutorial including figures: our work revolves around pattern evidence, so it is quite natural to include figures and graphs. There is a folder called images in the repo, within that each area has a subfolder. Pick the one that best describes your work. Create a folder with your name or your project’s name. All of your figures should live inside that. Use the local path to your images, i.e. I would include my file image.png as images/bullets/heike/image.png. Image files should be either PNGs (png) or JPEGs (extension jpg or jpeg). PDF files as images will not show up in on the websites. build: Make sure that your changes don’t introduce any technical problems. Also spellcheck your work. The html is living online, openly visible. Documents with lots of typos don’t show us at our best. commit: commits are git’s way of saving things. Try to be as specific in your commit message as you can. Heike's changes for the show and tell on May 21 might be a correct message, but are completely void of information two months later, because the commit message is also dated. A commit message of description of general work process for this-is-us repo is much more helpful. check that you have added all the figures or any other new files. Clicking on the checkmark in front of file in the Git panel will change the Status icon to a red A indicating that a file has been added. Be careful with this power, files that have been pushed online can practically never be taken back. So DO NOT ADD DATA before double checking with your PI. Do not add passwords, credit card information, or other personal information. pull-push: move your changes online by pushing. In order to make sure that you incorporate all the changes that collaborators have made since you have pulled your changes last (in 1), update your document with another pull first. Once that has successfully happened, push your changes to the repo by clicking on the greenish upwards arrow in the Git tab. Rinse and repeat 1-8. While it looks like a long shopping list of things to remember, once you have gone through the cycle a few times, you will find that it becomes second nature. Welcome on the CSAFE ship :) knitr::opts_chunk$set(error = T) "],
["intro.html", "Chapter 2 Introduction 2.1 Guidelines for show and tell 2.2 Rmarkdown how-to", " Chapter 2 Introduction This section will become the section for the administrative updates/organization once we have figured out how to use all of the bookdown features for our purposes. The github repo corresponding to these pages is located at https://github.com/CSAFE-ISU/this-is-us 2.1 Guidelines for show and tell A. What are the expectations from a spotlight presenter? Spotlight presentations allow you to go into a bit of detail when presenting your work. Aim for 8-10 mins. Spotlights do not have to be full fledged presentations, but should address: Background of the problem What you have done Obstacles and/or next steps Make sure to include a lot of pictures! Ideally a spotlight presentation sparks a bit of discussion. B. What about the time constraint when there is more than one spotlight? Usually we have 2-3 spotlight presenters scheduled for a show and tell session, so there should be plenty of time for all of you to present. The 8-10 mins are a guideline - sometimes you might need a bit more, sometimes less is fine as well. Talk to the other spotlighters to figure out the order in which you go :) C. Group Updates After spotlighter presentations are done, we go around the group to quickly talk about every one’s work/update. While we are in on-line mode we are not going to require a group update from everyone, but if you have something to share, please do so! D. What if I have to discuss ideas and want to practice for an upcoming big event presentation like a conference, seminar, creative component, or PhD defense etc? Please schedule a talk in the reading/working group which allows for deeper discussions and more time can be spent on different aspects of your research and presentation. You can set expectations from what kind of feedback you are expecting in the reading/working group talk. The working group allows for a longer period of time to not only get valuable feedback but also ensures that other spotlight presenters are not feeling underwhelmed by the response to their work 2.2 Rmarkdown how-to This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 6. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["dummy-file.html", "Chapter 3 Dummy file", " Chapter 3 Dummy file You can make changes and edits in this file as much as you want to - there’s no danger in messing anything up :) jkadsfhjkdsafjkhaskjkajfhjkhafs this is a test test test 123 "],
["bullets.html", "Chapter 4 Project CC: Bullets and Cartridge Cases 4.1 Data Collection 4.2 Computational Tools 4.3 Similarity Scores 4.4 Analysis of Results 4.5 Communication of Results and Methods 4.6 Explainable results: Usability and Trust survey 4.7 People involved", " Chapter 4 Project CC: Bullets and Cartridge Cases For both bullets and cartridge cases we are dealing with several inter-related aspects, that we want to address independently. Those are: data collection computational tools similarity scores for bullet lands: crosscut identification groove location curvature removal alignment of signatures feature extraction matching with trained Random Forest for breech faces analysis of results communication of results and methods 4.1 Data Collection 4.1.1 Breech face images cartridge cases DFSC (about 2000) taken with the Sensofar S-neox Cartridges for CSAFE persistence labelled for barrels 1 through 9. Cadre TopMatch Scanner Some images from Cadre: 4.1.2 Scans from land engraved areas on fired bullets Overview in numbers: scans from bullet lands (about 25,000 total) LAPD: 4 bullets per barrel for 626 firearms LAPD sub-study: 4 bullets per barrel for 92 out of 626 firearms (different ammunition) Variability study: 5 operators x 2 machines x 5 time points x 3 types of bullets x 3 bullets (~450 bullets) Hamby Sets 10, 36, 44, 224, and a clone (35 bullets each) Houston test sets (6 kits with 25 bullets each) Houston persistence: 8 barrels with 40 fired bullets each St Louis persistence: 2 barrels with 192 fired bullets each most of the CSAFE persistence study 4.1.3 LAPD All bullets are collected by Srinivasan Rathinam, LAPD. 4.1.3.1 Main study 4 bullets per barrel for 626 Beretta 92 F/FS firearms , ammunition used are 9 mm Luger Winchester 115 grain with a Copper surface. scans are on Raven. 4.1.3.2 follow-up study 4 bullets per barrel for 96 of the original 626 Beretta firearms using different ammunition bullets are being scanned 4.1.4 Hamby Sets Scans for Hamby Sets 10, 36, 44, and 224 Scans for 3 replicates of clones for Hamby 224 4.1.5 Houston Tests contact: Melissa Nally, Houston FSI 4.1.5.1 Pre-study 3 kits with 23 bullets each Figure 4.1: Bullet-to-bullet similarity scores for questioned bullets (y-axis) compared to all other bullets of the test set (x-axis). evaluation included in submission to JFI 4.1.5.2 Study 4 kits with 20 bullets each scans done, evaluation finished, some scans of doubtful quality 4.1.6 Houston Persistence contact: Melissa Nally, Houston FSI 8 barrels with 40 fired bullets each 4.1.7 St Louis persistence contact: Steve Kramer, St Louis PD 2 barrels with 192 fired bullets each (2 bullets collected every 25 shots) 4.1.8 DFSC Cartridge cases Breech face data for knowns are scanned and available on a private github repository evaluation 4.2 Computational Tools 4.2.1 x3ptools x3ptools is an R package for working with files in x3p format. x3p is an ISO standard for describing 3d topographic surface measurements. x3ptools is available on CRAN, i.e. can be installed with the command install.packages(\"x3ptools\"). The development version is available from github. Installation instructions and basic usage can be found at https://heike.github.io/x3ptools/ 4.2.2 bulletxtrctr bulletxtrctr is a developmental R package available from github (see https://heike.github.io/bulletxtrctr/) that allows an assessment of similarity scores using the data extraction pipeline described in Hare, Hofmann, and Carriquiry (2016). 4.2.3 grooveFinder grooveFinder is a developmental R package providing different methods for identifying the location of grooves in scans of bullets. Installation instructions and some basic usage can be found at https://heike.github.io/grooveFinder/ 4.3 Similarity Scores 4.3.1 Bullet Lands 4.3.1.1 Approaches to identify groove locations 4.3.1.1.1 Hough Transform Method for Identifying Grooves Charlotte 9/5/19 Update: State semester goals and iron out inconsistencies with 2-d and 3-d visualizations due to unit changes. Current Goals: - Iron-out issues with consistency of units with get_hough_grooves. I believe there are some issues translating from the 2-d visualization to the 3-d visualization that might have to do with inconsistent unit inputs? For Example Figure 4.2: 2-dimensional visualization of example bullet br411 with .999 strength threshold Figure 4.3: 3-dimensional visualization of example bullet br411 with .999 strength threshold So either somethin is wrong with get_mask_hough or something is funky with the units. Also need to think of including a sort of rounding component where lines with slopes that are practically infinite can be viewed as a vertical line Compare Hough results with manual identification using score calculations from Kiegan. Write up results in Hough Groove Paper (It’s coming I promise) Create graphical images to explain line selection method Include 2-d and 3-d visualizations of Hough groove area identifications Include crosscut visualization and comparison in results Charlotte update 09/12/19: This week I have been working on obtaining some results for the Phoenix set on Sunny. As a minor update the unit issues in get_mask_hough() are resolved ( I think). Below is an example of a nice image that has been generated using masks. Figure 4.4: Phoenix Gun1 A-9 B1 Land 4 generated at strength threshold of 0.99, initially did not generate estimates at the 0.999 or 0.995 level However the mask is only as good as the Hough estimates that supports it as shown here (less nice). Figure 4.5: Phoenix Gun1 F-6 B2 Land 5 generated at strength threshold of 0.9, initially did not generate estimates at the 0.999 or 0.995, or 0.99 level Hough crosscut predictions for the Phoenix dataset are now uploaded to the bulletQuality Github in the“results” folder and contains Hough groove estimates at the following five strength levels: 0.999, 0.995, 0.99, 0.95, 0.9. The source and the crosscut estimate are also included in the dataset. Here are some preliminary results of using Kiegan’s area of misidentification method (thanks Kiegan!) on Hough groove estimates at the strength threshold of 0.999 in comparison to the BCP and Lasso method. Figure 4.6: Left-hand groove area of misidentification log-transformed scores for BCP, Lasso, and Hough Figure 4.7: Right-hand groove area of misidentification log-transformed scores for BCP, Lasso, and Hough These scoresare log transformed to show better separation but it’s very clear that for the left groove both Lasso and BCP are out performing the Hough method in correctly identifying grooves. For the righthand side, scores tend to be more similar however once again, the Lasso method seems to bo the best job since it has a larger density of low scores and minimizes high score misidenfitications. For improvement before next week, I will investigate why there are 47 missing Hough predictions resulting in a score of 0 in these results and change the parameters in the get_grooves_hough() function to try and generate estimates for some of those missing values. Charlotte update 09/19/2019: This week we are trying to think of a new way for selecting Hough lines for bullet estimates. The previous method for selecting Hough lines was to find lines with x-intercepts at the top and bottom of the lands closest to the lower and upper one sixth of the bullet lands. However this process was highly dependent on score thresholding from the Hough transform which is frustrating when running a large number of bullets since if the right score threshold was not achieved, no result would be produced. So right now I’m working on a way of selecting Hough lines from the normalized Hough scores. To obtain a normalized Hough score I take the x-intercepts of each estimated Hough line generate and find the distance between the x-intercept at the top and the bottom of the land. This should give me the max possible score for each Hough line, rather than calculating based on theta. Then I take the Hough score and divide by this maximum to normalize scores between 0 and 1. Right now I am working on visualizing some of these results but my code is buggy because I’m getting negative values when I try to visualize the process using masks when I shouldn’t. Here is an example of a bullet land using the old and new method. Really similar results although it would appear that the new resut places the Hough transform lines further in to interior of the land than the old results. So that’s promising? Figure 4.8: Phoenix Gun 1-A9 Bullet 3 Land 1 visualized using current Hough process message Figure 4.9: Phoenix Gun 1-A9 Bullet 3 Land 1 visualized using new Hough process message Charlotte Update 09/26/2019: This week is focused on fixing the normalization of the scores for Hough grooves. So that the process can be automatic rather than rely on manual input for the score threshold. Instead of dividing by the geometric distance between the top and bottom intercepts of the bullet image. Now we only consider Hough lines that actually go through both the top and bottom of the land, therefore we can normalize each score by dividing the original hough score by the height of the image and multiplied by the cosine of theta which accounts for the difference in length of lines with differing angles. As far as selecting normalized scores from every score possible I found that there is really no visual difference between selecting the highest normalized Hough score and the other top five. Figure 4.10: Phoenix Gun 1-A9 Land 4 visualized using new Hough process index one Figure 4.11: Phoenix Gun 1-A9 Land 4 visualized using new Hough process index ten Figure 4.12: Phoenix Gun 1-A9 Land 4 visualized using new Hough process index twenty So for now we will continue to select the highest normalized Hough score to use as our bullet land estimates. After fixing the parameterization of the Hough scores and how we normalize Hough scores, the 3-dimensional images appear to have improved! Which is great news since no thresholding was necessary. Figure 4.13: Hamby Bullet 1 Land 1 visualized using new Hough process Still we run into the problem that our masks are only as good as our estimates, however even this terrible bullet land appears to have grooves identified somewhat well. Figure 4.14: Hamby Bullet 1 Land 4 visualized using new Hough process A comparison between the two methods finds that generally the new Hough process out-competes the old one on nearly every bullet land in the Hamby 252 demo set. Figure 4.15: Crosscut Results Hamby 252 Demo Set Comparison between old and new methods Charlotte Update 10/3/2019: The get_grooves_hough function has changed since last week, it previously slopes were calculated in x which is numerically less stable than a slope in y so for example when we were using the old slopes we had the possibility of dividing by zero which is not good. Changing to the new slope helps eliminate that likelihood. Other than that I am working on writing a grooveFinder vignette. I will be discussing every step of the Hough algorithm at length then demonstrating the function itself on the Hamby44 demo set. Charlotte Update 10/3/2019: Finished up the application section of the Hough grooves vignette, need to fill in a few demonstration images that explane how we calculate normalized scores using geometry. Now working on: - Finish visualization portion for the vignette - Expand testing for get_grooves_hough Charlotte Update 10/17/2019: Finally finished up with the vignette, but need to finish one or two more explanation diagrams before first-pass is complete. Having trouble figuring out what the results section should really look like. Need to finish tests for get_grooves_hough. Charlotte Update 10/22/2019: For this weeks spotlight I will focus on motivating the reasoning behind the Hough project, a demonstration of mechanics or how the function actually works and maybe a few results. ** Project Motivation ** One of the main objectives of the bullet project is to develop algorithms that can match bullet lands based on a set of features taken from a signature of a single bullet crosscut. In order to to extract these vital signatures we need to fit a robust loess to our crosscut data to remove the curvature inherent in each bullet land. However, there-in lies a problem. If the groove engraved areas are included in our fitting of the robust loess we observe boundary effects that negatively impact the accuracy of the extracted signature. So a key goal in the bullet project is to be able to automatically identify the location of bullet grooves. Other projects in pursuit of this goal use a statistical approach to calculating the location of bullet grooves over a single crosscut. However, we are given an entire land scan in the form of an x3p file. By using low-level image algorithms like the Hough transform, we can almost make full utility of the x3p scan by estimating bullet grooves over the entire bullet land image instead of a single crosscut. ** Hough Transform Mechanics** Hough transforms are essentially a computer algorithm for detecting imperfect incidences of a known class of shapes in an image by finding aligned points. In our case, grooves are typically linear so we want the Hough transform to detect straight lines. Anyone who has looked at a bullet scan knows that the striae are also straight lines, so some image pre-processing is necessary for the algorithm to be able to distinguish between weaker appearing striae and the prominent groove features. Traditionally a gaussian blur and Canny Edge detection are performed to reduce the noise found in a gradient image. However, we have found that using Canny Edge detection is pretty much unnecessary for identifying grooves. Figure 4.16: Bullet land with Canny Edge detection Figure 4.17: Same bullet land but only with gradient magnitude thresholding at the 99th percentile Utilizing the cleaned up edges in our bullet image, the Hough transform cycles through every pixel of an image in an attempt to find aligned points on an edge. To do so, the Hough transform operates by transforming each point in a line into a different feature space. Figure 4.18: Diagram of detecting aligned points by looking for intersections in the feature space. Source: ‘How Hough Transform Works’- Thales Sehn Körting Unfortunately, vertical lines have slopes in x that tend to infinity, which would make storing the results of the Hough transform impossible due to memory storage issues. So the Hough transform parameterizes lines in what is known as the Hessian Normal Form. \\[ \\rho = x\\ \\cos(\\theta) \\ + \\ y\\ \\sin(\\theta)\\] Figure 4.19: Hessian Normal Form of simple line over bullet image Figure 4.20: Gif of a Hough Transform Algorithm at work. Source: ‘How Hough Transform Works’ - Thales Sehn Körting So the output of the Hough algorithm (in this package we utilize the hough_lines function from the imager package) is thus a set of \\(\\rho\\) and \\(\\theta\\) that define the detected lines but also a “score” which indicates the number of points that the algorithm detected for this particular edge estimation. This allows us to use thresholding and other means to select only the strongest candidates as groove estimates. Previous iterations of the get_hough_grooves function used a user-specified score thresholding level which made results highly variable dependent on the inputted score threshold. Now we use a normalized “score” to select the strongest line detected in the image. Once our lines for the left-hand and right-hand grooves are selected, we choose to output two functions that define our estimated grooves. To compute the parameters of our Hough line, we first find the location of where each line first intersects the bullet (“xtop”) then we use our known “xtop” and our known “height” of the image to calculate “xbottom” using good ol’ SOH CAH TOA. Figure 4.21: Geometrically Calculating ‘xbottom’ The reason for calculating the top and bottom intersection points is so that we can derive a slope for our groove estimate in y. When we learned how to calculate slope in grade school, we were always taught to use “rise over run” which is slope in x. However when the lines are vertical, we are essentially dividing the height of our bullet land by 0 to obtain a slope. So it is numerically more stable to define the slope as \\(\\frac{(\\text{xtop - xbottom})}{\\text{height}}\\) so vertical lines simply have a slope of 0. Implementation As far as implementation goes, the function get_grooves_hough takes care of the edge detection, Hough algorithm, and line selection. Similar to other methods used for detecting lines, the get_grooves_hough function has an adjust parameter that allows the user to specify how far inward they want to “nudge” the groove estimates. The default for the Hough transform is set at 10, however this needs to be experimented with for a variety of different bullets to find appropriate adjust levels. Figure 4.22: 3d visualization of example bullet ** What’s Next? ** Fiddling with adjusts and how it affects score. To find an optimal adjust for the Phoenix set, I calculate the default Hough groove estimates then find what the estimate would be for a series of adjusts. Naively I have defined a new parameter called “difference_left” and “difference_right” which is simply the difference between the manually identified groove location at an optimized crosscut and our Hough estimate at a particular adjust level. For now, I have defined any negative values to indicate that the Hough estimate is further from the center than the identified truth. So we want to minimize these negative difference to better get rid of boundary effects. Figure 4.23: Difference between the left hand Hough estimate and the truth at various adjusts Figure 4.24: Difference between the righ hand Hough estimate and the truth at various adjusts Charlotte Update 10/31/2019: This weeks work has been focused on figuring out why we have such extreme mis-identifications for the adjust in the Phoenix set and work on edits for the vignette. So last week I showed a series of boxplots that show the impact of adjust levels on the difference between the manual identification and the groove estimates from the Hough transform. There were a few significant outliers shown in the boxplot. Upon further investigation it seems as if the Hough transform simply does not find a groove on the right side of the land. Figure 4.25: Adjust level at 100, image of bullet land with a right hand differene of -600 Figure 4.26: What the Hough Transform Sees I wanted to show scores this time around but I just found out from looking through my saved results that the number of observations produced differs for each adjust level and I don’t know why. So there’s some major error with my code that I need to work out before proceeding. Charlotte Update 11/07/2019: This week a major bug was discovered in the code! Still working to look at different adjust impacts on score and now the get_grooves_hough function takes x3ps and dataframes as inputs. Below are the visualized scores for the left and right hand estimates at various adjust levels. Figure 4.27: Phoenix Hough Scores for estimates with a series of 10 different adjusts for the Right Groove Figure 4.28: Phoenix Hough Scores for estimates with a series of 10 different adjusts for the Left Groove What we mightn notice the most from these two graphics is that there are still persistent extreme scores despite the adjusts with areas of identification in the thousands. The most extreme of which for the right side happens to have a score of about 5000. Upon taking a look at the specific lands that have problematic scores they look relatively normal. So then we want to estimate where the Hough transform thinks the grooves are. Figure 4.29: Gun1-U10 Bullet 3 Image Scan Which looks like a pretty regular scan but when we look at the groove estimates we see. Figure 4.30: Gun1-U10 Bullet 3 Crosscut Hough Estimates Which is bizarre because the Right hand estimate is way too far into the bullet land. However we filter out any lines that are within the middle 2/3rds of the bullet scan, and this estimate is clearly within the middle 2/3rds. So we have discovered a new bug in the code, but at least this bug is consistent. Charlotte Update 11/12/2019: This week I’ve dug more into the mysterious bug from last week. It turns out that the “bug” was actually a mis-understanding with regards to unit conversion. So get_grooves_hough is working like it should! But our estimates are still not very good. If we take a look at a single crosscut of a problematic bullet like Gun1-U1-Bullet-3-L1. We’ll see something different from other lands. This land in particular has a much larger width than other lands found in our dataframe. So consequently the heuristic we used in get_grooves_hough which filtered out any lines within the middle two-thirds of the bullet were problematic. Figure 4.31: Crosscut of Gun1-U10-B3-L1 with original Hough heuristic Figure 4.32: Crosscut of Gun1-U10-B3-L1 with middle fifty percent Hough heuristic Unfortunately even though the middle 50 percent heuristic is better suited for this particular land, the groove estimates are still not good. Let’s look at a second problematic case with Fun1-p7-B3-L5. Figure 4.33: Crosscut of Gun1-P7-B3-L5 with original Hough heuristic Figure 4.34: Crosscut of Gun1-P7-B3-L5 with middle fifty percent Hough heuristic Which is a marked improvement. Here is a comparion of the three dimensional lands Figure 4.35: 3D view of Gun1-P7-B3-L5 with original Hough heuristic Figure 4.36: 3D view of Gun1-P7-B3-L5 with original Hough heuristic Charlotte Update 12/5/2019: Short update today due to unforseen circumstances but we have resolved most of the issues with some of our previous problem bullets and are starting to compare to other methods. So the previous heuristic used the middle two/thirds as a way of eliminating lines from consideration. But this lead to a problem with scans that were a much larger size than other iterations of the Pheonix study producing poor results. Figure 4.37: Old Cetner Heuristic Now with an optimized adjust and a middle 50% heuristic we get Figure 4.38: New Cetner Heuristic Which is a much better estimate. Comparing the differences between groove estimates and the manually identified truth for the BCP, Lasso, and Hough methods we receive the following two density plots of differences. Figure 4.39: Left Groove Difference Estimates Figure 4.40: Right Groove Difference Estimates Charlotte Update 02/03/2020: Teaching this semester which has been fun and challenging in a new way. Had an issue with comparing LASSO Results between ones found by myself in the grooveFinder package and results that had been previously saved by Kiegan. Now I’m investigating optimal adjusts for the Hamby44 set and will be working on the summary report. A while back, I ran into an issue where the estimated grooves using the Lasso-full method did not match Kiegan’s previously saved results. It turns out that the lasso function in grooveFinder does not have a built in adjust parameter so the results are different from those create from the bulletxtrctr version. So accounting for the adjust, I re-ran results and obtained something suprising. I know that six additional scans were uploaded to Sunny since the time Kiegan ran the lasso results. However, those were removed for the sake of consistency. Figure 4.41: Left Groove Difference Estimates Figure 4.42: Right Groove Difference Estimates So it leaves the question, why are these results ever so slightly different and which results do we use to compare to Hough even though they’re largely consistent. Other than the LASSO question I have also run the Hough results on the Hamby 44 set to find the optimal adjust. One interesting observation is that there is a set of nine observations where no valid crosscut can be produced. Charlotte Update 03-30-20: Long time no see guys! Hope everyone is well! Last time, the Hough project had kind of stagnated as results fitted with an optimal adjust were not very competitive against other methods. To improve results we have imputed NA values in our bullet land. Where we were Figure 4.43: Left Groove Difference Estimates Figure 4.44: Right Groove Difference Estimates We had just instituted the optimal adjust, as well as the middle 50% heuristic since we had some relatively large bullet scans in the Houston set. However the Hough transform still can’t really compete with the LASSO method which is pretty dissapointing. Where we are Before returning to discussing the Hough Process, we were interested in creating a mask that can pick up striae and give us a better understanding of where grooves are located. We create a rather naive way of finding striae by taking the difference in the x and y direction between each element of the surface matrix of our bullet land. We then determine what is a large enough difference in heights to be coloured in our mask by first finding the distribution of differences between different sets of our bullets. One thing that’s interesting to notice is that the distribution of differences is essentially the same for the Phoenix, Hamby 44, and Houston set of scans. We might expect the Houston scan to have a different distribution because of it’s deep grooves but it appears that this is some sort of scanning effect. Figure 4.45: Distribution of differences in the x-direction for the Phoenix Set Based on this distribution we find that an absolute difference of about .2 would be considered to be a large difference since only about 10 % of all observed differences are more extreme than this threshold. Figure 4.46: Gun 1-A9 Bullet 1 Land 1 Difference Mask So this mask is interesting since it successfully identifies some Striae and breakoff from the bullet scan. But it’s not perfect. Some notable aspects of the grooves are not in fact coloured and on closer inspection this may be due to the presence of NAs. To combat this issue we have decided to impute the NA values by replacing them with 5% greater than the maximum height recorded in the surface matrix. Ideally this will give us a better view of our grooves and maybe some of the breakoff. Figure 4.47: Distribution of differences in the x-direction for the Phoenix Set with imputed NAs For all sets what is considered a large difference has now changed from an absolute difference of .2 to an absolute difference of .15. Figure 4.48: Gun 1-M2 Bullet 2 Land 2 NA-imputed Mask of Differences With these NAs imputed we see a very clear delineation of grooves and breakoff from striae based on the NA. This is a very promising result and we decided to apply the Hough transform process to the imputed lands. Because the imputed NAs so dramatically change the bullet lands, we re-calculated the optimal adjust for this bullet set and then compared the new scores for the NA-imputed bullet lands to the older results. Figure 4.49: Left-Hand groove score comparison between old Hough method and the new method on na-imputed bullet lands Figure 4.50: Right-Hand groove score comparison between old Hough method and the new method on na-imputed bullet lands Charlotte Update 04/20/20: Fairly quick update from me today, I’m not really making new progress on results, I’m mainly just writing up what I have done and finding good and bad examples of Hough estimates. I’m really struggling figuring out what is and isn’t important for Hough background details. Figure 4.51: Right-Hand differences distribution comparison Figure 4.52: Left-Hand differences distribution comparison 4.3.1.1.2 LASSO Method A paper is in preparation for submission to Forensic Science International describing this method (get_grooves_lassofull in grooveFinder), as well as the Bayesian changepoint method (get_grooves_bcp). 4.3.1.1.3 Robust LOESS Method A paper submitted to the Journal of Forensic Science is waiting for peer review response to the first round of revisions. 4.3.1.2 Bullet Land Comparisons Pipeline Most data analysis processes can be thought of as a data analysis “pipeline”. This process can involve data collection, decisions about data cleaning, data transformation or reduction, and feature engineering. For example, consider the general process below: In the case of the bullet project, we have a pipeline which starts with having two physical bullet LEAs and ends with a quantitative result, a random forest similarity score. Our pipeline could be described (roughly) as something like this: To make this a little easier to see, we can look at how a 3D scan is processed into a 2D signature: Now, something important to consider is whether each of these “data decisions” has an impact on the quantitative result (here, a similarity score between two LEA signatures). Consider a simple set of decisions we could make in our bullet pipeline: If we have a pair of signatures, we could theoretically end up with 16 different similarity scores depending on the decisions we make at each point. That is also assuming that both signatures were processed in the same way at each point. This year, I’ll be studying our bullet land “pipeline” here at CSAFE, as well as pipelines that are a little different than ours (e.g., Chu et al. (2010)). There are a few major goals I am working towards: Quantifying the uncertainty of our RF similarity scores based on data decisions Comparing reproducibility/robustness of differing bullet analysis approaches Hare, Hofmann, and Carriquiry (2016) vs. Chu et al. (2010), for example Crosscuts: method 1 vs. alternate? Crosscut parameter tuning? Groove methods Original RF vs. updated/retrained/re-engineering Reproducibility/robustness of different approaches when we consider data COLLECTION. The code in bulletxtrctr is already really well set up as a data “pipeline”, so now we are conceptualizing the best way to wrap the pipeline and keep track of what decisions are made along the way. The current work underway on this project is focused on developing a package in R that will assist users in keeping track of data science projects. We believe it will be particularly useful for those developing a data science process and tools to complete that process. This is most obviously the case for a lot of the ongoing projects at CSAFE; for example: The bulletverse, packages in development include x3ptools, bulletxtrctr, and grooveFinder. Methods are worked on at multiple stages (e.g., groove ID methods, work on modeling bullet scores) Multiple people are on the project and may make changes to things that impact other stages We want to keep track of when something (e.g., a function, model object, parameters) changes that will impact our analyses/scripts. handwriter, a package which processes handwritten documents and extracts data features to be used in modeling If processing code changes, how does that impact the features that are extracted? Can you check entire package for changes to see if you need to re-run processing? ShoeScrubR, shoeprintr packages… etc. Almost all of the methods developed here at CSAFE use non-traditional data that requires processing and feature generation before modeling/analysis can begin. These processes are continually improved as projects move forward, and thus as changes happen we need to be able to assess: The impact of differing processing approaches and how that changes results Whether results need to be re-run after changes are made in a package (time saver!) Confidence in reporting results: how much uncertainty is introduced by manipulating the data in a certain way? Earlier this year, we designed and collected a bullet scanning variability study of 9 bullets. I’m working on formally modeling the variability at the signature level, taking two major approaches: Subsampling and assuming independence; Directly modeling out the mean structure Ignoring peak/valley dependence Using time series/spatial dependence modeling Using a Bayesian shrinkage prior (w/help from Amy!) Results for Method 1, the subsampling, looks something like this: We are also investigating the variability of random forest scores, using pairs of signatures. The current process for taking a set of signatures and completing pairwise comparisons on each of them actually completes many comparisons twice, which has two impacts: It takes up more computational time and memory than we really need it to It has the potential to make our variance component estimates inaccurate - we double-count a bunch of comparisons! Over the summer I made a function to “fix” this, to address the estimation problem in my variability study. The bulletxtrctr pipeline calls for using expand.grid. My new function compares pairs by creating a pairing_id variable and ensuring no pairing_id is duplicated. This is the resulting set of comparisons: The changes to our results are minor, but it is an important detail when modeling things. Two papers in progress! Groove ID paper #1 needs another round of minor revisions (hopefully will be accepted soon!), Groove ID paper #2 is waiting for advisor comments. Sometimes, data collection goes awry… I am in the process of documenting all the data issues and double-checking everything. We are adding more operators (and another set of bullets) to our variability study! I am polishing two full chapters of the book this week. Starting to write up the full variability study (as a dissertation chapter). Writing, writing, writing! Update on data collection: We are in “Phase 2” of data collection, shown below as the purple numbers: March 23, 2020 Spotlight Update: We have completed Phase 2 of Variability Study Data Collection!!! This means I have been doing tons of data processing! Total scans: Barrel Blue (LAPD): 900 Barrel Pink (Houston): 1386 Barrel Orange (Hamby): 1386 Unfortunately, things aren’t perfectly balanced, because we sometimes see things like this: But, we now have variability data for LAPD bullets! So, we are at the point of fitting a model and getting a “final” set of results for publication (/dissertation). Recently, we have been looking at the correct way to model things at the signature level. For a traditional Gauge Repeatability and Reproducibility (Gauge R&amp;R) study, the model would look something like the following: n \\[z_{BLijkmn} = \\mu_{BLi} + \\beta_{j} + \\omega_{k} + \\gamma_{m} + \\beta\\omega_{jk} + \\beta\\gamma_{jm} + \\omega\\gamma_{km} + \\beta\\omega\\gamma_{jkm} + \\epsilon_{BLijkmn}\\] for location \\(i\\), bullet \\(j\\), operator \\(k\\), machine \\(m\\), and repetition \\(n\\). \\(\\mu_{BLi}\\) is a fixed mean for location \\(i\\) on barrel-land \\(BL\\). We assume all other effects are random and are normally distributed. Also note that we are using a subsampling model, so locations are spaced out to reduce location dependence. However, this model often results in singularities. Singularities can happen for several reasons: There is not enough data to estimate parameters The model is overspecified There are not enough grouping levels for a random effect The effects are too close to zero and optimization just pushes them to 0. These reasons are relatively intertwined with one another. For example, a model that is overspecified is often one where there is either not enough data or not enough grouping levels. A singularity essentially means that the matrix has become rank deficient. Due to the fact that our study was specifically designed to estimate effects of a Gauge R&amp;R study, we wanted to mostly maintain the traditional Gauge R&amp;R model framework. We also thought a LOT about what is happening underlying in our model, and decided to include location as an interaction with every other factor. Therefore, we won’t simply have a “bullet effect”, we will have a “bullet by location” effect: \\[z_{BLijkmn} = \\mu + \\alpha_{BLi} + \\alpha\\beta_{BLij} + \\alpha\\omega_{BLik} + \\alpha\\gamma_{BLim} + \\alpha\\beta\\omega_{BLijk} + \\alpha\\beta\\gamma_{BLijm}\\] \\[ + \\alpha\\omega\\gamma_{BLikm} + \\alpha\\beta\\omega\\gamma_{BLijkm} + \\epsilon_{BLijkmn}\\] Where \\(alpha_{BLi}\\) is a fixed location effect, but our other random effects are now split into “effect-by-location” groups. We also some simulation studies on our data with this model in order to understand optimization, singularities, and how our estimates of effects may be affected by singularities that exist in the model. All of the effects in the following image are interacted with location; for simplicity’s sake they have been labelled without location. We simulated data in the following way: Extract fixed means \\(\\mu\\) and \\(\\alpha_{BLi}\\) for a particular land. Identify a set of “target” random effects to see whether we can estimate them. For each run of the simulation, simulate a data set by doing the following: Simulate a set of random effect “true” values for each effect e.g., for \\(\\alpha\\beta_{BLij}\\), the bullet-by-location effect, if we have 20 locations and 3 bullets, we simulate 60 values from a \\(N(0, \\sigma_{BLij})\\) distribution. Each group is then assigned one value from that set of simulated values. Also do this for “residual” error for every single data point. After simulating a group effect for each “data point”, we add the fixed and simulated random effects together to create a simulated data point. Using the simulated data, fit a model with our given specification; save the resulting estimates and whether the model is singular or not. Discussion points: We (I) still need to get a handle on correctly estimating an upper bound or CI for when we have a singular model whose effect is estimated at 0. Model and study design are very important! When effects are small, they are difficult to estimate correctly; we will most likely overestimate them. 4.3.2 Cartridge Cases 4.3.2.1 Congruent Matching Cells (CMC) algorithm for comparing cartridge case breech face impressions Joe 9/5/19 Update: Explanation of missing value problem when calculating cross-correlations and some attempted fixes. Dealing with missing values in the x3p scans continues to be an issue. The Fast Fourier Transform method for calculating cross-correlation can’t handle missing data in an image, so we’ve attempted a few “fixes” that haven’t necessarily turned out as well as expected. One idea we had was to replace the NA values in a cell with the average pixel value. However, this is artificially introducing a signal where before there was none. This can (and demonstrably has) led to inflated/incorrect correlations between cells that shouldn’t have much at all in common. Unfortunately, this may be the only solution if we still wish to adhere to the CMC algorithm as described in Song et al. (2015). One improvement that I’ve implemented is to “crop out” the rows and columns of an image that only contain NAs. This at least means that we’ve weakened the strength of the artificial signal relative to the breechface’s signal. Below is a series of images that illustrate how we might compare a cell in one image to a region of another image. Figure 4.53: Comparing a cell in image 1 to a larger region in image 2. We wish to find the translations of the image 1 cell that yield the highest correlation within the image 2 region. For the sake of an example, let’s focus on the blue outlined cell in image 1. Our goal is to use the image 1 cell to “search” a corresponding larger region in image 2 for the horizontal/vertical translations needed to produce the highest correlation. Below is a zoomed-in version of the blue outlined image 1 cell on the left and the larger image 2 region (approximately: I made the gridded image above by-hand outside of R while the images below are from R). The image 1 cell may look larger than the image 2 region, but we can see from the axes that the image 2 region is indeed larger. Any white pixels in the two images are NA values that need to be dealt with in some way before we can use FFTs to calculate the cross-correlation. Figure 4.54: (Left) A cell from image 1. (Right) A region from image 2 centered in the same location as the image 1 cell, yet quadruple the area. As already discussed above, one “solution” is to replace the NA values with the average pixel value of each image. However, to avoid creating a stronger artificial signal than necessary, we can crop-out the NA rows and columns from the two images above. Below is the cropped version of the two images. The cropping doesn’t produce signficantly different images in this case, but you could imagine other examples in which a cell has captured only small amount of breechface in the corner. Such examples are fairly common and cropping signficantly changes the resulting correlation values. Figure 4.55: The same images as above after cropping NA rows/columns. The last step before calculating correlation for these cells is to replace the remaining NAs with the average pixel value. This is shown below. Figure 4.56: The NA-cropped images with remaining NAs replaced with the image’s average pixel values. The cross-correlation is then calculated between these two images via a standard fast fourier transform process (see Cross-Correlation Theorem). The benefit of using such a process is that (as the name suggests) it’s faster than calculating the raw correlation between the two images. Also, the translations that produce the highest correlation between the image 1 cell and the image 2 region fall out of the calculation for free. This pre-processing/cross-correlation calculation procedure is repeated for every cell in image 1 that contains breech face impression. Because it is not valid to assume that the two images are rotationally aligned by default, we perform the same procedure repeatedly while rotating image 2. Currently, we perform a “rough” grid search of \\(\\theta \\in [-177.5,180]\\) by increments of \\(2.5^{\\circ}\\). Theoretically, the final results tell us how we need to horizontally/vertically translate and rotate the two images to be correctly aligned. 4.3.2.2 Congruent Matching Tori: a promising solution to the missing value problem Joe 9/5/19 Update (cont’d): A brief introduction to a congruent matching “tori” method that may provide a better solution to the missing value problem. As discussed above, dealing with missing values is provign to be a pain. The good news is that the currently-implemented CMC as described above yields results very similar to those published in Song et al. (2015) that originally describes that CMC algorithm. While our results seem to agree with currently published results, it would be nice if we could avoid needing to artifically replace missing values. We can do so if, rather than breaking up the circular breech face impression scans into disjoint squares, we break up the breech face impression into donut-shaped regions containing only breech face impression. Below is an example of such a toroidal region. Figure 4.57: (Left) The original breech face impression scan image. (Right) A donut-shaped region cut out of the original image. By comparing such regions instead of the square cells, we would presumably only need to fill in a few missing value “holes” in the breech face impression scan rather than completely replacing a non-existent signal with an artificial one. In the near-future, I hope to finish up the pre-processing needed for this Congruent Matching Tori method by performing a polar transformation on these images to make them into strips that can easily be compared via an FFT. Joe 9/12/19 Update: Explanation of some of the pre-processing steps needed to make the CMC work as described in Tong et al. (2015) Before carving out toroidal regions from the two images we wish to compare, a fair amount of pre-processing needs to be completed. For example, the scans we work with begin with a considerable amount of auxiliary information, for example the firing pin impression, that we don’t want to use in our comparisons. This isn’t to say that firing pin impressions aren’t useful to determine a match between two cartridge cases. In fact there is quite a lot of published research on how to compare two firing pin impressions. Rather, it is common practice to compare breech face impressions and firing pin impressions separately since it is difficult to scan both simultaneously. Thus, there are regions of a breech face impression scan that we want to remove so that the breech face impressions are more easily comparable. Below is an example of two breech face impression scans before processing. Figure 4.58: Two cartridge case scans before pre-processing. There are a variety of techniques to segment an image into various parts. In image processing, common techniques are the Canny edge detector, which identifies edges of shapes in an image using image gradient techniques, and the Hough Transform, which can detect a variety of geometrical shapes in an image. The Hough Transform is what is used to segment the cartridge case images used in the previous section. However, we’ve found that the use of a Hough Transform doesn’t extract the “breech face signal” from an image as other techniques. Namely, the breech face can be effectively extracted using the RANSAC (Random sample consensus) method that iteratively fits a plane to a set of data until it settles upon a consensus-based “bulk” of the data. In the case of these cartridge case scans, the bulk of the data should predominantely be distributed around the mode height value. That is, the breech face impression. Once we’ve fit this plane to the breech face impression, we can extract the residuals of the fit to better accentuate the markings left in the cartridge case base by a firearm’s breech face. Below is an example of the residuals left after fitting a RANSAC plane to two cartridge case scans above. In the example below, we grab any residuals less than 20 microns in magnitude. Figure 4.59: Residual values of a RANSAC plane fit to the two cartridge case scans shown above. Although these two images are of two different cartridge cases, you can hopefully see that one looks very much like a rotated version of the other. These two cartridge case scans are in fact fired from the same gun (known matches), so it’s a good thing that they look so similar. We’ve now removed quite a bit of the unwanted regions of the original scans. However, there are still some areas of the image (e.g., the faint circular region of pixels in the center of the breech face scan) that just so happened to be close to the fitted plane and thus were brought along in the residual extraction. There are a few ways that we can clean up these last few areas. One is to use two Hough Transforms to detect the inner and outer circles of the breech face impression and filter out any pixels outside of the region between these two circles. The biggest issue with using a Hough Transform is that it must be given the radius of the circle that it is to search for in the image as an argument. That is, we need to know the radius of the breech face impression that we haven’t yet identified in order to identify the breech face impression. Instead, we can dilate/erode (or vice-versa) the pixels in the image to remove the remaining “speckle” in the image. Below is an example of of the breech face impressions cleaned via a dilation/erosion procedure. Figure 4.60: The selected breech face impressions based on dilation and erosion. The final step in the pre-processing is to align the two images in some consistent fashion. Luckily, the firing pin impression ring that’s left after performing the above dilation/erosion provides us with some idea of how to align the breech face impressions. The location of the firing ring impression in the breech face impression provides us with an indicator of where the cartridge case was located relative to the firing pin when it was sitting in the barrel. So aligning two cartridge cases so that their firing pin impression rings align will ensure that, at the very least, the breech face impression left on the cartridge case is horizontally/vertically aligned if not rotationally aligned. Joe 9/18/19 Update: Continuation of pre-process explanation with a discussion on how we can automatically detect the firing pin impression radius in an image. To automatically detect the radius of a given breech face impression, we can count the number of non-NA pixels in each row. If we were to imagine scanning down an image and counting the number of non-NA pixels in each row, then this count would obviously start to increase the moment we hit the top of the breech face impression. Because the breech face impressions are circular, the count would continue to increase the further down the image we scan. That is, until we hit the firing pin impression circle. At this point, because the firing pin impression circle consists of NAs, we would expect the non-NA pixel count to dip. This increasing followed by decreasing behavior in the non-NA pixel count constitutes a local maximum. We can use this local maximum of the non-NA pixel count to identify the beginning of the firing pin impression circle. Similarly, we would expect the non-NA pixel count to reach another local maximum once we hit the end of the firing pin impression circle. It’s then a simple subtraction of the two row indices containing these local maxima to determine an estimate for the diameter of the firing pin impression circle. We can see below an example of the non-NA pixel row sums plotted against the row indices (starting from the top of the image and moving down). You can hopefully see that the raw row sums are rather “noisy”. As such, we can pass a moving average smoother over the row sum values so that the local maxima are easier to identify. This may not be the most robust way to determine the local maxima. I hope to investigate the use of b-splines fit over the row sum values to see if these would be more effective at finding local maxima Figure 4.61: Non-NA pixel row counts and moving average-smoothed row count values plotted against row index. However, because firing pin impression circles have somewhat perforated edges, performing one pass through the image may not yield a particularly accurate estimate. As such, we can repeat the process of finding the distance between local maxima for both the row and column non-NA pixel counts. We can also rotate the image by a few degrees and perform the same process. I am currently rotating the image 0, 15, 30, 45, 60, and 75 degrees and calculating row and column diameter estimates per rotation. Obviously we can apply whatever aggregation function we desire to these estimates to determine a final estimate. Below we see what the Hough Transform selects as the breech face for 4 different radii values. In particular, for circles of radius 210, 213, 216, and 219. Figure 4.62: Hough Transform selected circles (red) of radius (1) 210, (2) 213, (3) 216, and (4) 219. Joe 9/25/18 Update: Dilation and erosion of the breech face impression image seems to be fairly effective, but require some parameter tuning based on the firing pin impression we’re considering (e.g., effective erosion in one image may have a different, adverse effect in another image). The watershed algorithm appears to be a promising alternative to selecting the breech face impression out of an image containing extra “minutiae”. When trying to select the breech face impression out of an image such as the one below (this is a slice of the original scan based on the RANSAC method-selected breech face impression z-value), we’re really just interesting in obtaining a yes/no answer for each pixel to the question: “Are you a part of the breech face impression?” As such, rather than looking at the considering the raw pixel values, we can binarize the image to a 1/0 (equivalently, non-NA/NA) pixel representation. Such a representation is below. Figure 4.63: (Left) Residual values of a RANSAC plane fit to a cartridge case scan. (Right) Binarized non-NA/NA image for segmentation. Using this “indicator image”, the beginning/end of the breech face impression should be much more obvious to, say, a Canny edge detector. Below is the output of such a Canny edge detector. Figure 4.64: The edges of the binarized image above via a Canny edge detector. From here, we can use a Watershed image segmentation procedure to identify various regions within this image. The Watershed algorithm needs to be given a set of pixel locations that the user believes to be within distinct regions of the image. With these “seed” pixels, the algorithm then searches neighboring pixels and attempts to identify them as within/without the same region. Almost as if a water source turned on at the given seed pixel and water began to spread to as many neighboring pixels as it could. The water should “stop” at the black lines in the image above, thus defining the boundary of a seed pixel’s region. An example of the above image post-segmentation is given below. The 5 seed pixels I used were the 4 corners and center of the image. As we can, the watershed algorithm “overflowed” into the breech face impression, but segmented the firing pin impression circle from the rest of the image. Because most of the minutiae that we want to remove is in within this firing pin impression circle, this is not a problem for our purposes. With 5 seed images, there are technically 5 segments represented in the image below (although it’s hard to see where the outer segments begin/end). So as shown below, we can just binarize the segments as being a part of the firing pin impression circle or not. Figure 4.65: (Left) Watershed segmentation of the Canny edge image above. (Right) The firing pin impression circle binarization of the Watershed segmentation image. Finally, now that we’ve identified where the firing pin impression circle is in the original image, we can simply replace any pixel values within this circle with NAs. The final filtered image is shown below. Figure 4.66: Final filtered image. Joe 10/3/18 Update: Determined a fairly computationally intensive yet (seemingly) effective way to find the firing pin impression circle in an image using a grid search of possible radius values. I will now start putting together a package for easy access. I’m not yet sure what to call the package, so any ideas are welcomed We can find a rough estimate for the firing pin radius estimate using a variety of methods. The one that I’ve found to be fairly consistent in the few examples I’ve worked with (detailed in the in the 9/18/19 update) is by counting the number of non-NA pixels in each row/column of the image and identifying the distance between the two largest local maxima in this non-NA count sequence. We can pass a grid of radius values centered on this estimate to a Hough Transform and determine which radius picks out the firing pin impression circle most effectively. The difficulty is in how we quantify “effective” using the output of the Hough Transform. Below you can see the original image including the “minutiae” within the firing pin impression circle that we hope to filter out. You can also see the result of filtering out the firing pin impression circle based on the original radius estimate (210 pixels) obtained from the “local maxima” method. Figure 4.67: (Left) Original breech face impression image. (Right) The breech face impression image after filtering based on a Hough Transform-selected circle of radius 210 pixels. As already discussed, we can test a variety of radius values around the 210 estimate to determine which is best. Below is a gif animating the result of filtering based on a Hough Transform for radius values ranging from 190 to 230. Although a radius of 210 does a decent job of filtering out the minutiae, a slightly smaller radius may be preferred as larger circles tend to cut into the breech face impression. We obviously want to retain as much of the breech face impression as possible for our later analysis. Figure 4.68: Gif showing the result of filtering based on Hough Transform circles of various radii. Using the output of the Hough Transform-selected circles shown above we would like to determine an optimal radius with which to filter out the firing pin impression circle. I explored a few ways of quantifying how “effective” a given radius is at filtering out the firing pin impression minutiae while simulataneously retaining as much of the breech face impression surface as possible. For example, it seemed logical to me to count the number of non-NA pixels we would be throwing out if we filtered based on a particular radius value. As you can see from the gif above, larger radii end up chewing into the breech face impression surface while smaller radii appear to sort of bounce around inside of the firing pin impression circle. We may be able to look at the count of filtered non-NA pixel values for each radius and determine a threshold in which the circles become large enough to start chewing into the breech face impression. Unfortunately, that is not the case. You can see from the plot below on the left that the number of filtered non-NA pixels increased fairly steadily. There isn’t an obvious location along the curve signalling when the circles are getting to be too large (the differences between successive counts are also shown). Since that metric didn’t end up being fruitful, I had to explore alternatives. One alternative that isn’t obvious from just visualizing which pixels are filtered by each radius is called the “Hough score” which essentially quantifies how confident the Hough Transform is that it indeed found the circle that it was told to find in the image. The plot on the right below shows the top Hough scores for each radius value. We can see that there is some variability depending on the radius value. However, there are a range of radius values starting at 210 in which the Hough Transform is consistently rather confident in its circle detection. In fact, we can see from the gif above that radius values between 201 and 206 indeed do a good job of filtering out the firing pin impression circle. Currently, I am basing my final firing pin radius estimate on the radius value in the middle of the longest-running sequence of high-confidence radius values. In both example breech face impressions that I’ve been working with (same type, fired from the same firearm), this final estimate ended up being 203. This is obviously promising, but I would like to spend time to verify that my current method is generalizable to other cartridge case scans. Figure 4.69: (Left) The number of non-NA pixels filtered out by the Hough Transform-selected circles for different radius values. (Right) The Hough score curve used to determine the firing pin radius estimate. Joe 10/10/18 Update: Discuss how the algorithm generalizes to different pairs of cartridge cases. Based on a sample of 5 known-match pairs, it appears that the algorithm does do a good job of deciding on a rotation value to make one breech face impression match up well with the other. Now that the skeleton of the algorithm has, for the most part, been fleshed-out, we can finally start testing it on different pairs of breech face impressions. For the sake of an example, I have 5 known-match breech face impressions shown below. In the state shown, the scans have been pre-processed to the point that we can visualy see when a pair matches. Hopefully, the scans should look to you as if one is just a rotated version of the other. One iteration of the CMC algorithm was already discussed in-detail in the 9/5/19 Update above, so I won’t go into detail about that here (I’m saving it for my Spotlight in November). Instead, we can see a gif that shows which cells from image A and image B we compare when calculating the cross-correlation. Recall that the image A cells are 100x100 and image B cells are 200x200, which is why the cells on the right appear to cover more of the breech face impression than the cells on the left. Figure 4.70: (Left) 100x100 cells from Image A. (Right) 200x200 cells from Image B. As we can clearly see from the 5 pairs above, we need to perform rotations to properly align one with the other. We perform the cross-correlation calculation for 43 different rotation angles (of image B) to determine which rotation angle yields the highest correlation (\\(\\theta \\in [-179.5,180]\\) by \\(2.5^\\circ\\)). However, because we have broken up our images into cells, each cell in image A gets to “vote” for the theta value for which it had the highest correlation with its paired cell in image B. Below, we see the distribution of such theta values (referred to as the “registration angle” in Tong et al. (2015)). The histogram shows that the many of the cells tend to vote for theta values in a relatively small range, which bodes well for us in determining the optimal rotation angle. Figure 4.71: Histogram of the registration angle of highest correlation for each of the 5 pairs of breech face impressions. Since we clearly have a region of popular theta values for each pair, we can perform a finer search around these theta values to arrive at a more precise estimate. The histogram for this finer grid search is shown below. According to Tong et al. (2015), the minimum number of cells that must agree upon a theta value (up to some margin) for two breech face impressions to be called a “match” is 6. We can clearly see from the histogram below that this criterion is met. There are other criteria that Tong et al. discuss including how far we need to shift each cell in image A to achieve the highest correlation with the neighboring cell in image B. Those criteria also seem to be met on the examples I’ve looked at. Figure 4.72: A finer grid search histogram of the registration angle of highest correlation for each of the 5 pairs of breech face impressions. Finally, we can pick the most popular rotation angle for each firearm pair and visually compare how well the two breech face impressions match up. This is done so below. We can see that the algorithm has indeed selected good rotation values for each pair. Continued testing the CMC algorithm on more known match and known non-match pairs of cartridge cases. It’s a time intensive process, but the current results show that the algorithm works for the majority of known match pairs and, most importantly, appear to be qualitatively similar to what is reported in Tong et al. (2015). I’ve continued to run the algorithm on a number of known match pairs of cartridge cases. Although the algorithm seems to work well for the majority of known match pairs, it isn’t perfect at picking the correct rotation angle. I haven’t yet determined the cause of when the algorithm fails to pick the correct rotation angle. Figure 4.73: A pair of known match cartridge cases that start off as rotationally mis-aligned. Figure 4.74: The same pair as above after being correctly aligned via the CMC algorithm. Below is an example of a pair for which the algorithm does a poor job of choosing the correct rotation to align the two images. Figure 4.75: A pair of known match cartridge cases that start off as rotationally mis-aligned. Figure 4.76: The same pair as above after being incorrectly aligned via the CMC algorithm. While running code, I’ve also been working on putting all of my working functions into a package. I should hopefully have something resembling a structured package by my spotlight in November. Finished computing (almost) all 780 possible known match and known non-match comparisons for the 40 cartridge case scans discussed in the Tong paper. We’re running into an issue where the correlations we’re getting out appear to be signficantly lower than what we expect them to be based on the results reported by Tong et al. The biggest challenge is that we effectively need to guess how the images in the Tong paper were pre-processed, so certain decisions we make may drastically affect the final results. We’re going to see if making a few minor changes to the way we pre-process the images will change the results to what we expect. Our current goal is to demonstrate that the current form of the package produces “qualitatively similar” results to those presented by Tong et al. Unfortunately, we don’t actually know which data they used to produce their results. We have a strong suspicion that they just used the first pair of cartridge cases encountered when downloading the study’s data from the NBTRD website, so we’re going to try to base our results comparison based on those. Below we can see the known match cartridge case pair in their raw format before pre-processing. In this state, it’s difficult to make any comparisons between the two breech faces. The first step is to process these images to both remove as much of the non-breech face region of the image as possible and accentuate the breech face impression markings left on the cartridge case. We can see the results of the pre-processing below. It will hopefully look to you as if one of the images is simply a rotated copy of the other. Our goal is to automatically detect what the correct rotation value is to properly align the two images. In order to find the correct rotational value to align the two images, we divide the first image (fadul1-1) into a 7x7 grid of cells. For each cell in image 1, we select a similarly located, wider region in image 2 and calculate the cross-correlation between the image 1 cell and the larger image 2 region. Below is an image that illustrates this for a particular cell. Below is a gif showing an example of cell/region pairs for which the CCF is computed. Figure 4.77: (Left) 100x100 cells from Image 1. (Right) 200x200 cells from Image 2. Once we calculate the CCF for each cell/region pair, we rotate image 2 by a few degrees, say 3 degrees, and repeat the process. We can obviously keep track of the correlation values for each rotation and determine for which rotation values a particular image 1 cell attains its maximum CCF. If two cartridge cases are genuine matches, then we would expect there to be some consensus among the cells for which theta value they attain their max CCF. For example, below we see a histogram of theta values for which the cells in the fadul1-1 attain highest correlation in their associated fadul1-2 regions. We can see a peak around -20 degrees. In particular, the consensus-based theta value turns out to be -21 degrees. If we then consider the CCF values at the -21 degree rotation comparison, we see there are quite a few cells that could be classified as “highly correlated”. Tong et al. discuss various criteria they use to define a cell as a “Congruent Matching Cell.” For example, they set a minimum CCF value of .25. Based on the criteria that they set, we can see in the table below that there are 14 cells that can be defined as CMCs. In their original paper, the number of CMCs they found was 15. The discrepancy likely comes from the fact that they perform different pre-processing steps than we do but don’t discuss what those pre-processing steps are. cell_ID corr dx dy x = 1 - 82,y = 407 - 487 0.4605801 -7 -19 x = 83 - 163,y = 83 - 163 0.3465763 -1 -13 x = 83 - 163,y = 488 - 568 0.2773731 -35 25 x = 164 - 244,y = 488 - 568 0.3917978 -24 2 x = 245 - 326,y = 488 - 568 0.4946205 -17 1 x = 327 - 407,y = 407 - 487 0.4824218 4 2 x = 327 - 407,y = 488 - 568 0.4830941 -17 4 x = 408 - 488,y = 83 - 163 0.4034100 9 -13 x = 408 - 488,y = 164 - 244 0.3274178 4 -14 x = 408 - 488,y = 407 - 487 0.4588278 7 -3 x = 489 - 569,y = 83 - 163 0.5382969 9 7 x = 489 - 569,y = 164 - 244 0.4523592 -31 21 x = 489 - 569,y = 326 - 406 0.5687978 8 16 x = 489 - 569,y = 407 - 487 0.5720020 2 24 We can visualize which cells in fadul1-1 are classified as CMCs. The image below shows the fadul1-1 CMCs as well as fadul1-2 rotated by -21 degrees (the consensus-based theta value chosen from before). We can see that most of the regions with the most obvious visual similarity between the two cartridge cases (in particular, the linear markings in the bottom-right of each image) are indeed classified as CMCs. We are currently running into issues where the correlations values we get out between the two images are not nearly as high as they appear to be in the Tong paper (although we don’t really know since they didn’t provide much context). Below is an example of a pair of known match cells. The two breech faces have already been rotationally aligned (by hand), so these should have very high correlation (apparantely something north of .55 according the the Tong paper). We can certainly see a mode in the correlation (red spot), but it is hard for me to interpret exactly what that mode represents. I believe it represents the shift needed to align the second, larger image with the first image. I need to do some more digging into what is actually happening with FFTs and cross-correlation implementations to know for sure. I’m still currently trying to determine the cause of the correlation issue we’re running into. In doing so, I’ve been making sure that the method by which we are calculating correlation works for simpler examples. I found these examples useful in understanding the strengths/limitations of using a FFT-based approach for calculating cross correlation, so I thought that I would share. For each of these examples, the image on the left is the “reference” image that we want to align the image on the right with. The left image will always be a square image with most pixel values 0 and a square of side length 20 pixels and value 255 in the middle. We want to see how changing characteristics of the right picture affects the correlation values calculated by the CCF. In the first example below, the image on the right just has the box shifted down 30 pixels and right 10 pixels. If we were to overlay the two images with their centers aligned, the correlation “map” that accompanies this pair provides instructions for how to shift the right image to correctly match up with the left image. We see from the location of the largest correlation value (of 1 at (-10,30)) that the CCF indeed detected the correct shift. In the second example, the 0-values pixels in the right image are replaced with randomly generated \\(\\mu = 0\\) and \\(\\sigma = 50\\) white noise. We see that the max correlation value still occurs at the right location, but with a smaller magnitude. We also see that the rest of the correlation map isn’t all 0 as it was in the 0-pixel example above. Lastly, we have an example where the second image contains a smaller square of side-length 10 pixels. We see that, again, the correlation value maximum occurs at the correct location yet with a magnitude well below 1. Joe’s 11/21 Spotlight: Visual Explanation of the “Improved” Congruent Matching Cells Algorithm proposed by Tong et al. (2015). A cartridge case is a type of pre-assembled firearm ammunition packaging a projectile (e.g., bullet, shots, or slug). Below is an image showing different examples of cartridge cases. When a firearm is discharged, the projectile stored in the cartridge case is propelled down the barrel of the firearm. In response, the rest of the cartridge case that remains inside of the firearm is forced towards the back of the barrel (for every action, an equal and opposite reaction). The force with which the remaining cartridge case (hereby referred to as simply “the cartridge case”) is propelled backwards causes it to strike against the back wall of the firearm’s barrel. This area is commonly referred to as the “breech face.” An example of a breech face from a 12 GAUGE, single-shot shotgun is shown below. The hole in the center of the breech face is where the firing pin shoots out to strike the cartridge case primer, which in turn ignites the propellant within the cartridge case causing an explosion that propels the bullet forward down the barrel. As you can see from the image above, there are markings (e.g., manufacturing imperfections) on the breech face. When the cartridge case slams against the breech face during the firing process, these markings can be “stamped” into either the primer of the cartridge case or the cartridge case itself. The markings left on a cartridge case from the firearm’s breech face are called “breech face impressions.” The image below shows the primer of a shotshell fired in the above shotgun. You can clearly see that some impressions were left on the primer during the firing process. The image below shows how the breech face scans come in their raw format. We want to preprocess the images to remove the firing pin impression in the center and accentuate the firing pin impression. Although it is non-trivial to preprocess the images, I am going to skip the explanation of the preprocessing steps so that we have time to focus on how we quantify similarity. The image below shows the result of the preprocessing steps. We could now directly calculate the pixel-wise correlation between these two images. However, there are a few issues with doing so directly. One issue is that we can visually see that the two images are not rotationally aligned, so we wouldn’t expect the correlation to be particularly high. We can fix this by considering a variety of rotations of the images to determine for which rotation the images are the “most aligned.” While it’s not obvious from the images, there is also an issue with the two images being translationally aligned. Lastly, due to the slight differences between the two images, taking the pixel-wise correlation between the entirety of the two images may lead to a deceptively small correlation. This is the motivation behind what Tong et al. (2015) describe as the “Congruent Matching Cells” algorithm. The idea is quite simple. Rather than calculating correlation across entire images, it might be better to break up each image into smaller pieces (“cells”) and calculate correlation for each cell pair. In doing so, a highly correlated pair of cells won’t be drowned-out by less correlated pairs of cells. Tong et al. propose this cell-based method for quantifying similarity between two breech face impressions. For a given pair of images (like the two shown below), the first image is divided into a 7x7 grid. Each cell in this grid is then compared to a region in the second image. Because the two images aren’t assumed to be translationally aligned, the region in the second image will be larger than its associated cell in the first image so that different \\((dx,dy)\\) translation pairs can be considered. Below is a gif showing an example of different pairs of image 1 cells (left) and image 2 regions (right). Figure 4.78: (Left) 100x100 cells from Image 1. (Right) 200x200 cells from Image 2. We measure the similarity between two breech face impressions using the cross-correlation function. The cross-correlation function (CCF) is a measure of similarity of two series as a function of the displacement of one relative to the other. The cross-correlation function for two complex functions \\(f(t)\\) and \\(g(t)\\) for \\(t \\in \\mathbb{R}\\), denoted \\(f \\star g\\), is defined by \\[ (f \\star g)(\\tau) \\equiv \\int_{-\\infty}^\\infty \\overline{f(t)}g(t + \\tau)\\ dt \\] where \\(\\overline{f(t)}\\) is the complex conjugate of \\(f(t)\\). The cross-correlation can be used as a measure of similarity between two different functions. In particular, for \\(f,g\\) real-valued we can interpret the CCF as a measure of common area or “overlap” between the two functions at different values of \\(\\tau\\). The value \\(\\tau\\) at which \\((f \\star g)(\\tau)\\) achieves its maximum can be interpreted as the horizontal translation of \\(g\\) at which \\(f\\) and \\(g\\) are overlapping the most (i.e., are the “most similar” to each other). Below is an animation of two sinusoids that illustrates this concept. We can view the blue sinusoid as being the stationary \\(f\\) function while the red sinusoid is function \\(g\\) that is being translated by different versions of \\(\\tau\\). For each \\(\\tau\\) value, the integral of the product of the two functions (since \\(f\\) is real-valued, \\(\\bar{f} = f\\)) is plotted as the green curve. We can see that the CCF achieves a maximum at \\(\\tau = 0\\). Intuitively, this because the peaks and troughs of the two functions line up perfectly when \\(g\\) is translated by \\(\\tau = 0\\) units, so their product contributes a large, positive amount to the integral. Because we work with discretized representations of cartridge cases, we need to use the discrete analogue of the cross-correlation function. For two discrete functions \\(f\\) and \\(g\\), the CCF is defined to be \\[ (f \\star g)(n) \\equiv \\sum_{m = -\\infty}^\\infty \\overline{f(m)}g(m+n) \\] The images we use are obviously 2-dimensional while the CCF defined above is for 1-dimensional functions. The 2-dimensional extension of the CCF is defined analogously. We can see two examples below of how the 2-dimensional CCF works for simple images. The image immediately below shows an image on the left of mostly 0-valued, black pixels with a 20x20 pixel square of 255-valued, white pixels. The image on the right is the same as the image on the left, but with the square shifted to the right 10 pixels and down 30 pixels. The blue/green/red image below these two shows the correlation “map” that indicates the CCF values for various \\((dx,dy)\\) pairs. If we were to overlay the two images such that their centers aligned with each other, the CCF map tells us how we should shift the second image so that it properly aligns with the first image. As we can see in this simple example, the maximum correlation (of 1) occurs at \\((dx,dy) = (-10,30)\\) indicating that we should move the second image to the left to pixels and up 30 pixels. This demonstrates that the CCF is sensitive to the correct translation values, at least for these simple examples. To show a situation in which the CCF map changes, consider the images below. The image on the left is the same as the example above. However, the image on the right now has WN\\((\\mu = 0,\\sigma = 50)\\) pixels instead of strictly 0-valued pixels. The rectangle in this right image is in the same location as in the example above, yet with an intensity of 100 rather than 255. Lowering the intensity means that we are making the rectangle’s “signal” weaker relative to the surrounding pixels. We can see from the CCF map that this weaker signal indeed affects the correlation values. Although the \\((dx,dy)\\) at which the CCF achieves its maximum is still at the appropriate location \\((-10,30)\\), we see that this max CCF is now only around \\(.4\\). Hopefully the two examples above provide some intuition in how the CCF function can be used to measure properly align two images to be the “most similar.” As already stated, we perform this CCF calculation for each cell in image 1 and associated region in image 2. For each pair, we keep track of the both the value and \\((dx,dy)\\) translation at which the CCF achieves its maximum. We count this as the that cell’s “vote” for how the two images should be shifted to become properly aligned. It’s reasonable to assume that the true \\((dx,dy)\\) translation values should receive many votes from cells, up to some error threshold (e.g., the votes should be within, say, 25 pixels of the true values). Determining the correct rotational alignment isn’t as straightforward. We need to search over a grid of \\(\\theta\\) rotation values and determine the rotation for which the most cells vote. Below is a gif in which we can compare the two breech face scans for various rotations of the second image. We can visually see that the two images are most similar at a rotation value of about \\(-24^{\\circ}\\). The blue region on the right image is meant to illustrate the region that is being compared to to the associated cell in the left image. Below is an example showing the CCF map associated with (left) an image 1 cell and (right) its associated image 2 region. We can see that the CCF hits a maximum of about .5 around \\((25,0)\\). This is, unfortunately, not nearly the magnitude of correlation that we should expect based on the results discussed in Tong et al. (2015). It sounds like the correlation values they were getting from their comparisons were consistently above .55. The CCF maps that we have calculated are rarely above .55. Our hunch is that they are using the CCF to extract the \\((dx,dy)\\) pairs, but then computing the raw, pixel-wise correlation between the two images once they translate appropriately. We can keep track of each cell’s vote for the rotation and translation values at which it achieved its maximum correlation with its associated region in image 2. Again, we would expect that the true rotation and translation values should receive a large number of votes. We can see below a histogram of the theta values voted for by the cells in the left image above. We can a clear mode at a \\(\\theta\\) value slightly less than -20. Based on the gif of the rotating breech face above, it appears that the automatic CCF-based “votes” agree with what we can visually see to be the best rotation value to align the two images. Based on the histogram above, the algorithm has detected the correct rotation value to align the two images. If we were to rotationally align the two images based on this value, we can count the number of pairs that are “highly similar” with each other. Tong et al. discuss various criteria to use to define “highly similar” including having a minimum CCF value (.55 in their paper). They also require that the \\((dx,dy)\\) translation values that a particular pair voted for must be within some number of pixels of the “majority-voted” translation values (they propose a maximum of 25 pixels away from the median translation values). All of this seems rather arbitrary, so it would be interesting to explore if alternative criteria would be more effective at differentiating between matches and non-matches. Joe 12/5 Update: I believe we’ve finally cracked the issue that we’ve been running into the past few weeks of our correlations not being as high as those reported in the Tong paper. The fix is that we can first use the FFT-based method of calculating the CCF to obtain the correct dx,dy translation pairs and then calculate the raw correlation between the two images once we translate them appropriately. Before, we were using the correlation given to us by the FFT-based CCF method, which were most often deflated relative to the true correlation (due to replacing missing values with 0). Below, I have three examples showing the raw correlation values between pairs of cells. Any pair with a correlation above .55 we would call “highly correlated” under the Tong paper’s criteria. The first example below shows a best-case scenario in which we have two images containing a lot of observed values and those observed values have a fairly high correlation (high enough to meet the arbitrary .55 threshold, at least). However, the second example shows a not-so-great scenario in which we don’t have a lot of observed values in the two images, but they are still highly correlated. We should be less confident that such examples indicate that the overall breech face impressions are “true matches” than we are in the example above, but the Tong paper doesn’t seem to make such a distinction. Lastly, I have an example of two images that certainly have many observed value that visually look like they should be fairly highly correlated. However, we can see by the raw correlation value that they, in fact, wouldn’t pass the threshold to be “highly correlated.” As such, we wouldn’t include them when we count the number of highly correlated pairs between the two images. I wanted this example to demonstrate that our eyes don’t always necessarily pick up on subtleties as strongly as a computer might. Joe 12/12 Update: I have been trying to implement the Tong paper’s criteria for calling two images “matches” and apply these criteria to all of the known match and known non-match image pairs. I have results to share regarding the number of CMCs counted for each known-match cell pair (the known non-matches take much longer to run). To recap, the CMC algorithm attempts to match two breech face impression images by breaking the image up into “cells” and counting the number of cells that are highly similar between the two images. So for each pair of images, we get a congruent matching cell count indicating how similar the overall images are. For the Fadul data that the Tong paper uses, there are 63 known match pairs. The bar chart on the left shows the various CMC counts for these 63 known match pairs as determined by our implementation in the algorithm. In contrast, the CMC count distribution as reported in the Tong paper is on the right (known matches in gold). We can see that our histogram is qualititatively similar to the gold distribution on the right. There are two pairs that seem to be giving us trouble in the histogram above (we expect all of the known match pairs to have a large number of “highly similar” cells). Below shows the se two pairs. Hopefully it’s clear to see that the first pair, while known matches, look fairly different. There are many regions in the image on the left that are missing from the image on the right. These differences possibly provide some justification as to why the current implementation of the algorithm fails to identify these two images as matches. The troubling example is below. It’s visually obvious that these two images are very similar to each other. However, the algorithm does not identify these as matches. Our current hunch is that the way we preprocess the images has a dramatic effect on the results we ultimately observe. We have a few ideas to tweak the preprocessing to hopefully fix such issues. Unfortunately, the Tong paper never describes how they preprocess the 3D scans, so we have no way of truly replicating their results. All we can hope for is qualitatively similar results, which appears to be the case for the most part. Joe 1/17 Update: Broad overview of project. A cartridge case is a type of pre-assembled firearm ammunition packaging a projectile (e.g., bullet, shots, or slug). Below is an image showing different examples of cartridge cases. Figure 4.79: Various cartridge cases When a firearm is discharged, the projectile stored in the cartridge case is propelled down the barrel of the firearm. In response, the rest of the cartridge case that remains inside of the firearm is forced towards the back of the barrel (for every action, an equal and opposite reaction). The force with which the remaining cartridge case (hereby referred to as simply “the cartridge case”) is propelled backwards causes it to strike against the back wall of the firearm’s barrel. This area is commonly referred to as the “breech face.” An example of a breech face from a 12 GAUGE, single-shot shotgun is shown below. Figure 4.80: Breech face of a shotgun The hole in the center of the breech face is where the firing pin shoots out to strike the cartridge case primer, which in turn ignites the propellant within the cartridge case causing an explosion that propels the bullet forward down the barrel. As you can see from the image above, there are markings (e.g., manufacturing imperfections) on the breech face. When the cartridge case slams against the breech face during the firing process, these markings can be “stamped” into either the primer of the cartridge case or the cartridge case itself. The markings left on a cartridge case from the firearm’s breech face are called “breech face impressions.” The image below shows the primer of a shotshell fired in the above shotgun. You can clearly see that some impressions were left on the primer during the firing process. Figure 4.81: Breech face impression on a cartridge case The image below shows how the breech face scans come in their raw format. We want to preprocess the images to remove the firing pin impression in the center and accentuate the firing pin impression. Figure 4.82: A pair of known match cartridge case scans Tong et al. propose this cell-based method for quantifying similarity between two breech face impressions. For a given pair of images (like the two shown below), the first image is divided into a 7x7 grid. Each cell in this grid is then compared to a region in the second image. Because the two images aren’t assumed to be translationally aligned, the region in the second image will be larger than its associated cell in the first image so that different \\((dx,dy)\\) translation pairs can be considered. Below is a gif showing an example of different pairs of image 1 cells (left) and image 2 regions (right). Figure 4.83: (Left) 100x100 cells from Image 1. (Right) 200x200 cells from Image 2. The two-dimensional CCF is used to quantify similarity between two breech face impressions. Below are two examples illustrating how to interpret the two-dimensional CCF. Below is an example showing the CCF map associated with (left) an image 1 cell and (right) its associated image 2 region. We can see that the CCF hits a maximum of about .5 around \\((25,0)\\). This is, unfortunately, not nearly the magnitude of correlation that we should expect based on the results discussed in Tong et al. (2015). It sounds like the correlation values they were getting from their comparisons were consistently above .55. The CCF maps that we have calculated are rarely above .55. Our hunch is that they are using the CCF to extract the \\((dx,dy)\\) pairs, but then computing the raw, pixel-wise correlation between the two images once they translate appropriately. The image below shows how the smaller breech face impression image aligns with the larger breech face impression image after being translated by the CCF-detected \\((dx,dy)\\) values. The animation below shows how the two breech face impressions compare for various rotations of the second breech face impression. We can keep track of each cell’s vote for the rotation and translation values at which it achieved its maximum correlation with its associated region in image 2. Again, we would expect that the true rotation and translation values should receive a large number of votes. We can see below a histogram of the theta values voted for by the cells in the left image above. We can a clear mode at a \\(\\theta\\) value slightly less than -20. Based on the gif of the rotating breech face above, it appears that the automatic CCF-based “votes” agree with what we can visually see to be the best rotation value to align the two images. Joe 1/27 Update: I’ve been starting to write up an article that we hope to submit to the R Journal to introduce the MARCC (Matching Algorithms in R for Cartridge Cases) package (name pending). It’s been rather time intensive, but I’ve also been re-running the algorithm with different pre-processing steps to determine how sensitive the final results are to how we pre-process the images. It seems like they are rather sensitive to the pre-processing procedure used. I thought I would share a few of the CMC distributions made for different pre-processing steps. These pre-processing steps include extracting the breech face impression region from the rest of the image, which may be done by-hand (Tong et al. (2014/15) may do this), using the RANSAC method (Xiao Hui and I have been doing this), or using a 2nd order robust Gaussian regression filter (Chen et al. (2017) does this). Further complications come in the form of how these methods are implemented across various MATLAB and R functions (I have yet to find a Gaussian Regression filter implemented anywhere in R or MATLAB) and whether they provide the same results. After selecting the breech face impression region from the scan, no paper I have come across details how they then resize their images to their desired dimension. Tong et al. resizes to 700x700 in 2014 and 560x560 in 2015. Chen et al. uses resizes to 600x600 in 2017. As discussed below, the resizing method (nearest-neighbors, linear, lanczos) does seem to affect the final results. To recap, we are supposed to call two cells a “match” (or “congruent matching cells”) if they pass a few criteria. These criteria are based in part on whether their associated translation and rotation “votes” are close to the consensus-based votes across all cells in the image. The motivation here being that we would expect the cells in a pair of truly matching cartridge cases to all vote for approximately the same translation/rotation values to achieve maximum similarity. Another criterion is that the cell pair must reach some minimum correlation cut-off (all of these criteria vary depending on the paper). Each of the following plots were subject to the same criteria and are based on the same data, so they should be the same plot. As we can clearly see, there is certainly some variability in how the CMC counts are distributed. This is an example from Chen et al. (2017) that the rest of the plots are trying to emulate. This plot was made using reportedly images downsampled to size 600x600 and an 8x8 grid of cells. Again, it’s unclear how this downsampling was performed since the raw surface data often have dimensions near 600x600, but never perfectly 600x600. The plots following include various resizing/padding techniques used to try to get this nominal 600x600 image size. This is a plot based on images that were resized to 560x560 (the image size specified by Tong et al. (2015)) using linear interpolation. This is a plot based on images that were resized to 600x600 (the image size specified by Chen et al. (2017)) using the x3ptools::resize_x3p() function followed by padding the image with NAs until it reaches a size of 600x600. This final distribution is the most promising of the ones I’ve made. This distribution is also based on images that were resized using x3ptools::resize_x3p(). However, rather than padding the exterior of the image with NA rows/cols until it achieves the nominal 600x600 size, the image is first split into an 8x8 grid of cells and each cell is individually padded until it reaches dimension 75x75. Joe 2/3 Update: I’ve been working on getting a Gaussian bandpass filter implemented for the cartridge cases in an attempt to match the results reported by Chen et al. (2017). I’ve also been writing more of the journal submission while code runs. Below is an image of black (0 intensity) grayscale image to which a low-frequency, 255 intensity box and high-frequency Gaussian noise has been added. Below is a 3D rendering of the same image. A “low pass” Gaussian filter can be interpreted as reducing the strength of high frequency signals in the image. In the image above, the Gaussian noise is higher in frequency than the white box, so it can be reduced without drastically affecting the box. In the spatial domain, we can think of a moving average as performing a similar operation as a low pass filter. Conversely, a “high pass” Gaussian filter reduces the strength of low frequency (longer period) signals and emphasizing high frequency signals. We can see in the image below that much of the white box has been removed from the image below by passing a high pass filter since the much of the white box can be explained with large period sinusoids. Only the edges, which would need to be explained with higher frequency sinusoids, remain after passing the high pass filter. The Gaussian noise also remains. Joe 2/10 Spotlight: A further explanation of how I have been pre-processing the images and how those steps affect the CCF values. I have been spending the last few weeks tweaking the pre-processing procedures in order to perform a “sensitivity analysis” of the final results given a variety of initial conditions. This has required me to look deeper into various image processing techniques that has taken up a surprising amount of time. I have also been trying to pull together the various threads of the project into a cohesive manuscript and R package. I’m planning on making this project my CC and hoping to present it in late March/early April. I’ve spent a considerable amount of this project, possibly more than half, agonizing over various image processing techniques. We had a working “prototype” of the CMC working as far back as October of last year. However, it quickly became evident that our results didn’t quite match the results reported by authors at NIST. This realization caused us to go down a variety of rabbit holes, some of which we’re still in, to determine the cause of these differing results. The biggest obstacle that we’ve faced is that we don’t quite know how the images were processed before they were used in the CMC method; and it doesn’t appear that NIST is willing to share much of their information (although Susan is currently trying to get them to share). There are a variety of decisions that need to be made when pre-processing the images that aren’t discussed in sufficient detail in any papers on the CMC method. So it has been up to us to test various combinations of these decisions to determine how they affect the final results. Because I have been spending so much time the past few weeks (months?) exploring various pre-processing techniques, I’m hoping to share with you all some of what I have learned. I’ve never really discussed the pre-processing procedure(s) in-full, so I hope to use this space as an opportunity to pull together various threads that I can hopefully use for my CC. When we say that we need to “pre-process” the cartridge case scans, we mean that there are regions of the scan that aren’t useful to us that we want to remove. In particular, any regions that don’t come into contact with the breech face of the firearm barrel aren’t going to be informative. The image below shows left-hand side and top-down views of a cartridge case. The regions we want to remove outright from the scan are highlighted in red in the image below. The red circe in the middle is caused by the firing pin pushing into the cartridge case primer, causing some of the metal to be pushed to the side. This region does not come into contact with the breech face, but instead goes inside of the firing pin hole. The cluster of points in the corners of the image are from the machine with which the scan was taken. Removing these regions by hand would be straightforward. The issue comes from trying to come up with automatic methods for identifying and removing these regions for an arbitrary scan. The first goal is to identify at which height value the breech face impression is in the scan. We currently use the RANSAC (RAndom SAmple Consensus) method to accomplish this. For our purposes, RANSAC iteratively fits a number of “candidate” planes to the cartridge case scan based on randomly selected points. At each iteration, the method determines whether a newly proposed candidate plane contains more “inliers” (defined as points within some threshold distance of fitted plane) than the previously proposed candidate planes. If so, then the method chooses this new plane as the current “consensus.” After a set number of iterations, the last consensually-selected plane is returned. The picture below provides a 2-dimensional illustration of the method (Source). Once the RANSAC plane is fit, we have two options. We can either take the residuals between the fitted plane and the observed breech face impression values or simply extract from the original scan the observed values within some specified distance of the fitted plane. Assuming that the cartridge case scans aren’t tilted off the horizontal, these should yield similar results. However, we have some evidence to suggest that the planes are slightly tilted. The pair of images below illustrates this point. The image on the left represents the raw, observed breech values while the image on the right contains the residuals. The grayscale intensity of the images reflect the height values. We can see that in the image on the left that there is a general trend from darker to lighter pixels (i.e, height values go from low to high) from the bottom-left corner to the top-right corner. The image on the right, on the other hand, does not contain this trend since it contains residual values. It should be noted that, although we can see a trend, this trend may be exaggerated since the pixel intensities are colored on a scale relative to the values in the image. That is, although there is a clear trend, this trend may not actually be that extreme when viewing the raw pixel values. We have yet to determine whether fixing this tilt affects the results in any major way, but, to be safe, we have mostly been using the residuals rather than the raw, observed breech face impression values. It should be noted that because the RANSAC method is dependent on randomly selected points in the scan, the final consensually-selected plane may differ in size/values between different applications of the method. However, we’ve observed this difference to be only slight across the many times we’ve applied the method. You’ll notice in the above image that we still haven’t removed all of the observed values inside nor outside the breech face impression. These remaining, unwanted pixels happen to have the same height value as the breech face impression pixels. Removing the outer pixels simply requires cropping the image down such that the breech face impression pixels touch the edge of the image. Removing the inner pixels is slightly harder and was the focus of a few weeks of work last semester. The method we implemented to detect where these pixels are in the image involves first estimating the radius of the inner circle and then using a Hough Transform to detect where that circle is in the image. For the sake of wanting to discuss other pre-processing more in-depth, I’m going to skip the details here. The additionally cropped/filtered image using the RANSAC residuals is shown below. The next pre-processing step involves passing a Gaussian filter over the image to improve the “signal-to-noise” ratio. For our purposes, the “signal” that we want to identify are the impression markings left by the breech face onto the cartridge case. “Noise” includes possible measurement error and larger structure on the cartridge case that isn’t an artifact of contact with the breech face. The Gaussian filtering is performed in the frequency domain using the Convolution Theorem. Gaussian Filter Intuition Below is an image of black (0 intensity) grayscale image to which a low-frequency, 255 intensity box and high-frequency Gaussian noise has been added. Below shows a 3D rendering of the same image. A “low pass” Gaussian filter can be interpreted as reducing the strength of high frequency signals in the image. In the image above, the Gaussian noise is higher in frequency than the white box, so it can be reduced without completely removing the box, although the box will be affected. In the spatial domain, we can think of a weighted moving average as performing a similar operation as a low pass filter. Conversely, a “high pass” Gaussian filter reduces the strength of low frequency (longer period) signals and emphasizing high frequency signals. We can see in the image below that much of the white box has been removed from the image below by passing a high pass filter since the much of the white box can be explained with large period sinusoids. Only the edges, which would need to be explained with higher frequency sinusoids, remain after passing the high pass filter. The Gaussian noise also remains. A Gaussian bandpass filter can be used to reduce signals that are either too low or too high in frequency - i.e., what we have deemed to be noise. Below we can see the processed RANSAC residuals after filtering using a Gaussian bandpass. I’ve implemented the Gaussian bandpass by-hand since effectively no package We can see that the “high-frequency” markings that were in the top-right quarter of the original RANSAC residuals have been visually reduced. Increasing the high-pass threshold of the Gaussian filter would mean allowing more of the high-frequency signals to be unaffected. A balance must be struck so that enough of the “true signal” remains in the filtered image while simulatenously reducing the effects of random noise. All of this pre-processing can be done while keeping the original resolution of the surface matrix. For the Fadul (2011) sets, this is 3.125 microns/pixel. The literature available commonly describes downsizing a breech face impression to speed up computational time. However, no direction is given into how this downsampling happens. For example, we could downsample simply by taking every every other row/column in an image. This is the method implemented in the x3ptools::sample_x3p function. On the other hand, a large number of downsampling interpolation techniques exist that take weighted averages in neighborhoods of points to determine the values in a downsampled image. Many of these interpolation methods (nearest neighbor, linear, cubic, lanczos) are implemented in the imager::resize function. However, imager is effectively an R frontend to the C library CImg, so the way in which these methods are actually implemented may be difficult to track down. Sensitivity Analysis Example To illustrate some of the sensitivity of the final results to the pre-processing steps, we’ll consider an example in which we have 3 cartridge case scans. We’ll consider two of them “known” and one of them to be “questioned.” We’ll consider combinations of two different decisions: 1) take the RANSAC residuals or the raw breech face impression values and 2) pass a gaussian bandpass filter over the images or not. In each of these examples, the questioned cartridge case is in the top left, its (known) match is in the top right, a (known) non-match is in the bottom left, and the distribution of maximum CCF values per cell comparing the questioned cartridge case to the two known cartridge cases is in the bottom right. Below we can see the results if the RANSAC residuals were taken and then bandpass filtered. We can see considerable overlap between the two distributions. Below we can see the results if the raw breech face values were taken and then bandpass filtered. Again, considerable overlap. Below we can see the results if the raw breech face values were taken, but no bandpass filter was applied. One could argue that the CCF values are slightly higher than in the bandpass cases, but there is still considerable overlap. Startilingly, the non-bandpassed, raw values yield the highest CCF values of these 4 combinations. However, it’s important to point out that the known match images very clearly have trend that starts low in the bottom left corner and increases to the top right corner. The non-match has a completely different trend. As we can see from the non-bandpassed, residual CCF histogram above, it appears that this global difference does have a fairly drastic effect on the final results. From what I have read, no author recognizes this global trend as a possible reason for the why matches tend to be more similar than the non-matches. Joe 2/17 Update: Not too much to report today. I’ve mainly working on the cmcR package and writing. Joe 2/24/20 Update: Not too much to update for today. I’ve been working on writing the package and associated RJournal submission. I have a few functions that I am still planning on implementing in the package, but I will for the most part be getting the package CRAN-ready. I’m planning on building upon the journal submission for my CC that I’ll be presenting in April, so I will soon begin writing that as well. Joe 3/2/20 Update: I spent a considerable amount of time writing my CC this week. I essentially only have the Literature Review and Results/Conclusions sections to write up before the initial draft is complete. I’ve also been working on pulling together results to put in the results section, but performing the entire CMC procedure on around 800 cartridge cases takes about 6 hours (I’ll try to parallelize better to cut this time down on the server). Joe 3/9/20: Still more CC writing, pulling together results for the Results/Conclusion section, and tweaking functions in the cmcR package. We’re seeing more promising results than we have in the past, but there are still avenues to explore. I’m looking forward to sharing these results and the cmcR package at my Spotlight the Monday after spring break! Joe 3/23/20 Spotlight: I’ve been spending a considerable amount of time putting the finishing touches on the R package cmcR. The core of the package is currently done, but I’m hoping to add a few more ancillary functions, improve the documentation, and write tests. I was hoping to spend some time during the spotlight showing off some of the package functionality and getting feedback. We are also now getting the results close to we’ve hoping for. I’m looking forward to sharing some of those results during the spotlight as well. I’ve pushed unevaluated code chunks to the this-is-us because it would otherwise require cartridge case scan data that many probably don’t have locally on their computer. See below for a link of how to access the cartridge case data To install the cmcR package from github, run the following command in your R console. devtools::install_github(&quot;https://github.com/jzemmels/cmcR&quot;) For this use case, suppose we have two known match cartridge cases (km1 and km2) and two of unknown source (unknown1 and unknown2). Let’s pretend that we don’t see the file path clearing indicating which of the unknown source cartridge cases match the known source. These cartridge case data came from a study done by Fadul et al. (2011) and are accessible here. km1_raw &lt;- x3ptools::read_x3p(&quot;~/bulletCartridgeScans/fadul_allScans/Fadul_1/cc/Fadul 1-1.x3p&quot;) km2_raw &lt;- x3ptools::read_x3p(&quot;~/bulletCartridgeScans/fadul_allScans/Fadul_1/cc/Fadul 1-2.x3p&quot;) unknown1_raw &lt;- x3ptools::read_x3p(&quot;~/bulletCartridgeScans/fadul_allScans/Fadul_2/cc/Fadul 2-1.x3p&quot;) unknown2_raw &lt;- x3ptools::read_x3p(&quot;~/bulletCartridgeScans/fadul_allScans/Fadul_1/cc/Fadul F.x3p&quot;) Cartridge case visualization: Visualizing cartridge case data is useful for determining, for example, how various pre-processing methods affect the final estimation of the breech face impressions. There are a number of ways to visualize cartridge case data, each with their own strengths and weaknesses. Imager with Base Plot: A quick yet inflexible way is to turn the surface matrix into a cimg object and use base plot. par(mar = c(1,1,1,1)) imager::imlist( km1_raw$surface.matrix %&gt;% imager::as.cimg(), km2_raw$surface.matrix %&gt;% imager::as.cimg(), unknown1_raw$surface.matrix %&gt;% imager::as.cimg(), unknown2_raw$surface.matrix %&gt;% imager::as.cimg() ) %&gt;% plot() km1_raw$surface.matrix %&gt;% imager::as.cimg() %&gt;% as.data.frame() %&gt;% ggplot(aes(x = x,y = y)) + geom_raster(aes(fill = value)) + scale_fill_gradient2(low = &quot;grey0&quot;, mid = &quot;grey50&quot;, high = &quot;grey100&quot;, na.value = &quot;white&quot;, midpoint = median(as.vector(median(km1_raw$surface.matrix, na.rm = TRUE)))) + coord_fixed(expand = FALSE) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = &quot;none&quot;, axis.title.x = element_blank(), axis.title.y = element_blank()) x3ptools::image_x3p: The x3ptools::image_x3p function provides 3D visualization functionality, but requires opening an RGL device on an individual machine (can’t be viewed in an R Markdown file without making it huge) km1_raw %&gt;% x3ptools::image_x3p(zoom = .7) Pre-processing: If we want to compare these cartridge case scans based on their breech face impressions, they need to undergo some pre-processing. The cmcR package has the modularized family of preProcess_ functions or the all-in-one (except Gaussian filtering) selectBFImpression_ functions to accomplish this. Below we can see how the preProcess_ family of functions can be used to process a cartridge case scan. The RANSAC method is applied twice for a more precise estimate of the breech face impression height value. Then the whitespace (all NA-valued rows/columns) on the exterior of the breech face impression is cropped. Next the firing pin circle is automatically estimated and any pixels within this circle are filtered out the scan. Lastly, a lowpass Gaussian filter meant to reduce the effect of high frequency “noise” in the scan is applied. set.seed(3152020) #RANSAC method is based on randomly sampling points in matrix km1_residuals &lt;- km1_raw %&gt;% x3ptools::sample_x3p(m = 2) #down-sampled copy of km1_raw km1_residuals$surface.matrix &lt;- km1_residuals$surface.matrix %&gt;% preProcess_ransac(inlierTreshold = 10^(-5), finalSelectionThreshold = 2*(10^(-5)), iters = 150) %&gt;% preProcess_levelBF(useResiduals = TRUE) %&gt;% preProcess_ransac(inlierTreshold = .1*(10^(-5)), finalSelectionThreshold = 2*(10^(-5)), iters = 300) %&gt;% preProcess_levelBF(useResiduals = TRUE) km1_residuals$surface.matrix %&gt;% imager::as.cimg() %&gt;% plot() km1_residuals$surface.matrix &lt;- km1_residuals$surface.matrix %&gt;% preProcess_cropWS(croppingThresh = 2) %&gt;% preProcess_removeFPCircle(aggregation_function = mean, smootherSize = 2*round((.1*nrow(.)/2)) + 1, meshSize = 1, houghScoreQuant = .9) km1_residuals$surface.matrix %&gt;% imager::as.cimg() %&gt;% plot() km1_residuals$surface.matrix &lt;- km1_residuals$surface.matrix %&gt;% preProcess_gaussFilter(res = km1_residuals$header.info$incrementY, wavelength = 16, filtertype = &quot;lp&quot;) km1_residuals$surface.matrix %&gt;% imager::as.cimg() %&gt;% plot() The selectBFImpression_ functions perform all of the pre-processing steps except for the Gaussian filtering in one function call. The following will process the 3 other scans in an equivalent manner to km1_residuals above. Note that the selectBFImpression_ functions actually returns a list of two elements: params containing the list of parameters under which the function call was made (for reproducibility purposes) and x3p containing the actual processed x3p object. km2_residuals &lt;- selectBFImpression_sample_x3p(x3p_path = &quot;~/bulletCartridgeScans/fadul_allScans/Fadul_1/cc/Fadul 1-2.x3p&quot;, ransacIters = 150, ransacInlierThresh = 10^-5, ransacFinalSelectThresh = 2*(10^-5), useResiduals = TRUE) unknown1_residuals &lt;- selectBFImpression_sample_x3p(x3p_path = &quot;~/bulletCartridgeScans/fadul_allScans/Fadul_2/cc/Fadul 2-1.x3p&quot;, ransacIters = 150, ransacInlierThresh = 10^-5, ransacFinalSelectThresh = 2*(10^-5), useResiduals = TRUE) unknown2_residuals &lt;- selectBFImpression_sample_x3p(x3p_path = &quot;~/bulletCartridgeScans/fadul_allScans/Fadul_1/cc/Fadul F.x3p&quot;, ransacIters = 150, ransacInlierThresh = 10^-5, ransacFinalSelectThresh = 2*(10^-5), useResiduals = TRUE) km2_residuals$x3p$surface.matrix &lt;- cmcR::preProcess_gaussFilter(surfaceMat = km2_residuals$x3p$surface.matrix, res = km2_residuals$x3p$header.info$incrementY, wavelength = 16, filtertype = &quot;lp&quot;) unknown1_residuals$x3p$surface.matrix &lt;- cmcR::preProcess_gaussFilter(surfaceMat = unknown1_residuals$x3p$surface.matrix, res = unknown1_residuals$x3p$header.info$incrementY, wavelength = 16, filtertype = &quot;lp&quot;) unknown2_residuals$x3p$surface.matrix &lt;- cmcR::preProcess_gaussFilter(surfaceMat = unknown2_residuals$x3p$surface.matrix, res = unknown2_residuals$x3p$header.info$incrementY, wavelength = 16, filtertype = &quot;lp&quot;) par(mar = c(1,1,1,1)) imager::imlist( km1_residuals$surface.matrix %&gt;% imager::as.cimg(), km2_residuals$x3p$surface.matrix %&gt;% imager::as.cimg(), unknown1_residuals$x3p$surface.matrix %&gt;% imager::as.cimg(), unknown2_residuals$x3p$surface.matrix %&gt;% imager::as.cimg() ) %&gt;% plot() Cartridge case cell pair CCF calculation: The Congruent Matching Cells (CMC) method was developed at the National Institute of Standards and Technology (NIST) to quantify the similarity between two spent cartridge cases based on their breech face impressions (Song (2013)). The CMC method involves dividing a breechface impression scan into a grid of cells and comparing each cell in one scan to a corresponding region in the other scan. The motivation for this particular methodis that two breech face impressions tend to have regions of high similarity, e.g., where the firearm’s breech face impressed strongly into the cartridge case, and regions of low similarity, e.g., where the breech face may not have come into contact with the cartridge case. The highly similar regions may be drowned-out, in a sense, by the less similar regions if one were to calculate a similarity score considering the entirety of the two scans. By breaking up the breech face scans into a grid of cells, one can instead use the number of highly similar cells between the two scans as a more granular similarity metric. The cellCCF and cellCCF_bothDirections functions perform the splitting of two surface matrices into a grid of cells and performing the cross-correlation calculation for a variety of rotation values. The only difference between the two functions is that cellCCF_bothDirections calls cellCCF twice where each scan in a pair plays the role of the “questioned” scan that is divided into cells. These functions simply return results of the comparison and don’t actually implement any of the CMC logic that goes into identifying “congruent matching cells.” The call below will compare km1_residuals and unknown1_residuals by performing the following: Divide one of the matrices into a grid of \\(8 \\times 8\\) cells Determine which of these cells contains fewer than 15% of observed values (i.e., 85% or more NA values). These cells are not considered for the CCF calculation. For each remaining cell: (a) rotate the other matrix (not the one divided into cells) by some theta value using a linear interpolation scheme (b) extract a region from this rotated matrix that is centered on the same index as the cell is in its matrix but is 4 times larger. (c) shift each cell/region by its respective average element value and divide by its standard deviation (d) calculate the translation, \\((dx,dy)\\), values by which the maximum CCF is attained between a cell/region pair (e) using these translation values, calculate the “raw” cross-correlation between the cell and the cell-sized portion of the larger region at which the max CCF was attained Compile a list of data frames, one per rotation value, of CCF, dx, and dy values per cell/region pair Perform steps 1-4 again, but by dividing the other matrix into a grid of cells to be compared to larger regions in the other matrix (i.e., the matrices swap roles). For now we’ll just compare km1 to the two unknown source cartridge cases. The comparisons of km2 will be discussed below. km1_unknown1_comparison &lt;- cellCCF_bothDirections(km1_residuals, unknown1_residuals$x3p, thetas = seq(-30,30,by = 3), cellNumHoriz = 8, cellNumVert = 8, regionToCellProp = 4, minObservedProp = .15, centerCell = &quot;individualCell&quot;, scaleCell = &quot;individualCell&quot;) km1_unknown2_comparison &lt;- cellCCF_bothDirections(km1_residuals, unknown2_residuals$x3p, thetas = seq(-30,30,by = 3), cellNumHoriz = 8, cellNumVert = 8, regionToCellProp = 4, minObservedProp = .15, centerCell = &quot;individualCell&quot;, scaleCell = &quot;individualCell&quot;) We can see one of these data frames below, specifically where unknown1_residuals was rotated by 21 degrees before being compared to the cells of km1_residuals. The cellID column corresponds to the locations of each cell in km1_residuals. The cellNum column indexes the cells starting in the top left corner of the matrix moving right. The fact that this cellNum == 1 is missing from this data frame means that the top left corner of the matrix contains too few observed (less than 15%) observed values to be considered for the CCF calculation. The ccf, dx, and dy columns represent the CCF\\(_{\\max}\\) value for a particular cell/region pair as well as the associated translation values at which this CCF value occurred. The values in the ccf column were calculated by replacing missing values (of which there are often many) with the average value in the cells/regions (which is necessary for calculating the CCF using the Cross-Correlation Theorem). The rawCorr column, on the other hand, contains correlation values calculated by effectively ignoring any NA-valued elements (specifically, using the cor function with use = \"pairwise.complete.obs\"). km1_unknown1_comparison$comparison_1to2$ccfResults$`21` We are interested in extracting features from these CCF comparison results to differentiate matches from non-matches. In particular, we are interested only in cell/region pairs that are above a certain correlation threshold and have dx, dy, and theta values that are “close” to consensus-based values. We need to define what we mean be a consensus by determining some way to aggregate values together, preferably via some measure of center. It is common in the CMC literature to use the median dx, dy, and theta values, although the cmcR package lets you use any function you would like (although only mean, median, and mode have been tested). The topResultsPerCell function will take the list of data frames returned by cellCCF (or cellCCF_bothDirections) and extract information about the dx, dy, and theta values at which each cell pair attained its maximum CCF value across the entire comparison (i.e., across every rotation). If we think of the CMC method as allowing cell pairs to “vote” for a translation and rotation of one cartridge case scan to align well with another, then the resulting data frame from topResultsPerCell is the number one choice per cell pair. km1_unknown1_comparison$comparison_1to2$ccfResults %&gt;% topResultsPerCell() Congruent Matching Cells Filtering Logic: Initially proposed method: Song (2013) initially proposed using this data frame to determine CMCs. In particular, we can take the dx, dy, and theta columns, determine some consensus value among each (by, e.g, taking the median), and then determine how close each cell pair’s votes are to that consensus. For true matches, we would expect at least a majority of cells to have votes in the same ballpark. Thus, we can specify thresholds, dx_thresh, dy_thresh, and theta_thresh within which we will say that a cell pair’s vote is “close” to the consensual value. We also only want to consider pairs that achieve some minimum correlation threshold. Thecorr_thresh argument sets this threshold. Below we can see the CMCs selected using the top results between km1 and unknown1. We’ll refer to these as the “initial CMCs” since they were determined under the initially proposed CMC method by Song. There aren’t very many initial CMCs between km1 and unknown1. km1_unknown1_comparison$comparison_1to2$ccfResults %&gt;% topResultsPerCell() %&gt;% cmcFilter(corr_thresh = .8, dx_thresh = 15, theta_thresh = 3) In contrast, there are more initial CMCs between km1 and unknown2. This indicates a greater level of similarity between km1 and unknown2. km1_unknown2_comparison$comparison_1to2$ccfResults %&gt;% topResultsPerCell() %&gt;% cmcFilter(corr_thresh = .8, dx_thresh = 15, theta_thresh = 3) “Improved” method: Having chosen correlation and translation thresholds by which we can classify cell pairs as “congruent matching cells,” a useful way to determine an estimate for the rotation values by which two cartridge cases will align is to plot the number of CMCs per rotation value. Identifying “modes” in these CMC-per-theta plots is what Tong et al. (2015) proposed as one improvement to the initially proposed method. The plots below show the “forward” and “backward” distribution of CMCs per theta value for the comparisons between km1 and unknown1 as well as km1 and unknown2. The “forward” and “backward” directions correspond to the which of the two scans in the comparison was partitioned into a grid of cells (think of this as the “questioned” cartridge case) and which was broken up into overlapping regions. For example, in the plot immediately below, the “x3p1 vs. x3p2” facet means that km1 was partitioned into a grid of cells, each of which were compared to larger regions in unknown1. Tong et al. (2015) propose using these CMC-per-theta distributions to identify which theta values have a “high” number of associated CMCs. They define a “high” number of CMCs to be the maximum number of CMCs for a particular comparison minus some empirical constant (1 in their paper). If a theta value has an associated high number of CMCs, but is far away from the theta value with the maximum number of CMCs, then there’s evidence to suggest that the pair is a non-match. The plot immediately below illustrates this idea. In both bar plots, the maximum number of CMCs is 2 (at \\(\\theta = -9\\) in the first plot and \\(\\theta = 3\\) in the second). However, a number of other theta values have high CMC numbers (1 in this example) that are far away from the max CMC theta values. We define “far away” as being greater than theta_thresh degrees away from the max CMC theta value. cmcPerThetaBarPlot(km1_unknown1_comparison, corr_thresh = .8, dx_thresh = 15, theta_thresh = 3, highCMCThresh = 1) In contrast, we can see a distinct mode in the bar plots below for the comparison between km1 and unknown2. Additionally, any theta values with associated high CMC counts are within theta_thresh degrees of the maximum CMC theta value. The two modes also agree with each other, up to a sign, which is what we would expect in comparing a true match pair in both directions. cmcPerThetaBarPlot(km1_unknown2_comparison, corr_thresh = .8, dx_thresh = 15, theta_thresh = 3, highCMCThresh = 1) To settle on a final CMC number, Tong et al. (2015) proposes first identifying whether the high CMC theta values are too diffuse around the max CMC theta value using the logic described above. If a particular pair doesn’t pass this criterion, then we assign to it the initial CMCs (described above) as its final CMC number. Otherwise, if a theta value mode is identified, then we count the number of CMCs in this mode, including those associated with the max CMC as well as CMCs associated with theta values within theta_thresh of the max CMC theta value. We do this for both the “forward” and “backward” CMC distributions and combine the results as the final CMC count, excluding any duplicate pairs. The cmcFilter_improved function counts the number of CMCs under this improved method. It will return not only the final CMCs, but also the initial CMCs determined under the initially proposed method (in both directions). km1_unknown1_cmcs &lt;- cmcR::cmcFilter_improved(km1_unknown1_comparison, consensus_function = median, corr_thresh = .8, dx_thresh = 15, theta_thresh = 3) km1_unknown2_cmcs &lt;- cmcR::cmcFilter_improved(km1_unknown2_comparison, consensus_function = median, corr_thresh = .8, dx_thresh = 15, theta_thresh = 3) The improved CMC method did not yield any “final CMCs” for the comparison between km1 and unknown1. As such, we would assign the pair whichever was the minimum number of initial CMCs calculated between both directions. The CMC(s) between km1 and unknown1 are shown below. km1_unknown1_cmcs$initialCMCs[[1]][[which.min(c(nrow(km1_unknown1_cmcs$initialCMCs[[1]][[1]]), nrow(km1_unknown1_cmcs$initialCMCs[[1]][[2]])))]] In contrast, the comparison between km1 and unknown2 did yield final CMCs, which are shown below. km1_unknown2_cmcs$finalCMCs %&gt;% arrange(cellNum) The same type of comparisons can be done between km2 and the two unknown source scans, although we now have very strong evidence (apart from being able to read file paths) that unknown1 does not match km1 while unknown2 does. km2_unknown1_comparison &lt;- cellCCF_bothDirections(km2_residuals$x3p, unknown1_residuals$x3p, thetas = seq(-30,30,by = 3), cellNumHoriz = 8, regionToCellProp = 4, minObservedProp = .15, centerCell = &quot;individualCell&quot;, scaleCell = &quot;individualCell&quot;) km2_unknown2_comparison &lt;- cellCCF_bothDirections(km2_residuals$x3p, unknown2_residuals$x3p, thetas = seq(-30,30,by = 3), cellNumHoriz = 8, regionToCellProp = 4, minObservedProp = .15, centerCell = &quot;individualCell&quot;, scaleCell = &quot;individualCell&quot;) km2_unknown1_cmcs &lt;- cmcR::cmcFilter_improved(km2_unknown1_comparison, consensus_function = median, corr_thresh = .8, dx_thresh = 15) km2_unknown2_cmcs &lt;- cmcR::cmcFilter_improved(km2_unknown2_comparison, consensus_function = median, corr_thresh = .8, dx_thresh = 15) Interestingly, we see that the comparison between km2 and unknown1 does actually yield final CMCs, although only 5. This illustates an important property of this method - its extreme sensitivity to making the “correct” pre, inter, and post-processing decisions. For example, changing corr_thresh from .8 to .81 in the call to cmcFilter_improved between km1 and unknown1 above yields 0 initial and final CMCs. km2_unknown1_cmcs$initialCMCs The final CMCs for the comparison between km2 and unknown2 are given below. km2_unknown2_cmcs$finalCMCs Diagnostic tools: The cmcPlot function allows us to visualize the regions of a cartridge case scan that have been identified as “congruent matching.” For example, the 1 initial CMC calculated between km1 and unknown1 is shown below. cmcR::cmcPlot(unknown1_residuals$x3p, cmcDF = km1_unknown1_cmcs$initialCMCs[[1]][[2]]) In contrast, the final CMCs between km1 and unknown2 are shown below. cmcR::cmcPlot(unknown2_residuals$x3p, cmcDF = km1_unknown2_cmcs$finalCMCs) The same types of plots can be made for the comparison between km2 and the unknown source scans. Below are the final CMCs between km2 and unknown1. cmcR::cmcPlot(unknown1_residuals$x3p, cmcDF = km2_unknown1_cmcs$finalCMCs) And the CMCs for the comparison between km2 and unknown2 are shown below. cmcR::cmcPlot(unknown2_residuals$x3p, cmcDF = km2_unknown2_cmcs$finalCMCs) Most promising results: Below are plots related to what we have found to be the most promising pre, inter, and post-processing conditions for differentiating known matches from known non-matches. In particular, if one takes the residuals from the RANSAC method and applies a bandpass filter with wavelength cut-offs of 16 and 250 microns (similar to Chen et al.) Joe 4/20/20 Spotlight: Today I wanted to share some of the final results and conclusions related to our implementation of the Congruent Matching Cells method. I also wanted to discuss some ideas we have for the diretion of the project. The Congruent Matching Cells method was developed to identify a pair of cartridge case scans as “matches” or “non-matches.” A brief summary of how we compare two cartridge case scans using this cell-based procedure is given below. The animation below illustrates the procedure. Divide first cartridge case scan into a grid of cells. Pair a larger region in the second cartridge case scan to each cell in the first. Compare each cell to its associated region across various rotations of the second cartridge case scan. For each rotation value, determine the translation values at which each cell/region pair attains maximum “similarity” (quantified by the cross-correlation function). Use these estimated “alignment parameter” values (rotation &amp; translation) as well as the estimated \\(CCF_{\\max}\\) values for each cell/region pair as features to determine whether the two cartridge case scans match. The idea here is that each cell/region pair for a truly matching pair of cartridge case scans should have alignment parameter estimates that “agree” with each other (within some tolerance). On the other hand, cell/region pairs from a truly non-matching pair of cartridge case scans should have alignment parameter estimates that vary randomly. Count the number of “congruent matching” cells (CMCs) and use that as a similarity score for the cartridge case pair. Song (2013) recommend using a minimum CMC count of 6 to classify a pair as a match, but many authors since then have demonstrated that this threshold isn’t particularly effective in general. Two versions of the method are implemented in our R package, cmcR. The initially proposed method from John Song in 2013 and an “improved” method from Tong et al. in 2015. For both methods, the cell-based comparison procedure remains the same (so the first 4 bullet points above are performed for both). The only difference between the two methods is the manner by which the extracted features (i.e., the estimated alignment parameter values and the \\(CCF_{\\max}\\) values) are used to say whether a cell/region pair match. We’ll skip the particulars of how these two methods work, but just keep in-mind that they both introduce some “similarity criteria,” such as a minimum \\(CCF_{\\max}\\) threshold, used to call a particular cell/region pair a match. A commonly used set of cartridge case scans is from Fadul et al. (2011). This set is made up of 40 cartridge case scans - 63 known match (KM) pairs and 717 known non-match (KNM) pairs. We have applied both versions of the method implemented in the cmcR package to these scans and have been able to perfectly identify matches and non-matches for both. The plots below show the distributions of “Congruent Matching Cell” counts for all known match and known non-match pairs from the Fadul set. The first distributions shows the “initial” CMCs calculated under the initially proposed method from Song (2013) and the second the “final” CMCs calculated under the improved method from Tong et al. (2015). Pefect identification of matches and non-matches using a minimum CMC count classification threshold would correspond to separation of the known match and known non-match CMC count distributions. The ROC curves associated with each plot are created by varying the minimum CMC count classification threshold. The associated AUCs equal to 1 indicate that the KM and KNM CMC distributions are separated; that is, perfect identification is achieved. Take note that while we were able to achieve perfect identifictaion for both methods, we were not able to do so under the same set of conditions. We have yet to find a set of conditions for which perfect identification is achieved for both methods simultaneously, although we certainly have not considered every possible set of conditions. I am quite certain that further tuning of the various parameter values will yield such results. It should also be noted that other authors have been able to achieve better separation between the KM and KNM CMC distributions. The plots below show the analogous “initial” and “final” CMC distributions from Chen et al. (2017), who referred to the improved method from Tong et al. (2015) as the “high” CMC algorithm. They also used the Fadul set, except they used pre-processing procedures currently unavailable to us. Specifically, a second-order robust Gaussian regression filter. We are currently working to get this method implemented. Additionally, as already stated, finer tuning of the various parameters used in the method would undoubtedly lead to results closer to those of other authors. Sensitivity Discussion Additionally, we were interested in determining how “sensitive” the method is to various pre, inter, and post-processing conditions. In particular, we wanted to determine how well the method was able to differentiate between matches and non-matches as we change how “restrictive” the classification thresholds are. For example, how does the method respond to increasing the minimum \\(CCF_{\\max}\\) threshold used to determine if a cell/region pair is truly a match? The plot below show the CMC count distributions under the initially proposed method. We glazed over the exact details of how the CMCs are calculated, so the role of the translateThresh variable may not mean anything to you. Suffice it to say, our classification thresholds are becoming more restrictive as we move from the top right corner plot the bottom left corner plot. We can see that more lenient classification thresholds (top right corner) assign higher CMC counts to both known match and known non-match pairs - which makes some sense intuitively. We can see a clear migration of both CMC distributions as our thresholds become more restrictive. The plot below shows the CMC count distributions under the improved method, again for the purpose of determining its sensitivity to various classification thresholds. We can again see a migration of the CMC distributions as we move from less to more restrictive thresholds. The improved method seems to assign higher CMC counts to both known match and known non-matches than the improved method. In particular, considering the first two rows of plots below, we can clearly see that the known non-match CMC distribution is bimodal. This is an artifact of the way in which the improved method determines CMCs differently from the initial method, again skipping over the exact details. We have a few ideas of how we could “tweak” the improved method to reduce the number of “false positive” CMCs assigned to known non-matches. While we haven’t been able to perfectly replicate results of other authors, I do believe based on our results that our implementation of the two versions of the CMC method are quite close to the ones proposed. In the near future, we are hoping to write a submission for the R Journal related to our implementation and submit the cmcR package to CRAN. We are basically to the point where we have to write unit tests for functions in the package, so that should hopefully be completed in the near future (assuming I can motivate myself to write those unit tests). In the slightly further future, we are hoping to implement a few ideas we have on how to improve current CMC methodology. We should hopefully have related results to share sometime over the summer. Joe 6/22 Spotlight: I think we’ve finally found a tone for the paper that discusses the ambiguity and lack of computational reproducibility in the current CMC literature while not being overly harsh. We’re hoping to use this project as an example to illustrate (1) common patterns in non-reproducible research such as omitting code, parameter settings, etc. and (2) what must be done to implement an algorithm that has only been qualitatively described. First, I wanted to provide some context for those of us who are not familiar with the cartridge case project or the CMC method. This will also give me an opportunity to show-off some visualizations that I’ve spent a good deal of time putting together and incorporating into the cmcR package that I believe illustrates the concepts well. The back wall of the barrel of the firearm is commonly referred to as the breech face. For a variety of reasons, the breech face of a firearm contains markings that can uniquely identify the firearm. During the firing process, cartridge cases, often colloquially referred to as “bullet casings,” slam into the breech face. The breech face markings are impressed upon the cartridge case when this happens. Two cartridge cases fired from the same firearm can often be matched based on these “breech face impressions.” The plots below show a known match and known non-match pair of cartridge case scans. Figure 4.84: Known match pair of cartridge cases Figure 4.85: Known non-match pair of cartridge cases The Congruent Matching Cells (CMC) method was developed at the National Institute of Standards and Technology to determine whether two cartridge cases match. The method involves partitioning a scan of one cartridge case into a grid of cells. Each of these cells is paired with a counterpart region in the other cartridge case scan. In their “raw” format, two matching cartridge case scans are not necessarily aligned. In particular, one scan may need to be translated and rotated to properly align with the other. These “alignment parameter” values by which two scans will be properly aligned can be estimated for each cell/region pair (I’ll skip over how this is done). The CMC method tries to determine whether a “consensus” exists among the estimated alignment parameter values across cell/region pairs. For a truly matching pair of cartridge cases, we would expect the estimated alignment parameter values to agree with each other (possibly within some tolerance). We would also expect the estimated alignment parameter values to vary randomly for a truly non-matching pair of cartridge cases. These are assumptions underlying all methods proposed in the CMC literature. Effectively, the only difference between the various methods is how a “consensus” is determined. We have implemented two methods, the initially proposed method from Song (2013) and the High CMC method from Tong et al. (2015), in the cmcR package (these were the first two versions of the CMC method proposed). I won’t delve into the details of either method here. The plots below illustrate the CMC method. The plot immediately below shows results from a comparison between two known match cartridge case scans. The scans were partitioned into an \\(8 \\times 8\\) grid of cells. Cells shown in black have been identified as “Congruent Matching Cells.” Red cells have not been identified as CMCs. Recall that the CMC method uses the estimated alignment parameter (translation and rotation) values to identify matches. The plot on the right-hand side shows how each cell in the left-hand side plot aligns best (where “best” is quantified using the cross-correlation function). 40 cells were identified as CMCs while 3 were not. Figure 4.86: CMC results for known match pair In contrast, the plots below show the results from a comparison between two known non-match cartridge case scans. Figure 4.87: CMC results for known non-match pair A popular cartridge case data set in the CMC literature is from Fadul et al. (2011). This data set contains 63 known match and 717 known non-match cartridge case pairs. We can count the number of CMCs for each of these cartridge case pairs. Obviously, assuming the methods applied are valid, we would expect matching pairs to have more CMCs than non-matching pairs. In particular, we hope that all KM pairs have higher CMC counts than all KNM pairs. The plot below shows the distributions of the CMC counts assigned to all 63 KM and 717 KNM pairs. We can see that there is separation between the two CMC count distributions between 15 and 16 CMCs (vertical line drawn at 15.5). Figure 4.88: Initially proposed CMC method applied to 63 KM and 717 KNM cartridge case pairs The CMC count distributions based on the High CMC method are shown below for the same data set. Again, we see that there is separation between the two distributions. We’ve observed that both the initially proposed and High CMC methods tend to assign more “false positive” CMCs to non-match pairs than the initially proposed method. The number of non-matching pairs assigned a high false positive CMC count is heavily dependent on the the various parameters that require setting in the method. We hope to provide a deeper discussion on how sensitive the two methods are to various processing decisions than what is currently provided in the CMC literature. Figure 4.89: High CMC method applied to 63 KM and 717 KNM cartridge case pairs As stated above, I think we’ve found an appropriate tone for the paper that provides a tempered critique of and discussion of the lack of reproducibility in the CMC literature. Reproducibility of results (by other parties) is a necessary, if sometimes overlooked, component of research. For computationally-intensive research (e.g., algorithm development), computational reproducibility is vital. Anything short of the original data and code used to arrive at the results does not equate to full reproducibility. We are planning on using the cmcR package as an example of how unreproducibility, specifically computational unreproducibility, manifests in academic literature. This includes ambiguity in how parameters are chosen, using unreplicable processing procedures (e.g., processing data manually), or reliance on proprietary software. The CMC literature is rife with all of these Reproducibility is especially important for algorithms that may affect legal decisions (e.g., suspect’s firearm fired a particular cartridge case). While few would disagree with the need for reproducible research, I believe the cmcR package provides an illustrative example of the typical challenges, failures, and successes that one experiences when implementing a previously closed-source method. I have also been adding functionality to the cmcR as ideas come up while writing. For example, the cartridge case pairs and CMC plots shown above were created with functions available in the cmcR package. Joe 10/5 Spotlight: We are currently fine tuning aspects of the paper and hoping to submit soon! I’m very excited to move onto the next stage of this project. Additionally, I have been thinking a good deal about how we might improve the cartridge case identification procedures while providing more rigorous justification than the current “state-of-the-art” which is effectively guess-and-check what procedures work well. I am surprised how little the literature borrows from state-of-the-art techniques in image processing (MRI imaging, etc.) and I believe that there is a lot of untapped potential in using such methods. The paper is methodically coming along. We are to the point of finely tuning aspects of the paper, specifically sections that explain the CMC methodology, to make it clearer how/why the procedures work. The original algorithm descriptions are incomplete in many respects. As such, we are not only introducing our particular implementation of the procedures, but also trying to provide an unambiguous description of the methods themselves. This has proven to be a time-intensive, frustrating process and often feels like we are patching holes caused by others. The figure shown below illustrates one way in which we are trying to clarify the CMC methodology. It shows 3 cartridge cases and the results obtained from their comparisons. Fadul 1-1 and Fadul 1-2 are a matching pair while Fadul 1-1 and Fadul 2-1 are non-matching. The gray cells in Fadul 1-2 and Fadul 2-1 represent where the Fadul 1-1 cells “align” based on the maximum cross-correlation. So each cell in Fadul 1-2 and Fadul 2-1 has an associated horizontal/vertical translation, a rotation, and a cross-correlation score based on those transformations. These are visually depicted as histograms below below each scan. The purple bars represent user-defined thresholds used to classify a particular value as “congruent” or not. We can see that, in general, more values tend to be classified as congruent for the matching pair than for the non-matching pair. Interestingly, however, we can see that the CCF scores are not as “well-behaved” in that a good deal of non-matching cells have “congruent” CCF scores (based on this particular set of thresholds, at least). In order for an entire cell to be classified as congruent, each of its associated values (translation, rotation, and CCF) need to be classified as congruent. This is how the originally proposed CMC method works (from Song (2013)). We have also implemented one of the descendants of the original method, but are still brainstorming illustrative visualizations. On top of introducing the implementation, we are also trying to abstract the process we went through in its creation to a larger discussion on reproducibility in computational research. In particular, we focus on why/how computational research should/can be rendered reproducible and use this particular CMC project as an illustrative example. The example shown above effectively skips over 2 of the 3 critical steps to the CMC procedure: the preprocessing and feature extraction. Both of these steps require a good deal of user input and little has been done in the CMC literature to create a principled approach to making informed decisions. This is one aspect of the project that I hope to focus on after we’re done with this paper. The figure below illustrates the new preprocessing pipeline we have used to obtain better results. This new pipeline is semi-automatic, as opposed to the fully-automatic procedure we were previously using, but is much faster and yields considerably better results. The downside to the current version of this procedure is that it requires the user to specify the radii of the two circles between which the observations are kept (everything is handled automatically). Optimally we would use a fully automatic way to remove the undesired observations on the exterior and in the center of the scan. I would be happy to hear any ideas. Steps 2 and 3 are of our design. There is no discussion in the CMC literature concerning whether the breech face surfaces are leveled (step 3). I will discuss the ramifications of not leveling the surfaces below. Step 4, applying a Gaussian filter, is common in the CMC literature. However, inexplicably, different types of Gaussian filters (band-pass, low-pass, regression, robust regression, etc.) are used between CMC papers which compounds our inability to actually compare published results. Apart from it being a common tool in some domains, it’s unclear why even a Gaussian filter is used – this is something I would like to explore in the future. The figure belows shows an example of the superior results that we obtain from the new preprocessing pipeline. The number of cells that are deemed “congruent” under the CMC method can be used as a similarity score between a cartridge case pair. We anticipate matching cartridge case pairs to have a higher “CMC count” than non-matching cartridge cases in general. Each plot below shows the CMC counts for a set of 63 matching and 717 non-matching cartridge case pairs. How these plots differ is in the set of translation and CCF thresholds used to classify each cell as “congruent.” We would like to see a large amount of separation between the distribution of matching and non-matching CMC counts. The AUCs are calculated based on varying the minimum CMC count used to declare a cartridge case pair “matching.” AUCs equal to 1 indicate perfect separation between the matching and non-matching distributions. As we can see, numerous combinations result in perfect separation. As such, we introduced a simple diagnostic to further differentiate these threshold combinations based on the fact that we would prefer large separation between the matching and non-matching distributions: \\[ \\text{Var. Ratio} = \\frac{\\frac{1}{n_1 + n_2 - 1} \\sum_{i=1}^2 \\sum_{j=1}^{n_i} \\left(\\text{CMC}_{i,j_i} - \\overline{\\text{CMC}}\\right)^2}{\\frac{1}{2} \\sum_{i=1}^2 \\frac{1}{n_i - 1}\\sum_{j=1}^{n_i} \\left(\\text{CMC}_{i,j_i} - \\overline{\\text{CMC}}_i\\right)^2} \\] The diagnostic is the ratio between the “total” sample variance of the CMC counts, irrespective of match or non-match group membership, divided by the average within-group sample variance. Larger values of the simple diagnostic are preferred. Compare the figure about to an analogous figure in Chen et al. (2017). Again, no justification for this particular set of thresholds is given. Compared to previous results we’ve obtained, there is considerably more separation between the match and non-match CMC count distributions using this new preprocessing scheme. However, this plot also uncovers another important point that we want to emphasize in the paper: the sensitivity of the method to various processing decisions. Method sensitivity has never been described in the CMC literature. Rather, authors seem to share what “looks good” on the Fadul dataset. Consider what happens to the CMC distributions if we skip the surface-leveling (step 3) in the preprocessing above. The issue with keeping such information in the scans is that the “tilt” visible in the preprocessing pipeline in steps 1 and 2 aren’t consistent across cartridge cases fired from the same firearm. The CCF, being a non-robust similarity metric, gravitates towards large-scale information such as a tilt in the height values. However, we specifically want to make classifications based on small, intricate details of the two scans. Thus, not removing information such as a large-scale tilt causes the CCF to be smaller for matching scans that don’t have the same tilt and larger for non-matching that do have the same tilt. We use such examples to illustrate why the tenants of reproducibility need to be adhered to, especially when they apply to forensic methods. 4.3.3 Modified Chumbley non-random test 4.3.3.1 Land-to-land scores The moified Chumbley non-random algorithm is a statistical non-paramaetric test that compares two signatures under consideration and gives a test statistic. The test statistic is used to make classifications and compute error rates based on different nominal type I levels. The basic principle behind the method is to first take two marking that have to be compared, choose a segment length which is a portion of the signature, and use this window segement to find which windows give the maximum correlation. The lag between these respective markings is computed based on the location of the two maximum correlation windows in the two markings. Now the algorithm works in two steps where first, lag congruent correlations between several smaller windows of the two markings are computed, this is called the same-shift. The second step serves the purpose of computing windows of correlation between the two signatures with window sized the same as the same-shift, but the with the purpose of finding correlations when the windows are not lag synchronized. The second step is called different shift step and has a specific order in which the pair of windows are chosen between which the correlations are to be computed. The different-shift serves as benchmark for comparison. It shows a set of bad correlations, against which the same-shift correlations are compared. A U-statistic is computed for the comparison based on the correlations in this procedure. The modified chumbley method (Krishnan and Hofmann 2019) can work with two markings at a time. Therefore the method can be used for comparing signatures from one land to signature from another land. The land-to-land comparison was performed for Hamby 44 dataset from (Zheng 2016) and CSAFE (Krishnan and Hofmann 2019) and associated error rates were computed for these comparisons. 4.3.3.2 Bullet-to-bullet scores In this method we extend the modified chumbley non-random method from land-to-land scoring to bullet-to-bullet scoring. In order to do this, first 6 ordered pairs of lands between the two bullets are chosen for comparison. The modified chumbley method is used on these 6 pairwise comparisons. This results in the same-shift and different-shift comparisons from each of the 6 comparisons. We do not need a land-to-land pairwise U-statistics and classification in this method. Instead all the same-shift and different-shift correlations are now aggregated from the 6 comparisons and a new non-parametric U test is used on the aggregated sets. This gives a test statistic at the bullet level and consequently we can compute p-values. This is used with different nominal significance levels to identify bullet level error rates. 4.3.4 Congruent Matching Profile Segments 4.3.4.1 Introduction to CMPS and Algorithm Summary This section provides an introduction and a summary of the Congruent Matching Profile Segments (CMPS) algorithm. The Congruent Matching Profile Segments (CMPS) method [Chen et al. 2019], in general, can be used for objective comparison of striated tool marks, but in our examples, we mainly use it for bullet profiles/signatures comparison. The CMPS number is expected to be large if two signatures are similar. So it can also be considered as a feature that measures the similarity of two bullets (yet another bullet feature). Right now, we are developing a R package that implements the CMPS method [Chen et al. 2019]. The following sections will provide with a summary of the CMPS algorithm and examples on how to use the package. The development version of the package can be installed from GitHub with: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;willju-wangqian/CMPS&quot;) In this section we use a known match (KM) compasison to illustrate the main ideas of CMPS algorithm. The bullet data used here can be found in Chapter 3.5 of Open Forensic Science in R. We can also run the following chunk. In this example, we are taking the signature of the third land engraved area (LEA) of the second bullet as the reference signature and the second LEA of the first bullet as the comparison signature. This is a KM comparison. library(CMPS) data(&quot;bullets&quot;) land23 &lt;- bullets$sigs[bullets$bulletland == &quot;2-3&quot;][[1]] land12 &lt;- bullets$sigs[bullets$bulletland == &quot;1-2&quot;][[1]] x &lt;- land23$sig y &lt;- land12$sig cmps &lt;- extract_feature_cmps(x, y, full_result = TRUE) cmps # $CMPS.score # [1] 14 # # $rec.position # [1] -5 # # $pos.df # position cmps # 1 -9 14 # 2 -8 14 # 3 -7 14 # 4 -6 14 # 5 -5 14 # 6 -4 14 # 7 -3 14 # 8 -2 14 # 9 -1 14 # # $nseg # [1] 22 And we have the plot of x and y. Figure 4.90: A KM Comparison, x and y we take the first signature as the reference signature (x) and cut it into consecutive and non-overlapping basis segments of the same length. In this case, we have 22 basis segments in total. Figure 4.91: Cut x into consecutive and non-overlapping basis segments of the same length. Only 4 basis segments are shown here for each basis segment, we compute the cross-correlation function (ccf) between the basis segment and the comparison signature (y) Figure 4.92: y and 7th basis segment Figure 4.93: the cross-correlation function (ccf) between y and segment 7 for the ccf curve, the position represents the shift of the segment. A negative value means a shift to the left, a positive value means a shift to the right, and 0 means no shift (the segment stays at its original position in the reference signature); we are interested in the peaks in the ccf curve and the positions of those peaks (as indicated by the red vertical line in the plot above). In other words, if we shift the segment, which position would give us the “best fit”? If two signatures are similar (a KM comparison), most of the basis segments should agree with each other on the position of the best fit. Then these segments are called the “Congruent Matching Profile Segments”. Figure 4.94: compare y to the basis segments of x And ideally, if two signatures are identical, we are expecting the position of the highest peak in the ccf curve remains the same across all ccf curves (we only show 7 segments here); Figure 4.95: ideal case: compare x to itself. The highest peak has value 1 and is marked by the blue dot But in the real case, the basis segments might not achieve a final agreement, but we have the majority; Figure 4.96: real case: compare x to y. The 5 highest peaks are marked by the blue dots We mark the 5 highest peaks for each ccf curve because the position of the “highest peak” might not be the best one. each ccf curve votes for 5 candidate positions, then we ask two questions in order to obtain the CMPS number/score: which position receives the most votes? -&gt; the best position (indicated by the red vertical line) how many segments have voted for the best position? -&gt; CMPS number If we focus on these 7 segments only, and have a very short tolerance zone, the CMPS number is 6. (If we consider all 22 segments, and have a default tolerance zone (+/- 25 units), the CMPS number is 20.) false positive: how can the segments vote more wisely? -&gt; Multi Segment Lengths Strategy by increasing the segment length (scale), one can reduce the number of “false positive” peaks Figure 4.97: Multi Segment Lengths Strategy - increasing the segment length could decrease the number of false positive peaks in ccf curves Figure 4.98: Multi Segment Lengths Strategy - increasing the segment length could decrease the number of false positive peaks in ccf curves we choose 5 peaks at scale 1; 3 peaks at scale 2; 1 peak at scale 3 the peak shared by all three scales is a consistent correlation peak (ccp). And the position of the ccp is our best choice. Sometimes a ccp might not be found. Trying to identify a ccp for each basis segment is called a “multi segment lengths” strategy. and then, each ccf curve votes for only 1 best condidate position if a ccp can be found; again, we ask two quesitons: which position receives the most votes? how many segments have voted for this position? -&gt; CMPS number by default, CMPS algorithm uses the multi-segment lengths strategy. if we consider all segments and use the multi-segment lengths strategy when comparing x and y, a KM comparison, we have extract_feature_cmps(x, y, seg_length = 50, seg_scale_max = 3, Tx = 25, npeaks.set = c(5, 3, 1), full_result = TRUE) # $CMPS.score # [1] 14 # # $rec.position # [1] -5 # # $pos.df # position cmps # 1 -9 14 # 2 -8 14 # 3 -7 14 # 4 -6 14 # 5 -5 14 # 6 -4 14 # 7 -3 14 # 8 -2 14 # 9 -1 14 # # $nseg # [1] 22 if we have a KNM (known non-match) comparison, e.g. compare land 2-3 with land 1-3: land23 &lt;- bullets$sigs[bullets$bulletland == &quot;2-3&quot;][[1]] land13 &lt;- bullets$sigs[bullets$bulletland == &quot;1-3&quot;][[1]] result &lt;- extract_feature_cmps(land23$sig, land13$sig, seg_length = 50, seg_scale_max = 3, Tx = 25, npeaks.set = c(5, 3, 1), full_result = TRUE) result$CMPS.score # [1] 1 result$rec.position # [1] 128 result$nseg # [1] 22 4.3.4.2 Full Comparison Between Two Bullets This section provides an example on how to use the CMPS method to make a comparison between two bullets. extract_feature_cmps() can also be used in a pipeline fashion library(tidyverse) library(bulletxtrctr) lands &lt;- unique(bullets$bulletland) comparisons &lt;- data.frame(expand.grid(land1 = lands[1:6], land2 = lands[7:12]), stringsAsFactors = FALSE) comparisons &lt;- comparisons %&gt;% mutate( aligned = purrr::map2(.x = land1, .y = land2, .f = function(xx, yy) { land1 &lt;- bullets$sigs[bullets$bulletland == xx][[1]] land2 &lt;- bullets$sigs[bullets$bulletland == yy][[1]] land1$bullet &lt;- &quot;first-land&quot; land2$bullet &lt;- &quot;second-land&quot; sig_align(land1$sig, land2$sig) })) comparisons &lt;- comparisons %&gt;% mutate(cmps = aligned %&gt;% purrr::map(.f = function(a) { extract_feature_cmps(a$lands$sig1, a$lands$sig2, full_result = TRUE) })) # comparisons.cmps &lt;- comparisons.cmps %&gt;% # mutate(cmps = aligned %&gt;% purrr::map_dbl(.f = function(a) { # extract_feature_cmps(a$lands$sig1, a$lands$sig2, full_result = FALSE) # })) # comparisons.cmps %&gt;% select(land1, land2, cmps) comparisons &lt;- comparisons %&gt;% mutate( cmps_score = sapply(comparisons$cmps, function(x) x$CMPS.score), cmps_nseg = sapply(comparisons$cmps, function(x) x$nseg) ) cp1 &lt;- comparisons %&gt;% select(land1, land2, cmps_score, cmps_nseg) cp1 # land1 land2 cmps_score cmps_nseg # 1 1-1 2-1 2 23 # 2 1-2 2-1 2 22 # 3 1-3 2-1 1 21 # 4 1-4 2-1 2 22 # 5 1-5 2-1 1 23 # 6 1-6 2-1 14 22 # 7 1-1 2-2 5 23 # 8 1-2 2-2 2 22 # 9 1-3 2-2 1 21 # 10 1-4 2-2 1 22 # 11 1-5 2-2 2 23 # 12 1-6 2-2 2 22 # 13 1-1 2-3 3 23 # 14 1-2 2-3 13 22 # 15 1-3 2-3 2 21 # 16 1-4 2-3 1 22 # 17 1-5 2-3 1 23 # 18 1-6 2-3 2 22 # 19 1-1 2-4 4 23 # 20 1-2 2-4 1 22 # 21 1-3 2-4 11 21 # 22 1-4 2-4 1 22 # 23 1-5 2-4 2 23 # 24 1-6 2-4 2 22 # 25 1-1 2-5 3 23 # 26 1-2 2-5 1 22 # 27 1-3 2-5 2 21 # 28 1-4 2-5 10 22 # 29 1-5 2-5 2 23 # 30 1-6 2-5 1 22 # 31 1-1 2-6 2 23 # 32 1-2 2-6 1 22 # 33 1-3 2-6 2 21 # 34 1-4 2-6 1 22 # 35 1-5 2-6 15 23 # 36 1-6 2-6 1 22 4.3.4.3 Apply CMPS to Hamby252 Will 09/19 Update: We applied the CMPS method to Hamby252 (the dataset used in the original paper). And now we are able to reproduce the main results qualitatively. Additionally, in order to obtain more reliable bullet signatures and effectively investigate outliers, a shiny app was implemented for bulletxtrctr. 4.3.4.3.1 Hamby252 and Reproduce On the bullet level, we mainly consider two similarity scores: \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\bar{CMPS}_{max}}\\). For Hamby252 each bullet has 6 lands. In order to compare two bullets, we want to compute the CMPS score for all possible land-by-land pairs. In this case, we have 36 pairs in total and thus will have 36 CMPS scores. We want to summarize these 36 CMPS scores into a single similarity score at the bullet level. \\(\\mathrm{CMPS_{max}}\\) is the maximum CMPS score among all 36 scores; here is result of the paper[Chen et al. 2019]: and here is our result: \\(\\mathrm{\\bar{CMPS}_{max}}\\): for the CMPS scores belonging to the same phase we compute the average. Since there are 6 phases in total, there would be 6 such averages. \\(\\mathrm{\\bar{CMPS}_{max}}\\) is the maximum of these averages. CMPS scores and 6 phases [Chen et al. 2019] the result of the paper [Chen et al. 2019] our result: Comments: We are only able to qualitatively reproduce the results because in the paper the researchers were using different ways of obtaining bullet signatures. But both \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\bar{CMPS}_{max}}\\) give us a clear seperation between the known-match and known-non-match comparisons. Ukn is a barrel label used in the Hamby252. Although it’s labeled “Unknown”, we do have the ground truth of which is which. There are 46 KM comparisons if we include the ground truth. And there are 46 observations for the cluster with higher CMPS score. Improving the quality of bullet signatures improves the CMPS scores. 4.3.4.3.2 Shiny App for Bullet Investigation If something unusual happened to the crosscut, ccdata, and grooves (all these are important in order to extract reliable bullet signatures), users might want to step in, take a look, and try different values to get a better bullet signature. This shiny app provides a way. Users need to have a tibble object in their current environment called shiny.tt, which contains x3p, crosscut, ccdata, and grooves. Use the following code to launch the shiny app. bulletxtrctr::runExample() 4.3.4.4 Future Work improve and manage to submit the CMPS package to CRAN cross-validate some hyper-parameters apply CMPS to Hamby44 4.4 Analysis of Results 4.4.1 Stability of the Analysis Process evaluation: Yawei is going to work through all 626 barrels of knowns to assess similarity scores Figure 4.99: Results from assessing scans of barrel FAU 1 similarity. Figure 4.100: Results from assessing scans of barrel FAU 2 similarity. Why some of the cases failed? (181/626 = 30%) x3p_crosscut_optimize() failed to find the positions to get cross cut for some lands. Figure 4.101: Land scan for barrel FAU 3 bullet A land 6. Figure 4.102: Land scan for barrel FAU 4 bullet C land 5. Figure 4.103: Land scan for barrel FAU 5 bullet B land 5. Assess the land-land comparasion and bullet-bullet comparasion For bullet-bullet comparasion: we use the “sequence average maximum”(SAM), i.e. average ccf of “lines” of land-land comparasions, as the bullet similarity score(currently). By making use of 92 manually generated comparasion data, we try to produce the KM(known-matches) and KNM(known-non-matches) plot. For the known mathches, we have totally 626 x 6 = 3756 for the LAPD data, excluding the comparasions for same bullet. For the known non-matches, we don’t have the data in hand. We need to generate the data in a way. We have totally 626 x 625/2 x 16 = 313000 known non-matches. We can only generate a sample from the data. We sampled 100 bullet-bullet known non-matches from our 92 cases. Figure 4.104: KM and KNM Is the SAM(sequence average maximum) a good choice? Need to do a permutation test. Figure 4.105: SAM permutation result Figure 4.106: SAM permutation result among maxmum Possible dependence structure in land-land comparasions: Assume beta distributions for the ccf for both known mathces and known non-matches. For the real known match cases, we consider a mixture distribution of two/three beta distribution. Figure 4.107: KM three components for all data Figure 4.108: KNM two components for all data Figure 4.109: common component Ten-fold-cross validation (no validation yet), check the model estimator sensibility Figure 4.110: KM ten-fold models for two components Figure 4.111: KM ten-fold models for three components Figure 4.112: KNM ten-fold models for one component Figure 4.113: KNM ten-fold models for two components Increasing sample size: 1, 2, 6, 12, … number of barrels Figure 4.114: KM increasing sample models for two components Figure 4.115: KNM increasing sample models for one components Figure 4.116: KM increasing sample for first component More on the weight Figure 4.117: Prior weight on components More on one barrel case Figure 4.118: KM one barrel model for the frist ten barrels Some conclusions from current plots: Generally, the fits are stable in both ten-fold-cross validation and increasing sample cases In ten-fold model, group8 model behaves a little different from others when in three components case In increasing sample models, the one barrel model is not stable, but the one in our case is still a rarare case The three components model for KM is less stable than two components one especially in small sample cases Two components model for KNM is not stable in small sample case(in terms of weight) After fitting Beta mixture models with different number of components and analysis with the actual fit (more on this). We finally choose Beta mixture models with two components for both KM and KNM distributions of land-land comparisons. Then we will make further inference about the land-land comparisons based on that. Specifically, we will use log likelihood ratios to quantify how strong our evidence is to support that two lands are from the same source. To evaluate the Beta mixture models for KM and KNM, we produce ROC curves using log likelihood ratios. We also evaluate the models in different cases to see how stable the fits are and how effective the procedure is to classify the land-land comparisons. Since the log likelihood ratios require both KM and KNM models, we will control the KNM model as the full-data fitted KNM model and focus mainly on the KM models in the following sections unless otherwise specified such as in the one-barrel comparisons later where we use the KNM fitted from within barrels with three bullets. We produced ROC curves in three different cases: Cross validation KM models and full data KNM model. Each KM model is fitted with 54 barrels, 1944 land-land comparisons. Different size KM models and full data KNM model. KM models has sample size from 1 to 60 barrels. (1, 2, 6, 12, …, 60 as stated before) One-barrel KM models and full data KNM model. One-barrel KM models use the first ten barrels. From the ROC curves, we can easily see how stable our procedure is in terms of the classification performance. Note that those ROC curves start with the y-axis higher than 0.5 (which may be changed later). And the TPR(y-value) goes up steeply to about 0.75 with almost no change in FPR(x-value). Then the curve gradually goes up with different rates. There are two significant points from the ROC curves: 1) the pattern is very stable in the ROC curves across different cases and within each case; 2) the models perform very well. And also note that when we produce ROC curves, we need to get test sets and specify sample size and proportion. Here we generate the test set from the full data randomly, and set KM/KNM ratio as 200/1000. The same set is used in all three ROC plots. We choose such a ratio to make the true positive classification more difficult. If we set KM/KNM as 1000/1000, the curves will be smoother and get rid of those concave regions in current plots and of course, perform a little better in those concave regions but not significantly different in overall performance and pattern. Figure 4.119: ROC-cv-models Figure 4.120: ROC-different-size-models Figure 4.121: ROC-one-barrel To summarize the model fits we have, we compare several important parameters in the following tables. We primarily focus on how stable the model we have with fixed sample size and across different sample sizes. From the KM-table and KNM-table, we have the following observations: 1) The weight of the components is not stable in terms of large standard errors in both KM and KNM cases with small sample sizes. 2) The mean values of the distributions (parameter mu) are relatively stable, especially for the primary components (first component for KNM and second component for KM). 3) The variance of the estimated distributions (parameter phi) is not stable, which is reasonable because it takes care of the variation of the data in each fit. 4) The variance of the estimated distributions (parameter phi) decreases as sample size increases. To better understand how well the model fit is and how stable it is we should also refer to the actual curves to help us see what those parameters actually mean in the curves. And I would say three barrels may be a safe minimum sample size to get a stable fit across different cases in terms of handling differences and variations existing in data sets but it’s not the minimum sample size required to make good classifications. Figure 4.122: KM-table Figure 4.123: KNM-table We’re also interested in the variation of mean values of KM ccf in different barrels. The following boxplot with the global box as the right most one is shown for this comparison. From this plot, the constant mean value assumption may be challenged, however, we must take the practice work of scan and identification into considerations when making such conclusions. Figure 4.124: ccf in each barrel To show how our model works in a single barrel case, we conduct the model fit as in the real case. We fit the KM and KNM models with three bullet comparisons (18 KM and 135 KNM land-land ccf respectively), and conduct a test on those comparisons with the forth bullet (18 KM and 105 KNM land-land ccf respectively). We conduct the above procedure in a LOO frame work, i.e. to leave one bullet out each time. The ROC curves for two barrels are shown as follows. We can see the classification results are almost as good as the previous ones. Figure 4.125: within barrel ROC for barrel 42 and 56 Error Rate Control and Cutoff Based on our estimated models for KM and KNM, we hope to construct a test statistic to quatify how likely a new bullet has the same source with our testing bullets. We start from the land-land comparison and finally reach a bullet-bullet comparison based land-land comparison. We start with the well-known log-likelihood ratio, and see how well it works in our case. We define the statistic as: \\[T = log(\\frac{KM}{KNM})\\] In which case, we reject the null hypohesis that the new bullet is from different source from our testing bullets when we have a large value for \\(T\\). First we have a look at how this log-likelihood ratio statistic changes with respect to the ccf score we calculated. Figure 4.126: loglr-fau42 Clearly, there is a potential issue with the log-likelihood ratio since it is not monotone w.r.t ccf, which means a larger ccf may lead to a smaller test statistic sometimes. As we can see when ccf is 0.45, the log-lr is 0.127 &gt; 0, however it goes below 0 when ccf increase to 0.523 (the cutoff with type one error rate as 0.05). We can also find another issue from a different aspect. If we set the type one error as 0.05 (which should be smaller in practice), then we find the ccf with this error rate from KNM (null hypothesis) is 0.523. Then the log-likelihood ratio we calculated at this point is -1.11, which means we have larger likelihood for KNM than KM even with this large enough cutoff in terms of the type one error rate. These two criteria give us contradicted results. The cutoff we set according to the usual type one error rate contradicted with the one indicated by log-likelihood ratio. However, the lower tail of the KM distribution at this cutoff here is 0.12, which is twice as larger as 0.05, which is contradicted with the log-lr again. Another issue arised in the situation above is that the cutoff according to type one error is also a lower tail cutoff with small probability for the alternative hypothesis. We may be able to reject the null hypothesis, but it is not enough to support the alternative hypothesis and those hypotheses may be rejected both depending on your construction of hypothesis. This is a problem caused by the fact that these two hypothetic distributions are far apart from each other. A smaller type one error rate might help. But the issue with the log-likelihood ratio can be severe sometimes because we have non-unimodal distributions and the comparison may happen beween two components of KM distribution, which might result in a misleading result. Besides, the tail behavior is quite sensitive to the model assumption and estimation, which we should avoid to rely on. So we might consider a probability based criteria instead of a density based criteria. The odds ratio statistic we are going to construct is promising. Given a cutoff \\(x_0\\), we can determine the probabilities to be classified as matches for both KNM and KM distributions. Denote these probabilities for KNM and KM are \\(p_1\\) and \\(p_2\\) respectively, we can get the following form of odds ratio given a cutoff: \\[odds~ratio = \\frac{p_2/(1-p_2)}{p_1/(1-p_1)}\\] notice that the numerator descreases w.r.t \\(x_0\\), and the denominator descreases w.r.t \\(x_0\\). We are able to calculate a maximum value of the odds ratio w.r.t \\(x_0\\), which gives us a reasonable cutoff of ccf to be classified into matches. Other possible ways to get a cutoff and control the error rate may be Set a reasonable type one error rate in advance and report the cutoff From the ROC curve An additional requirement may be \\[(1-p_2) &gt; Mp_1\\] for a large M. We are going to compare the above methods next. Figure 4.127: two curves Figure 4.128: odds ratio Figure 4.129: comparison table We also have a look at the random forest scores for the LAPD sets. Our goal is to specify distributional forms to model the scores for known match (KM) and known non-match (KNM), and reach the estimated underlying theoretical error rates to evaluate the automatic matching algorithms. We would also investigate how this estimated theoretical error rates change with different sample sizes. Particularly, we make use of the LAPD data set which provide large samples to make this evaluation of underlying error rates possible. And we consider the beta distributions and beta-mixture distributions as candidate distributional forms. By consider beta-mixture distributions, we would like to introduce a second component to account for the heavy tail and the potential second mode (for KM particularly) of KM and KNM. We started with the more complex beta-mixture distribution which takes the single beta distributions as simpler cases and determine if the beta-mixture/beta distribution is sufficient to describe the data by conducting the lack of fit test through likelihood which has an asymptotic chi-square distribution. And the error rates will be evaluated under controlled conditions (given threshold selection method) and the effect of sample sizes will also be established based on the this. This is something partly covered in the previous sections. Currently, We are working to extract from that and form a well organized discussion. Many results will need to be reproduced and more closely scrutinized. One change is that we are trying to transfer to the random forest scores from cross correlations to avoid values less than 0 and catch up with the latest method, however the random forest scores don’t perform as expected so far (in terms of the distribution of scores and estimation process) which requires more investigation. And we established more formal statistical inference procedures to do model selection and evaluation. We also control the threshold selection method fixed for the current discussion of error rates (to focus on the underlying model selection) and acknowledge that the threshold selection methods could greatly affect the resulting error rates, which we would like to discuss further in another paper in ideal case. To start the discussion, have a look at the distributions of the random forest scores for FAU 330 and the full available data. The figure 4.130 was produced for a particular barrel. The curves are estimated by fit beta-mixture distributions for both KNM and KM without bullet A, the rugs are those comparison with bullet A. Just to get some sense how this behaved and how the data look like. We can get the primary observations from the plot: Points for KNM and KM are entirely separated from each other (for rugs which are comparisons with bullet A) The KNM has an unexpected heavy tail The KM has a lot of extreme large values (including 1’s), and the corresponding distribution has infinite probability density at rfscore = 1 The figure 4.131 shows beta-mixture models for both KNM and KM for full data. This plot reveals a few serious issues/doubts with the data and the estimation process: KNM has two modes, which is unexpected Estimated KM distribution seems to be able to improve, potentially affected by extreme values Figure 4.130: FAU330: rugs with A, densities without A for RF scores Figure 4.131: Random forest scores and estimated distributions for full data Currently, we have the following major issues to solve before going further. And these are more of technical issues. Evaluate the estimation process, double check the pattern revealed in the full random forest score figure. (with different starting values? and other tuning parameters?) Report standard error of estimation (bootstrap or asymptotic normality? Is the package reliable?) Conduct lack of fit test for the beta-mixture distribution (what’s the saturated model?) Refine the writeup 4.4.1.1 Look colser to the dependence structure of the land engraved areas There are some discussions about the existing patterns of dependence in the observed data. Inspired by that, we are trying to build a model to formalize those thoughts. We start from a model for the land engraved areas and try to develop a model for the ccfs in hand. We would naturely think about the following effects when producing characteristic features(in some measurement) on the lands we focused: 1) the barrel specific effect, 2) the land specific effect, 3) explosion effect, 4) random errors. If we are going to model this effect, there could be some models doing this jobs with different assumed denpendence structures accounted or expressed there. We are thinking about 1) the additive random effect model(if this is the correct name) , 2) some hierarchical structure with denpendence expressed somehow in the hyper parameters, 3) the glm. We start with the additive random effect model which has clearer structure at the first glance. We formulate the model as the following: \\(Y_{ijk}\\) is some measurement of a land engraved area from barrel \\(i\\), land \\(j\\)(nested), and explosion \\(k\\). \\[Y_{ijk} = \\beta_0 + \\mu_i + \\nu_{ij} + \\omega_k + \\epsilon_{ijk}\\] where, barrel effect, land effect, explosion effect, random effect are respectively: \\(\\mu_i\\)~ iid \\(N(0, \\sigma_{\\mu}^2)\\), \\(\\nu_{ij}\\)~ iid \\(N(0, \\sigma_{\\nu}^2)\\), \\(\\omega_{k}\\)~ iid \\(N(0, \\sigma_{\\omega}^2)\\), \\(\\epsilon_{ijk}\\)~ iid \\(N(0, \\sigma_{\\epsilon}^2)\\). (some more complicated structures are possible) Note this is a land model, however we want a ccf model. We first check the variance, covariance and correlations in different cases. \\[Var(Y_{ijk}) = \\sigma_{\\mu}^2 + \\sigma_{\\nu}^2 + \\sigma_{\\omega}^2 + \\sigma_{\\epsilon}^2\\] For the same barrel, same land but different expplosions(KM), we have the correlation: \\[corr(Y_{ijk_1}, Y_{ijk_2}) = \\frac{\\sigma_{\\mu}^2 + \\sigma_{\\nu}^2}{\\sigma_{\\mu}^2 + \\sigma_{\\nu}^2 + \\sigma_{\\omega}^2 + \\sigma_{\\epsilon}^2}\\] For the same barrel, different land and different expplosions(some of KNM), we have the correlation: \\[corr(Y_{ij_1k_1}, Y_{ij_2k_2}) = \\frac{\\sigma_{\\mu}^2}{\\sigma_{\\mu}^2 + \\sigma_{\\nu}^2 + \\sigma_{\\omega}^2 + \\sigma_{\\epsilon}^2}\\] Now we have a point estimator of the correlations in the above cases. To address our insterest, we need more than this. We could possiblly assume the beta distribution now as what we did previously(or mixed beta later). We are building a hierarchical model (not necessarily Bayesian now) for the ccfs with dependence structure considerred. We can reasonablly assume the above point estimation a mean value of the \\(Beta(\\mu, \\phi)\\) (mean value parameterization, different \\(\\mu\\) from previous). We can somehow check the assumaption by some data driven methods. But the model we could have is then: \\[Z_i \\mbox{~}iid ~Beta(\\mu_i, \\phi_i)\\] Where \\(i\\) indicate the above two(or more) cases of correlations we are looking at. We could specify(assume) the model parameters by the additive random effect model through maximum likehood method or somehow. 4.5 Communication of Results and Methods The results are communicated through an interactive user interface. The first part of this interface lets you add all the bullets, barrels and lands for which the random forest and other scores are to be computed. A preliminary diagnostic of the orientations and dimensions of the lands tell us, if we can proceed safely to extraction of markings and then to cross-comparisons. After this step, we can apply any sampling or interpolation needed on the land images, all these operations can be batched to the entire set of comparisons under consideration. Then we can make transformations like rotation, transpose etc on a sample image, visualize the results, and since we are dealing with conforming orientation and dimensions of lands present in the entire set, we can batch the transformations. We extract markings, locate grooves, align signatures, and generate cross-comparison results. Each step is notified in UI and all steps are logged. The scores and results are then communicated through an interactive visualization. We first interact at the top most level where we have bullet-to-bullet scores for all the cross-comparisons presented in a grid. We can select one comparison at a time which would generate a second level of grid visualization that shows the land-to-land scores for all 36 comparisons within a bullet. Interacting with this visualization, we can now pull up score tables, profiles, location of grooves, aligned signatures and raw images. The framework of interactions, allows for validation of classification recommended by the RF model as well as gives an opportunity to critically asses, identify the cause and diagnose any problems encountered in the bullet matching pipeline. Figure 4.132: An instance of the interactive visualizations for communicating results 4.6 Explainable results: Usability and Trust survey Brief background of what the App is doing in terms of FTEs The explanation system and the framework which is designed to build trust, better understanding, and diagnose problems. The plan- a focus group (now virtual and online) need for anonymity- why? possible way to set this up Questionnaires before and after experiment - standard examples Specific questions, for the generated b-to-b and land-to-land comparisons to scale how well critical analysis can lead to trust, understanding and diagnosing problems. Other issues on the timeline - IRB approval for Virtual setups? Connecting survey questions: 1. Trust related questions 2. Linking possible questions to the conceptual cognitive theory 3. Best set of questionnaires need to be figured out. 4.6.1 Measuring Trust and calibration Figure 4.133: Questionnaire setup for comparing trust in scores Trust: Means different things to different people Outright questions measuring trust are highly susceptible to prior understanding and bias Trust in scores with visualizations as explanations Pre- questionnaire along with Score without explanations of comparisons Post- questionnaire after experiencing a methodical array of visualizations (representations) Framework design (Literature based): Optimized design with factors that can influence trust in scores. Criteria: Aligned to the steps of the model, present ordered set of visualization, so that the inference from the score, is drawn under the light of the right kind of uncertainty. Exploratory understanding of impact of problems Theoretical arguments defining change of trust within the visualization setup (application) and the interest in change of trust 2- Agent game; 1st Agent [1] - System ; 2nd Agent [2] - Human/ Domain expert For Agent [1]- Trust in Agent [2] meaningless; For Agent [2]- Trust in Agent [1] of concern Each addition of visualization means Agent [2] sequentially always comes 2nd; assesses the score in the light of new visualization. Leads to a pure prediction problem:: Hence, change in trust + or - within the system. (*) These considerations are important as we only look to quantify trust in the light of a change and not in the light of absolute magnitude, which is not trivial. Trust is confusing term before jumping into measuring trust. Need to establish some ground rules that examiners can self-assess (not necessarily explicitly) during the experience. Without this, post- questionnaire based outright questions on trust can have widely unimaginable answers not representing the true calibration of trust. Trust here is identified as a function of uncertainty around scores Trust is a function of a non-zero initial trust in model scores, and a zero trust in model scores if acceptance of facts and reason is possible. Trust a function of facts and implication of fact of scores. In the light of above points, trust is a function of detectable problems in each visualization Probability of causation, argument, brings to light the concept of change in uncertainty around the scores. Seen to be in this light. Fact: Detectable problems Pre- vs Post- questionnaire as a paired Wilcox ranked sum or paired t- test Assumptions? for a difference in mean of 1 need about 15 participants IRB requisites 4.6.2 Conference Presentations 4.6.2.1 American Academy of Forensic Sciences “Validation Study on Automated Groove Detection Methods in 3D Bullet Land Scans” February 2019 Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann Presentation given by Kiegan Rice 4.6.2.2 Association of Firearms and Toolmark Examiners Annual Training Seminar Heike’s talk “Reproducibility of Automated Bullet Matching Scores Using High-Resolution 3D LEA Scans” May 2019 Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann Presentation given by Kiegan Rice 4.6.2.3 Joint Statistical Meetings “A non-parametric test for matching bullet striations: extending the chumbley score for bullet-to-bullet matching” July 2019 Authors:Ganesh Krishnan, Heike Hofmann Talk given by Ganesh Krishnan “Repeatability and reproducibility of automated bullet comparisons using high-resolution 3D scans” July 2019 Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann Poster presented by Kiegan Rice 4.6.2.4 Miscellaneous 10th International Workshop on Statistics and Simulation in Salzburg, Austria, September 2019 “Reproducibility of High-Resolution 3D Bullet Scans and Automated Bullet Matching Scores” Authors: Kiegan Rice, Ulrike Genschel, Heike Hofmann Poster presented by Kiegan Rice, won 2nd Springer Poster Award “Case Study Validations of Automatic Bullet Matching” Authors: Heike Hofmann, Susan VanderPlas Presentation given by Alicia Carriquiry 4.7 People involved 4.7.1 Faculty Heike Hofmann Susan VanderPlas 4.7.2 Graduate Students Ganesh Krishnan Kiegan Rice Nate Garton Charlotte Roiger Joe Zemmels Yawei Ge Wangqian (Will) Ju 4.7.3 Undergraduates Talen Fisher (fix3p) Andrew Maloney Mya Fisher, Allison Mark, Connor Hergenreter, Carley McConnell, Anyesha Ray (scanner) References "],
["project-g-handwriting-signatures.html", "Chapter 5 Project G: Handwriting (&amp; Signatures) 5.1 Data Collection 5.2 Computational Tools 5.3 Statistical Analysis 5.4 Communication of Results 5.5 People involved", " Chapter 5 Project G: Handwriting (&amp; Signatures) The handwriting project has four major focuses: data collection computational tools statistical analysis glyph clustering closed set modeling for writer identification Alexandra’s project open set modeling for writer identification from distribution of glyphs in clusters communication of results 5.1 Data Collection We are conducting a large data collection study to gather handwriting samples from a variety of participants across the world (most in the Midwest). Each participant provides handwriting samples at three sessions. Session packets are prepared, mailed to participants, completed, and mailed back. Once recieved, we scan all surveys and writing samples. Scans are loaded, cropped, and saved using a Shiny app. The app also facilitates survey data entry, saving that participant data to lines in an excel spreadsheet. As of September 2019, Marc and Anyesha are the primary contacts for the study. Phase 2 recruiting is underway. The first 90 complete writers were published: &gt; Crawford, Amy; Ray, Anyesha; Carriquiry, Alicia; Kruse, James; Peterson, Marc (2019): CSAFE Handwriting Database. Iowa State University. Dataset. https://doi.org/10.25380/iastate.10062203. Anyesha and I worked on a variety of data quality checks and batch processing prior to publication. Scanner cuts a portion of the left side of the documents off sporadically. Survey date checks. Batch removal of header. Figure 5.1: Batch process samples: rotate 0.05 degrees, turn black line pixels white, crop 450 pixels from top (under QR code), rename. We hit a bump with the ISU Office for Responsible Research (ORR). They had concerns with the demographic variables we collected on the surveys and wanted to publish. We \"negotiated’’ and settled on the following survey information. A data article was accepted at Data in Brief. Crawford, A., Ray, A., &amp; Carriquiry, A. (2020). A database of handwriting samples for applications in forensic statistics. Data in brief, 28, 105059. 5.2 Computational Tools handwriter is a developmental R package hosted at https://github.com/CSAFE-ISU/handwriter. It is our major computational tool for the project. The package takes in scanned handwritten documents and the following are performed. Binarize. Turn the image to pure black and white. Skeletonize. Reduce writing to a 1 pixel wide skeleton. Break. Connected writing is decomposed into small manageable pieces called glyphs . Glyphs are graphical structures with nodes and edges that often, but not always, correspond to Roman letters, and are the smallest unit of observation we consider for statistcal modelling. Measure. A variety of measurements are taken on each glyph. See Section 4.2.1. Figure 5.2: Connected text processed by handwriter. The grey background is the original pen stroke. Colored lines represent the single pixel skeleton with color changes marking glyph decomposition. Red dots mark endpoints and intersections of each glyph. For an input document, functions in the package give back a list of glyphs with path and node location information, adjacency grouping assignment, centroid locations, measurements (such as slope, pictured below), among other things. More on measurements in Section 4.2.1. Figure 5.3: A visual of the ‘’slope’’ calculation for two glyphs. 5.2.1 Graph measurements (extracted in RCpp by James) The goal is to extract features that will discrimiate between writers within clusters. 1. Compactness measurements \\[\\sum_{Num.\\: Pixels}\\frac{d(pixel,centroid)^2}{Num.\\: Pixels -1}\\] While informative for comparing like-sized graphs, this measurement suffers from the inherit spacial dependence between pixels and (unfortunately) does not scale well for modelling. Figure 5.4: A visual of the ``compactness’’ calculation for graphs 2. Rotation angle measurements The first principal component for each graph, or “letter”, is calculated through an eigendecomposition of the covariance matrix. The first principal component enjoys the property of having two invariant directions, and so we take the vector that lies in the upper half plane. The rotation angle of the graph is defined as the angle between the vector (0,1), and the unit principal component vector on the upper half plane. The angle is measured in radians and is considered a feature of the graph defined on the interval (0,\\(\\pi\\)). Figure 5.5: Top: writing from one of Writer 95’s training documents. Bottom: the graphs from handwriter that correpsond to the writing, and the unit vector in the first pricipal direction. Red vectors are rotated by \\(\\pi\\) to lie in the upper half plane prior to calculation of the rotation angle from the positive x-axis. Figure 5.6: Data: All Cluster #29 graphs in Writer 95’s training documents. Left: A Nightingale Rose diagram displaying the rotation angles in 20 ‘petals’. A point for each observed rotation angle is plotted on the outermost ring of the diagram. Right: A traditional histogram of the rotation angles with 20 bins, a point for each observation is plotted on the x-axis. Figure 5.7: Top: writing from one of Writer 1’s training documents. Bottom: the graphs from handwriter that correpsond to the writing, and the unit vector in the first pricipal direction. Red vectors are rotated by \\(\\pi\\) to lie in the upper half plane prior to calculation of the rotation angle from the positive x-axis. Figure 5.8: Nightingale rose diagrams and traditional histograms for the rotation angles from Writer #1’s training documents in Clusters #29 (top) and #16 (bottom). 3. Loop measurements Figure 5.9: A few graphs with loops that come from the Writer 95 sample from Figure 4.5 (top). Figure 5.10: Distribution of loop ratios for Writer 95 across all training documents. Figure 5.11: Distribution of log loop ratios for Writers 1 and 95 across all training documents. 5.3 Statistical Analysis 5.3.1 Clustering Paper submitted to ASA Statistical Analysis and Data Mining. Rather than impose rigid grouping rules (the previously used ‘’adjacency grouping’’) we consider a more robust, dynamic \\(K-\\)means type clustering method that is focused on major glyph structural components. Clustering algorithms require: A distance measure. For us, a way to measure the discrepancy between glyphs. A measure of center. A glyph-like structure that is the exemplar representation of a group of glyphs. Glyph Distance Measure We begin by defining edge to edge distances. Edge to edge distances are subsequently combined for an overall glyph to glyph distance. Edge to edge distances: Consider the following single edge glyphs, \\(e_1\\) and \\(e_2\\). Make \\(3\\) edits to \\(e_1\\) to match \\(e_2\\). The combined magnitude of each edit make the edge distance. Figure 5.12: Two edges that are also glyphs, \\(e_1\\) and \\(e_2\\). Shift. Anchor to the nearest endpoint by shifting. Figure 5.13: Shift = 1.4. Stretch. Make the endpoints the same distance apart. Figure 5.14: Stretch = 9.9. Shape. Bend and twist the edge using \\(7\\) shape points. Shape points are ‘’matched’’ and the distance between them is averaged to obtain the shape contribution to the distance measure. Figure 5.15: Shape Components. Figure 5.16: Shape = 8.4. Combine the three measurements for a final edge to edge distance. D\\((e_1, e_2) = 1.4 + 9.9 + 8.4 = 19.7\\). For multi-edge glyphs get pairwise edge distances. Minimize total glyph distance using linear programming to match edges (sudoku). Down-weight edge distance contributions based on edge lengths. There are nuances associated with all of this. We must handle glyphs with differing number of edges, and consider the direction in which edges are compared (endpoint labels are arbitrary). All of this is addressed in detail in the paper. Measure of glyph centers Take the weighted average of endpoints, \\(7\\) shape points, and edge length. Figure 5.17: Weighted mean between two glyphs. p = weight on blue. The mean of a set can be iteratively calculated by properly weighting each newly introduced glyph. For stability, the \\(K-\\)means algorithm finds the glyph nearest the mean and uses that as measure of cluster center. \\(K-\\)means Algorithm for Glyphs Implement a standard \\(K-\\)means12 with handling of outlier observations.3 Begin with fixed \\(K\\) and a set of exemplars. Iterate between the following steps until cluster assignments do not change: Assign each glyph to the exemplar it is nearest to, with respect to the glyph distance measure. Calculate each cluster mean as defined. Find the exemplar nearest the cluster mean and use it as the new cluster center. Figure 5.18: One of \\(K=40\\) Clusters. Exemplar &amp; cluster members (left). Cluster mean (right). Use one document from each training writer to cluster and obtain a template. Make cluster assignments for the remaining documents by finding the template exemplar each glyph is nearest to. Below is an example of data that arises from the clustering method. Figure 5.19: Data arising from the cluster grouping method. Cluster #’s ordered by most to least populated. 5.3.1.1 Outliers During clustering, outliers are considered glyphs that are at least \\(T_o\\) distance units from cluster exemplars. The algorithm sets a ceiling \\(n_o\\) on the allowable number of outliers. Initially, we were going to ignore the outliers, but now they are considered the \\(K+1\\)th cluster and contribute information about writer. Figure 5.20: Cluster of outliers. 5.3.1.2 CVL + CSAFE + IAM Template Development Template development on: 1. CSAFE Handwriting Database, 25 documents. 2. Computer Vision Lab (CVL) Database, 25 documents. 3. IAM Handwriting Database, 50 documents. With the new template, we’ve made a few adjustments to the original algorithm and measurement tools: Starting values. Changes to the distance measurement to emphasize shape, and focus less on character size. \\(\\frac{1}{2}\\times\\) Stretch (straight line distance component, ~ size) 1 \\(\\times\\) Shift (nearest endpoint) 2 \\(\\times\\) Shape (ghost edge comparisons)\\(^2\\) A sample template is picture below. 27 CVL documents (6 unique), 73 CSAFE documents (2 unique) Currently working on 20+ algorithm runs with a variety of starting values to select final template. The final template. Figure 5.21: Exemplars for the first 20 most populated clusters (according to training data, later). The red lines are not plotting correctly.. Figure 5.22: Exemplars for the first 20 least populated clusters (according to training data, later). The red lines are not plotting correctly.. 5.3.2 Statistical Modeling Model #1, Straw Man Let, \\(\\boldsymbol{Y}_{w(d)} = \\{Y_{w(d),1}, \\dots, Y_{w(d),K}\\}\\) be the number of glyphs assigned to each cluster for document within writer, \\(w(d),\\) and \\(\\boldsymbol{\\pi}_w = \\{\\pi_{w,1}, \\dots, \\pi_{w,K} \\}\\) be the rate at which a writer emits glyphs to each cluster. Here, \\(K = 41\\), \\(d = 1,2,...,5\\), \\(w = 1,...,27\\). Then a hierarchical model for the data is \\[\\begin{array}{rl} \\mathbf{Y}_{w(d)}\\: &amp;\\stackrel{ind}{\\sim} \\:Multinomial(\\boldsymbol\\pi_{w}), \\\\ \\boldsymbol{\\pi}_w \\:&amp;\\stackrel{ind}{\\sim}\\: Dirichlet(\\boldsymbol{\\alpha}),\\\\ \\alpha_{1}, \\dots, \\alpha_{K}\\: &amp;\\stackrel{iid}{\\sim}\\: Gamma(2, \\:0.25). \\label{model_line3} \\end{array}\\] Posterior Predictive Analysis For a holdout document of unknown source, \\(w^*\\), Extract glyphs with and assign groups, \\(\\boldsymbol{Y}^*_{w^*}=\\{Y_{w^*,1}, ... Y_{w^*,K}\\}\\) Assess the posterior probability of writership under each known writer \\(\\boldsymbol\\pi_{w}\\) vectors. i.e. \\(p(w^* = w^\\prime|\\boldsymbol{Y^*}, \\boldsymbol{Y})\\) for each writer \\(w^\\prime\\) in the training data. The posterior probability that the questioned document \\(\\boldsymbol{Y}^*_{w^*}\\) belongs to writer \\(w^\\prime\\) with respect to the closed set is, \\[\\begin{array}{rl} p(w^* = w^\\prime|\\boldsymbol{Y}^*, \\boldsymbol{Y}) &amp; \\propto p(\\boldsymbol{Y}^*| w^* = w^\\prime, \\boldsymbol{Y}) p(w^*=w^\\prime | \\boldsymbol{Y})\\nonumber\\\\ &amp; \\propto p(\\boldsymbol{Y}^*| w^* = w^\\prime, \\boldsymbol{Y}) \\nonumber\\\\ &amp; = \\int p(\\boldsymbol{Y}^*| \\boldsymbol{\\pi}_{w^\\prime}) p(\\boldsymbol{\\pi}_{w^\\prime}| \\boldsymbol{Y}) d \\boldsymbol{\\pi}_{w^\\prime}\\nonumber\\\\ &amp; \\approx \\frac{1}{M} \\sum_{m=1}^M p(\\boldsymbol{Y}^*| \\boldsymbol{\\pi}^{(m)}_{w^\\prime}) , \\mbox{ where } \\boldsymbol{\\pi}^{(m)}_{w^\\prime} \\sim p(\\boldsymbol{\\pi}_{w^\\prime}|\\boldsymbol{Y}) \\nonumber \\end{array}\\] for MCMC iterations \\(m = 1, \\dots, M\\). Then, for a given iteration \\(m\\), \\[ p(\\boldsymbol{Y}^*| \\boldsymbol{\\pi}^{(m)}_{w^\\prime}) = p_{w^\\prime}^{(m)} = \\frac{Mult(\\boldsymbol{Y}^*; \\boldsymbol{\\pi}_{w^\\prime}^{(m)})}{\\sum_{w_i = 1}^{27}Mult(\\boldsymbol{Y}^*; \\boldsymbol{\\pi}_{w_i}^{(m)}) }. \\nonumber \\] Calculate the quantitity for each known writer \\(w^\\prime = w_1, \\dots, w_{27}\\) in training set to get \\[ \\boldsymbol{p}^{(m)} = \\{p_{w_1}^{(m)}, ..., p_{w_{27}}^{(m)}\\}, \\] and compute summaries over the MCMC draws, \\[ \\boldsymbol{\\bar{p}} = \\{\\bar{p}_{w_1},...,\\bar{p}_{w_{27}}\\}. \\] Results If we use a single holdout document for each writer (doc 4). The \\(\\boldsymbol{\\bar{p}}\\) vector as given above is shown graphically for each holdout document (the rows). Figure 5.23: Posterior predictive results. Rows are evaluated independently and the probability in each row sums to one. Log Loss = 0.2013 We can use a cross validation routine to estimate error. For each writer, shuffle the order of their body of documents. For the first fold we holdout the first document for testing, for the second fold we hold out the second document for testing, etc.. This yields 6 results analogous to that of the figure above. Figure 5.24: Posterior predictive results for each fold of the CV routine outlined above. Log Loss is clearly not a great summary of performance. Evaluating Over-/Under-dispersion (Writer Variability Index) With a count data model it is important to investigate the presence of over- and under-dispersion. An index of intra-writer variability with respect to a model. Multivariate Generalized Dispersion Index (GDI) of Kokonendji and Puig4. Relative Dispersion Index (RDI) = \\(\\frac{\\mbox{writer data GDI}}{\\mbox{model simulated GDI}}.\\) High RDI \\(\\implies\\) data over-dispersion and high sample to sample variability. Figure 5.25: RDIs for the 27 CVL writers under Model #1. High RDI \\(\\implies\\) data over-dispersion &amp; high sample to sample variability (in the sense of glyph cluster membership rates). Figure 5.26: Handwriting Samples from Writer #12. Notice the repeated patterns in letters ‘a’, ‘e’, ‘f’, ‘g’, ‘h’, ‘t’, etc. across samples. Figure 5.27: Handwriting Samples from Writer #23. Note the red ’h’s, green ’ll’s, purple terminal ’y’s, and blue ’loo’s Model #2, Mixture There seem to be greatly varying shapes in the relative frequency of cluster fill. We think maybe a more flexible Dirichlet distribution is warranted. Figure 5.28: Average relative frequency of cluster fill for the 8 most common clusters. Notice writers 12/21 and 6/17/18. Add flexibility to the original model by including a mixture component in the Dirichlet parameter space. \\[\\begin{array}{rl} \\mathbf{Y}_{w(d)} | \\boldsymbol\\pi_{w}, w\\: &amp;\\stackrel{ind}{\\sim} \\:Multinomial(\\boldsymbol\\pi_{w}), \\\\ \\boldsymbol{\\pi}_w|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}, \\rho_w \\:&amp;\\stackrel{ind}{\\sim}\\:Dirichlet(\\rho_w \\: \\boldsymbol{\\alpha}+ (1-\\rho_w) \\: \\boldsymbol{\\beta}), \\\\ \\alpha_{k}, \\: \\beta_{k} \\: &amp;\\stackrel{iid}{\\sim}\\: Gamma(2, \\: 0.25) \\:\\: \\mbox{ for } k &gt; 3, \\\\ \\rho_w\\: &amp; \\stackrel{iid}{\\sim}\\: Beta(1,1) \\end{array}\\] Label Switchin Issues. The model is non-identifiabile. We need to either place constraints on the parameter space to exclude the possibility of a second mode apriori, or estimate the posterior in both modes and post-process for label reassignment after. This has been resolved (?) by placing constraints on the first three elements of \\(\\alpha\\) and \\(\\beta\\) with moderately informative priors. \\(\\mathbf{\\alpha_1 &lt; \\beta_1}\\), \\(\\quad\\) \\(\\alpha_1 \\sim Gamma(2, 0.25) \\:\\: \\&amp; \\:\\: \\beta_1 \\sim Gamma(4, 0.25)\\) \\(\\mathbf{\\alpha_2 &gt; \\beta_2}\\) , \\(\\quad\\) \\(\\alpha_2 \\sim Gamma(4, 0.25) \\:\\: \\&amp; \\:\\: \\beta_2 \\sim Gamma(1, 0.25)\\) \\(\\mathbf{\\alpha_3 &gt; \\beta_3}\\) , \\(\\quad\\) \\(\\alpha_3 \\sim Gamma(4, 0.25) \\:\\: \\&amp; \\:\\: \\beta_3 \\sim Gamma(1, 0.25)\\) Results The mixing component gives insights into writing style. Figure 5.29: Average ho_w value for each writer. Figure 5.30: Writing samples in a variety of styles based on the mixing parameter. From top to bottom, writer #’s 21, 4, 29, 20, 18. Evaluate the posterior probability of writership for each of the holdout documents. Figure 5.31: Posterior predictive results. Rows are evaluated independently and the probability in each row sums to one. Log Loss = 0.2078 5.3.2.1 Model #2, CSAFE + CVL (not most recent) Template The following analysis was done for the first 20 CSAFE writers. Training docs: s01_pLND_r01, s01_pLND_r03, s01_pWOZ_r02, s01_pPHR_r03 Testing doc: s01_pLND_r02 Figure 5.32: Average relative frequency of cluster fill for the 8 most common clusters. Posterior probability results. Figure 5.33: Posterior predictive results. Rows are evaluated independently and the probability in each row sums to one. Not surprising. Working on: More writers (need to go through handwriter) Testing on the phrase prompt Including not yet seen CVL writers in analysis. Model #3, Normal Slopes. Work in progress. Adding slope measurements into the model. Let, \\(\\boldsymbol{Y}_{w(d)} = \\{Y_{w(d),1}, \\dots, Y_{w(d),K}\\}\\) be the number of glyphs assigned to each group for document within writer, \\(w(d),\\) \\(\\boldsymbol{\\bar{S}}_{w(d)} = \\{\\bar{S}_{w(d),1}, \\dots, \\bar{S}_{w(d),K}\\}\\), be the average slope in each group for document \\(w(d)\\) \\(G_{w(d),j}\\) be the \\(j^{th}\\) glyph in document \\(w(d)\\), \\(g_{w(d), j} = 1, \\dots, 41\\) be the available cluster assignments for the \\(j^{th}\\) glyph in the document, \\(S_{w(d),j}\\) be the slope of the \\(j^{th}\\) glyph in the document. Here, \\(K = 41\\), \\(d = 1,2,...,5\\), \\(w = 1,...,27\\), \\(j= 1, \\dots,\\mathrm{J}_{w(d)}.\\) \\[ \\bar{S}_{w(d),k}= \\frac{1}{Y_{w(d),k}} \\sum_{j \\ni \\mathrm{I}[g_j = k]}S_{w(d),j} \\quad \\&amp; \\quad Y_{w(d),k} = \\sum_{j = 1}^{\\mathrm{J}_{w(d)}} \\mathrm{I}[g_j = k] \\] Then a hierarchical model for the data is \\[\\begin{array}{rl} \\mathbf{Y}_{w(d)} | \\boldsymbol\\pi_{w}, w\\: &amp;\\stackrel{ind}{\\sim} \\:Multinomial(\\boldsymbol\\pi_{w}), \\\\ \\boldsymbol{\\pi}_w|\\boldsymbol{\\alpha} \\:&amp;\\stackrel{ind}{\\sim}\\:Dirichlet(\\boldsymbol{\\alpha}), \\\\ \\alpha_k\\: &amp;\\stackrel{iid}{\\sim}\\: Gamma(a = 2, \\: b= 0.25), \\\\ \\bar{S}_{w(d),k} | Y_{w(d),k}, w, \\mu_{w, k}, \\sigma^2_{k} \\: &amp; \\stackrel{iid}{\\sim}\\: N \\left(\\mu_{w, k}, \\frac{\\sigma^2_{k}}{Y_{w(d),k}}\\right) \\\\ \\mu_{w, k} | \\mu_{k} \\: &amp; \\stackrel{iid}{\\sim}\\: N(\\mu_{k}, 3) \\\\ \\mu_{k} \\: &amp; \\stackrel{iid}{\\sim}\\: N(0, 3) \\\\ 1/\\sigma^2_{k}\\: &amp; \\stackrel{iid}{\\sim}\\: Gamma(0.5, 0.1) \\end{array}\\] Consider a new document with cluster fill counts \\(\\boldsymbol{Y}^*\\) and average slope measurements \\(\\boldsymbol{\\bar{S}}^*\\) from unknown writer \\(w^*\\). Posterior Predictive Analysis Posterior probability that the questioned document belongs to writer \\(w^\\prime\\) with respect to the closed set is: \\begin{array}{rl} &amp;p(w^* = w|* ^, ) \\ &amp;p(^ | w^* = w^, ) \\ &amp; _{m=1}^{M} Mult(^*; _{w}{(m)}) \\end{array} for MCMC samples \\(m = 1, \\dots, M\\). Results Evaluate the posterior probability of writership for each of the holdout documents. Figure 5.34: Posterior predictive results. Rows are evaluated independently and the probability in each row sums to one. Log Loss = 0.0883. Modeling Summary Model Data LogLoss #1 Adjacency Grouping (not presented) 0.5413 #1 Cluster Grouping 0.2013 #2 Cluster Grouping 0.2078 #3 Cluster Grouping &amp; Glyph Slopes 0.0883 5.3.2.2 Modeling update (Feb 17) 90 writers, Session #1 Train: 3 London Letters A few writers only had 2 (working on this) Test: 1 Wizard of Oz Recall rotation angle measurements: Figure 5.35: Data: All Cluster #29 graphs in Writer 95’s training documents. Left: A Nightingale Rose diagram displaying the rotation angles in 20 ‘petals’. A point for each observed rotation angle is plotted on the outermost ring of the diagram. Right: A traditional histogram of the rotation angles with 20 bins, a point for each observation is plotted on the x-axis. A NAIVE Model for Rotation Angles Naively we can treat the rotation angle values as if they are linear on the interval (0,\\(\\pi\\)). This is of course not the case, since a value of 0 is extremely similar to a value of \\(\\pi\\), both corresponding to letters that do not lean a great deal to the left or right, and are wider than they are tall. This can be seen in Figure 2, where the unimodal density ends abuptly at zero and “wraps around” in the higher values near \\(\\pi\\). Nonetheless, we can specify a disribution on the linear, or unwrapped, rotation angles as follows: \\[\\begin{align} \\nonumber \\mathbf{Y}_{w(d)} | \\boldsymbol\\pi_{w}, w \\: &amp;\\stackrel{ind}{\\sim} \\:Multinomial(\\boldsymbol\\pi_{w}), \\\\ \\nonumber \\boldsymbol{\\pi}_w|\\boldsymbol{\\gamma} \\: &amp;\\stackrel{ind}{\\sim}\\:Dirichlet(\\boldsymbol{\\gamma}), \\\\ \\nonumber \\gamma_k\\: &amp;\\stackrel{iid}{\\sim}\\: Gamma(a = 2, \\: b= 0.25), \\\\ \\nonumber &amp; \\quad \\\\ \\nonumber SRA_{w,k} | w,k \\: &amp; \\stackrel{iid}{\\sim}\\: Beta \\left( \\alpha_{w,k}, \\beta_{w,k} \\right) \\\\ \\nonumber &amp; \\quad \\\\ \\nonumber \\mu_{w,k} &amp;= \\frac{\\alpha_{w,k}}{\\alpha_{w,k}+\\beta_{w,k}} \\: \\sim Beta(2,2) \\\\ \\nonumber variance_{w,k} &amp;= \\frac{\\mu_{w,k}(1 - \\mu_{w,k})}{\\alpha_{w,k} + \\beta_{w,k} + 1} = \\frac{\\mu_{w,k}(1 - \\mu_{w,k})}{\\kappa_{w,k}+1} \\\\ \\nonumber \\kappa_{w,k} &amp;= \\alpha_{w,k} + \\beta_{w,k} \\: \\sim Gamma(shape = 0.1, rate = 1) \\end{align}\\] Where \\(SRA_{w,k,i}, \\: \\:i = 1, \\dots, n_{w,k}\\), are rotation angles from cluster \\(k\\), and writer \\(w\\), scaled to the beta density support of (0,1). For a given writer and cluster, \\(\\mu_{w,k}\\) is the mean of the data distribution, expressed as a functions of \\(\\alpha_{w,k}\\) and \\(\\beta_{w,k}\\). We place a weak prior, \\(Beta(2,2)\\), on all \\(\\mu_{w,k}\\). Similarly, \\(\\kappa_{w,k}\\) is expressed as a function of \\(\\alpha_{w,k}\\) and \\(\\beta_{w,k}\\) and is a sort of “sample size”, or “precision” parameter. Notice that \\(\\kappa_{w,k}+1\\propto\\frac{1}{variance_{w,k}}\\). Figure 5.36: Writer 95, Cluster 29. A plot depicting elements of the data model \\(SRA_{w,k} \\: \\sim \\: Beta ( \\alpha_{w,k}, \\beta_{w,k})\\). Top: values of \\(\\alpha^{(m)}_{95,29}\\) and \\(\\beta^{(m)}_{95,29}\\) for \\(m = 1, \\dots,12\\) iterations taken from the middle of the chain and used to plot the \\(12\\) correpsonding beta densities. Bottom: the unscaled rotation angles (unscaled \\(SRA_{w,k}\\)), serving as data for the parameter estimation. For a different writer who does not tend to lean one way or another… Figure 5.37: Nightingale rose diagrams and traditional histograms for the rotation angles from Writer #1’s training documents in Clusters #29 (top) and #16 (bottom). And we can look at a handful of beta distributions that arise from \\(w=1\\) and \\(k=16\\)… Figure 5.38: Writer 1, Cluster 16. A plot depicting elements of the data model \\(SRA_{w,k} \\: \\sim \\: Beta ( \\alpha_{w,k}, \\beta_{w,k})\\). Top: values of \\(\\alpha^{(m)}_{1,16}\\) and \\(\\beta^{(m)}_{1,16}\\) for \\(m = 1, \\dots,12\\) iterations taken from the middle of the chain and used to plot the \\(12\\) correpsonding beta densities. Bottom: the unscaled rotation angles (unscaled \\(SRA_{w,k}\\)), serving as data for the parameter estimation. Why am I showing you histograms and MCMC samples from clusters #29 and #16?? Becasue these are great examples of where the problems occur with the naive approach to modeling the “unwrapped” angles. Figure 5.39: Cluster members in grey, and cluster exemplars in red (plotting is a few pixels shifted left) for clusters \\(\\#29\\) (left), and \\(\\#26\\) (right) These are clusters that tend to hold the letters \\(n\\) and \\(u\\) and thus, are generally wider than they are tall. When a writer emits one of these nearly straight up and down, but slightly right leaning, the rotation angle will be just above zero. If the graph is very slightly left leaning, then the rotation angle will be just under \\(\\pi\\). While the beta distribution can take on a \\(U\\)-shape (like the Writer 1, Cluster 16 data in Figure 4), this is still undesirable, becuase we want to express the fact that values near \\(0\\) and \\(\\pi\\) are actually very similar. For example, a mean near \\(\\pi/2\\) does not do a good job of expressing the center of those rotation angles, and a traditional “unwrapped” variance will overstate the variability in data that is actually quite similar. Consider the following distribution of cluster #16 rotation angles from writer #85’s questioned document. The largest observation is 3.129, which scales to 0.996, the next largest is 3.024, which scales to 0.963. These are going to accumulate a huge amount of likelihood when evaluated under the beta parameters for Writer #1, displayed in Figure 6. The rest of the rotation angles are relatively spread out over the \\(x\\)-axis, and not necessarily reflective of the Writer #1 distribution in Cluster #16. Figure 5.40: Questioned Document. Writer #85. Cluster #16. QD evaluation under beta distributed rotation angles using \\(k=1,\\dots,20\\), the \\(20\\) most populated clusters. Notice the probability for questioned writer #85 that is assigned to known writer #1. This error is explained by the behavior depicted above. Not all off-diagonal probability assignment can be attributed to similar behavior, but much of it can. Figure 5.41: Posterior predictive evaluation of QDs under beta distributed rotation angles. A Wrapped Model for Rotation Angles A more appropriate approach to the problem is to consider the rotation angles in the polar coordinate system and treat them as spanning the full circle. So we map the upper half plane values to \\((0,2\\pi)\\), where values above the \\(x-\\)axis indicate a right leaning graph and below the \\(x-\\)axis indicate left leaning graphs. Graphs that are relatively straight up and down will have values near \\(0/2\\pi\\) if they are wider than they are tall, and near \\(\\pi\\) if they are taller than they are wide. Figure 5.42: Writer 1, Cluster 16. Left: linear consideration of the rotation angles. Right: wrapped rotation angles on the full circle. Figure 5.43: Two candidate distributions defined on circular data, both specified at MLE parameter values calculated from the data. We consider two distributions that are appropriate for circular data. First, the von Mises distribution is a close approximation to the wrapped normal distribution, which is the circular analouge of the normal. This is the “go-to” unimodal wrapped distribution, like the normal is the traditional “go-to” unimodal distribution. It is specified through the mean, \\(\\mu\\), and concentration, \\(\\kappa\\) (\\(\\frac{1}{\\kappa}\\) analogous to \\(\\sigma^2\\)). The second is the wrapped Cauchy distribution, which is the wrapped version of the Caucy distribution. Similar to the traditional Cauchy, this distribution is symmetric, unimodel, and specified by a location parameter, \\(\\mu\\), and concentration parameter, \\(\\rho\\). This distribution is also heavy-tailed in the sense that it will place more density on the “back” of the circle opposite the peak of the distribution. Now, consider \\[\\begin{align} \\nonumber \\mathbf{Y}_{w(d)} | \\boldsymbol\\pi_{w}, w \\: &amp;\\stackrel{ind}{\\sim} \\:Multinomial(\\boldsymbol\\pi_{w}), \\\\ \\nonumber \\boldsymbol{\\pi}_w|\\boldsymbol{\\gamma} \\: &amp;\\stackrel{ind}{\\sim}\\:Dirichlet(\\boldsymbol{\\gamma}), \\\\ \\nonumber \\gamma_k\\: &amp;\\stackrel{iid}{\\sim}\\: Gamma(a = 2, \\: b= 0.25), \\\\ \\nonumber &amp; \\quad \\\\ \\nonumber RA_{w,k} | w,k \\: &amp; \\stackrel{iid}{\\sim}\\: Wrapped \\: Cauchy \\left( \\mu_{w,k}, \\phi_{w,k} \\right) \\\\ \\nonumber \\mu_{w,k} &amp;\\sim Uniform(0,2\\pi) \\\\ \\nonumber \\phi{w,k} &amp;\\sim Uniform(0,1) \\\\ \\nonumber \\end{align}\\] Where \\(RA_{w,k,i}, \\: \\:i = 1, \\dots, n_{w,k}\\), are rotation angles from cluster \\(k\\), and writer \\(w\\), on the full circle \\((0,2\\pi)\\). For a given writer and cluster, \\(\\mu_{w,k}\\) and \\(\\phi_{w,k}\\) are the location and concentration parameters, respectively. We place non-informative uniform priors on each. Again, there is no borrowing set up in the model for rotation angles. Each writer/cluster combination gets its own estimated rotation angle distribution. QD evaluation under wrapped Cauchy distributed rotation angles, using \\(k=1,\\dots,20\\), the \\(20\\) most populated clusters. Notice the probability for questioned writer #85 that was previously assigned to known writer #1 is no longer an issue. Figure 5.44: Posterior predictive evaluation of QDs under wrapped Cauchy distributed rotation angles. 5.3.3 Interesting documents 5.3.3.1 Where do we miss? We tend to do worse on writers who only had 2 training documents. Writers 32 and 133. Figure 5.45: All 40 documents, wrapped cauchy rotation angles, where do we miss? Samples: Writer 32 Test Figure 5.46: Writer 32 Test Writer 133 Train Figure 5.47: Writer 133 Train 5.3.3.2 Writer 4 Data London Letter #1 Figure 5.48: Writer 4 data. London Letter #2 Figure 5.49: Writer 4 data. London Letter #3 Figure 5.50: Writer 4 data. 5.3.3.3 You all! Figure 5.51: CSAFE writers. Wolfgang Figure 5.52: A training sample from Wolfgang. Soyoung Figure 5.53: A training sample from Soyoung. Other writer Figure 5.54: A training sample from the other writer with off-diagonal assignment for Wolfgang. 5.3.4 Ali’s project :) 5.3.5 Open Set Modeling for Writer Identification from Distribution of Glyphs in Clusters The next step in this research extends the scope of potential sources of handwriting samples to an open set. So if there are two handwriting samples, are what is the probability they are from the same writer or different writer without the neccesary assumption of knowing all of the potential sources. The distribution of glyphs assigned to clusters in a document is can be used to explore the authorship because it is hypothesized that a particular writer would have their own writing style resulting in similar distributions of glyphs in each cluster over time across many different writing samples. Instead of comparing multinomial distributions, the proportions of total glyphs in each cluster can be compared with two Dirichlet distributions. If there are two different samples where the length of the documents differ the expected number of glyphs per cluster would also differ. For example, if samples of handwriting were found at two different scenes and the goal is to compare the two documents, it would be expected that the number of glyphs in each cluster would vary unless they happened to write the exact same thing in both. Dirichlet distributions would compare the proportions instead of the counts of glyphs per cluster. To start we will look at a dirichlet with three proportions, which would correspond to only having three clusters, and calculate the euclidean distance. KDE (kernel density estimation) can be used to estimate the behavior of the distributions when the random variables are indeed from the distribution with the same parameters and when they are drawn from the distribution with different parameters. These density estimates can be used to calculate score-based likelihood ratios (SLRs) using leave-one-out of another simulated dataset. We would expect SLRs that are greater than 1 to be associated with the distance measures from the same distribution. This process was conducted with just the euclidean distance to start. Next, it is possible to assess how well a random forest with multiple distance measures (euclidean distance and the absolute difference between each of the clusters) can correctly identify if two dirichlet random variables are from a distribution with the same parameters or different parameters. To do this, 10,000 random variables were simulated for two dirichlet with the parameters (5,10,25) and one dirichlet with parameters (6,12,22) as a training set. The distance measures were calculated between the two dirichlet from the same parameters and also between different parameters. These distance measures were added to a dataset along with an indicator if the distance measures were from the same or different dirichlet. Next, a random forest was trained to predict the if the distance measures were from the same or different dirichlet. A testing dataset was simulated in the same way as before and the random forest was applied. The random forest returns a value between 0 and 1 which is the proportion of trees in the forest that predicts the distance measures are from the same dirichlet. The training dataset was split into those that were actually from the same and different parameters. The stat::density() uses kernel density estimation with a Gaussian kernel to estimate the probability density function of the proportions that predicted the dirichlet were simulated from the distrution with the same parameters after defining the range of the pdf to be between 0 and 1. Using the leave-one-out method, 1 observation was excluded from the dataset and then KDE was performed. The ratio of the KDE estimate for the curve if the excluded point was from the distribution with the same parameters to different parameters is computed. The SLRs are compared for those that are actually from the same vs different and we would expect there to be little overlap. The same process that was conducted with 3 clusters was repeated with 40 clusters. A random forest was trained on simulated data with 40 clusters with randomly simulated parameters drawn from a uniform distribution. There appears to be little overlap between the kernel density estimates that were drawn from the same distribution compared to those from different distributions. While the small SLRs are based on the distance measures from dirichlet with different parameters and larger SLRs are from the same as expected, the score likelihood ratios computed using the leave-one-out method are not as distinct of those from the same compared to different dirichlet distributions as we would expect. Application to CSAFE Database The data provided in the CSAFE database consist of writing samples from 90 writers who repeated three prompts three times throughout three sessions for a total of 9 writing samples per writer. The three prompts varied in length. “The London Letter” is the longest and has been used in other handwriting analyses because it includes all of the letters upper and lowercase and numbers. The second is an excerpt from “The Wizard of Oz” book. The last prompt is the short phrase: “The early bird may get the worm, but the second mouse gets the cheese”(Crawford, et al.). This initial test on the data only includes the documents from the first session. We repeated the process of evaluating the score-based likelihoods calculated from the kernel density estimates of the output a random forest previsouly applied to simulated data to the CSAFE handwriting data. We used 4 different prompts: London Letter, Wizard of Oz, short phrase, and all prompts per writer per repetition combined. The combined document is longer and would be expected to be more representative of the writing style of each writer in the study. For each data set, a random sample of 80% of the writers was drawn. The distance measures between all pairs of documents were calculated (absolute difference between each glyph and euchlidean distance) from sample. The number of distances from the same source were counted and the same number of different source distances were down sampled. This created a training set that includes an equal number of distances between same and different sources to train the random forest. The resulting random forest was trained using the remaining 20% of the writers not initially sampled for the training data set. As before with the simulated data, distributions of the output from the random forest by source was estimated using kernel density estimation with a Gaussian kernel with bounds between 0 and 1. SLRs were computed using the leave-one-out method as before. The first prompt is the short phrase. The next prompt is an excerpt from “The Wizard of Oz”. The third prompt is the “London Letter”. Lastly, all three documents are combined into one large template per writer. Comparing all four implementations of the process, the method did the best at correctly classifying same vs. different source with longer documents. The euclidean distance was the most important variable according to the mean decrease in Gini index. Additionally, when comparing the absolute difference by cluster, some clusters appear to be more influential across prompts, such as cluster 31. The kernel density estimates for the shortest prompt are not well separated. As the length of the writing sample increases, the KDEs between same and different sources become more distinct. It is important to note that the training set did down sample the number of different source differences, but the testing set did not so the number of SLRs in that are from different sources will be more than from the same (a pattern not seen in the simulated data). 5.4 Communication of Results Presenting author is in bold. 5.4.1 Papers “A Clustering Method for Graphical Handwriting Components and Statistical Writership Analysis” Authors: Nick Berry and Amy Crawford Submitted to The Annals of Applied Statistics in September 2019. “A Database of Handwriting Samples for Applications in Forensic Statistics” Authors: Anyesha Rey, Amy Crawford, and Alicia Carriquiry Submitted to Data in Brief in October 2019. “Bayesian Hierarchical Modeling for Forensic Handwriting Analysis” Authors: Amy Crawford, Alicia Carriquiry, and Danica Ommen In preparation for submission to PNAS. 5.4.2 Talks “Statistical Analysis of Handwriting for Writer Identification” August 2019 Authors: Amy Crawford, Nick Berry, Alicia Carriquiry, Danica Ommen American Society of Questioned Document Examiners (ASQDE) Annual Meeting in Cary, NC. “A Bayesian Hierarchical Mixture Model with Applications in Forensic Handwriting Analysis” July 2019 Authors: Amy Crawford, Nick Berry, Alicia Carriquiry Danica Ommen Joint Statistical Meetings (JSM) in Denver, CO. “Forensic Analysis of Handwriting” July 2019 Authors: Alicia Carriquiry, Amy Crawford, Nick Berry, Danica Ommen VI Latin American Meeting on Bayesian Statistics (VI COBAL), Lima, Peru. “Exploratory Analysis of Handwriting Features: Investigating Numeric Measurements of Writing” February 2019 Authors: Amy Crawford, Nick Berry, Alicia Carriquiry, Danica Ommen American Academy of Forensic Sciences (AAFS) Annual Meeting in Baltimore, MD. “Toward a Statistical and Algorithmic Approach to Forensic Handwriting Comparison” August 2018 Authors: Amy Crawford and Alicia Carriquiry American Society of Questioned Document Examiners (ASQDE) Annual Meeting in Park City, UT. “A Bayesian Approach to Forensic Handwriting Evidence” July 2018 Authors: Amy Crawford and Alicia Carriquiry Joint Statistical Meetings (JSM) in Vancouver, BC, Canada. “Bringing Statistical Foundations to Forensic Handwriting Analysis” May 2018 Authors: Amy Crawford and Alicia Carriquiry American Bar Association, 9th Annual Prescription for Criminal Justice Forensics Program in New York, NY. 5.4.3 Posters “A Bayesian Hierarchical Model for Forensic Writer Identification” September 2019 Authors: Amy Crawford, Alicia Carriquiry, Danica Ommen 10th International Workshop on Statistics and Simulation in Salzburg, Austria 1st Springer Poster Award Click for Poster Image “Statistical Analysis of Handwriting” May 2019 Authors: Amy Crawford and Nick Berry CSAFE Annual All-Hands Meeting in Ames, IA “Statistical Analysis of Letter Importance for Document Examination” February 2018 Authors: Amy Crawford and Alicia Carriquiry American Academy of Forensic Sciences in Seattle, WA YFSF Best Poster Award (Presented AAFS 2018 Poster for a Second Time) May 2018 Authors: Amy Crawford and Alicia Carriquiry CSAFE Annual All-Hands Meeting in Ames, IA 5.5 People involved 5.5.1 Faculty Alicia Carriquiry Hal Stern (UCI, Project G PI) Danica Ommen 5.5.2 Graduate Students Amy Crawford 5.5.3 Undergraduates Anyesha Ray (data collection) James Taylor (feature extraction) Forgy, E. (1965). Cluster Analysis of Multivariate Data: Efficiency vs. Interpretability of Classifications. Biometrics, 21:768–780.↩︎ Lloyd, S. (1982). Least Squares Quantization in PCM. IEEE Trans. on Information Theory, 28(2):129–137.↩︎ Gan, G. and Ng, M. K.-P. (2017). K-means clustering with outlier removal. Pattern Recog. Letters, 90:8–14.↩︎ Kokonendji, C. C. and Puig, P. (2018). Fisher Dispersion Index for Multivariate Count Distributions. Journal of Multivariate Analysis, v165 p180-193.↩︎ "],
["glass.html", "Chapter 6 Glass 6.1 Data collection 6.2 Data collection 2.0 6.3 The ASTM standard method 6.4 Learning algorithms to evaluate forensic glass evidence 6.5 Evaluation and comparison of methods for forensic glass source conclusions", " Chapter 6 Glass 6.1 Data collection Since 2016, it has started to collect the float glass panes from two glass industries in the U.S. Glass panes were consecutively manufactured. We could construct the challenging comparisons. The elemental chemical concentrations were measured by LA-ICP-MS by the help from Univ. of Iowa. Each pane was broken to several fragments. Then we pick 24 fragments randomly. Each fragment was measured 5 times for 21 fragments and measured 20 times for 3 fragments. Using the data, we construct mates and non-mates. 6.2 Data collection 2.0 We are starting a second set of data collection of the float glass samples. Float glass samples were donated from two glass industries which are the same companies that we had for the first collection. Company A and B sent samples that were manufactured from July to September in 2019. We continue to collaborate with Chemistry lab in the University of Iowa. We sent the first 30 samples and the second 30 samples. The measurement process is ongoing by LA-ICP-MS. They sent us the first six sample measurements from company A. Company A 0800 R 072219 - A1 Company A 0800 R 072619 - A2 Company A 0800 L 072919 - A3 Company A 0800 L 073019 - A4 Company A 0800 R 080919 - A5 Company A 0800 L 081819 - A6 We keep the same sampling procedure to sample 24 fragments from each pane: 3 fragments measured 20 times and rest of them measured 5 times. Each time of measurement, we will have 18 chemical elements observations. Box plots of means from the fragments within the pane: red colors are the first data collection and blues are new samples. RSD (Relative Standard Deviation, 100 \\(\\times\\) SD / abs(Mean)) plots. Black lines are the interval of 25% and 75% quantile of within fragment RSD. Red dot is the pane RSD, from the means of fragments within the pane. Blue line is the manufacturer RSD, using the means of panes within the manufacturer. RSDs in the pink background are from the new samples. 6.3 The ASTM standard method ASTM-E2330-12 (2012) and ASTM-E2927-16 (2016) detail the steps recommended to carry out forensic glass comparisons. At a minimum, the standard approach calls for three fragments from the source at the crime scene (\\(n_{K} \\geq 3\\)), each measured at least three times (\\(M_K \\geq 3\\)), and one fragment, also measured three times, recovered from the suspect (\\(n_Q = 1, M_Q \\geq 3\\)). Using those measurements: Calculate the mean concentration for each element \\(i = 1, \\dots, p\\) from the known fragments, denoted \\(\\bar{y}_{Ki\\cdot\\cdot}\\). Also record 3% of the mean, \\(0.03\\cdot \\bar{y}_{Ki\\cdot\\cdot}\\). Calculate the standard deviation of each elemental concentration from the known fragments. Denote this value by \\(\\sigma_{Ki}\\). Compute an interval for each \\(i\\): \\(\\bar{y}_{Ki\\cdot\\cdot}\\) \\(\\pm\\) 4 \\(\\times\\) \\(\\max(\\sigma_{Ki}, 0.03\\cdot \\bar{y}_{Ki\\cdot\\cdot})\\). This is also known as the \\(4\\sigma\\) interval in the literature (cf.~Weis et al. (2011)). Calculate the mean concentration of each element \\(i = 1, \\dots, p\\) in the questioned fragment(s), denoted \\(\\bar{y}_{Qi\\cdot\\cdot}\\). Compare the value \\(\\bar{y}_{Qi\\cdot\\cdot}\\) to the \\(4\\sigma\\) interval from Step 3: if \\(\\bar{y}_{Qi\\cdot\\cdot} \\notin (\\bar{y}_{Ki\\cdot\\cdot} \\pm 4 \\times \\max(\\sigma_{Ki}, 0.03\\cdot \\bar{y}_{Ki\\cdot\\cdot}))\\) for one or more values of \\(i\\), the glass sources for two groups \\(G_K\\) and \\(G_Q\\) are distinguishable. Otherwise, they are indistinguishable. This means that if any single element mean value from the questioned source does not fall in the \\(4\\sigma\\) interval, the glass sources are declared distinguishable, otherwise they are declared indistinguishable. 6.4 Learning algorithms to evaluate forensic glass evidence by Soyoung Park &amp; Alicia Carriquiry, published in The Annals of Applied Statistics in 2019.06. Take the pariwise differences of 18 element values between two fragments which assigned to classes of mates and non-mates. Developed the scoring metric using several machine learning algorithm and the existing methods, including the ASTM standard. We proposed expressing the criterion suggested by the ASTM standard as a , representing the degree of similarity between \\(G_K\\) and \\(G_Q\\). Mathematically, this score is computed as: \\[\\begin{eqnarray} S_{ASTM,i}&amp;=&amp;\\left|\\frac{\\bar{y}_{Ki\\cdot\\cdot}-\\bar{y}_{Qi\\cdot\\cdot}}{\\max(\\sigma_{Ki}, 0.03\\cdot \\bar{y}_{Ki\\cdot\\cdot} )}\\right| \\label{eq:scorestad1} \\\\ S_{ASTM}&amp;=&amp;\\max(S_{ASTM,i}) \\label{eq:scorestad2} \\end{eqnarray}\\] where \\(i \\in \\{1, 2, \\dots 18\\}\\) indexes element. The maximum (\\(S_{ASTM}\\)) across all element-wise scores (\\(S_{ASTM,i}\\)) becomes the final score to determine if fragments from the pane \\(K\\) and the pane \\(Q\\) have a common source. If \\(S_{ASTM} &gt; 4\\), then the two groups of fragments, \\(G_K\\) and \\(G_Q\\) from panes \\(K\\) and \\(Q\\) are declared to be distinguishable. Test the source prediction performance among the RF, BART, ASTM, hotelling T^2 shrinkage and more.. Density plots by the RF and the ASTM 6.5 Evaluation and comparison of methods for forensic glass source conclusions by Soyoung Park &amp; Sam Tyner, accepted for publication in Forensic Science International in 2019. Understanding the ASTM method Comparison of the RF method and the ASTM method on source conclusion. Depart the ASTM criteria into five pieces: \\[\\begin{eqnarray} S_{ASTM,i}&amp;=&amp;\\left|\\frac{\\bar{y}_{Ki\\cdot\\cdot}-\\bar{y}_{Qi\\cdot\\cdot}}{\\max(\\sigma_{Ki}, 0.03\\cdot \\bar{y}_{Ki\\cdot\\cdot} )}\\right| \\\\ S_{ASTM}&amp;=&amp;\\max(S_{ASTM,i}) \\end{eqnarray}\\] The absolute difference in mean of two fragments Standard Deviation or 3% Known Mean? Deciding Element Decision Threshold Method AUC-ROC EER OT FPR(OT) FNR(OT) RF 0.965 0.098 0.522 0.142 0.045 ASTM 0.936 0.137 2.870 0.140 0.133 Number of Known Fragments Stressing test "],
["shoes.html", "Chapter 7 Shoes 7.1 Longitudinal Shoe Study 7.2 Shoe Scanner - Passive Shoe Recognition 7.3 Maximum Clique Matching 7.4 Project Tread (formerly Cocoa Powder Citizen Science) 7.5 3d Shoe Recognition 7.6 Shoe outsole matching using image descriptors", " Chapter 7 Shoes 7.1 Longitudinal Shoe Study Github repository 7.1.1 Paper describing the database Paper subdirectory of Github repository Goal: Describe experiment Describe database function Publicize data for analysis by others in the community Accompanied by Figshare upload of static data sets (grouped by type). Data Analysis Tools Working with the EBImage package - very fast processing of images ShoeScrubR package Cleaning methods for the Longitudinal data are contained in the ShoeScrubR package. The package includes (as of 2019-10-17) a complete set of tests to guard against regressions (100% test coverage!) and uses continuous integration to ensure that the installation process is also monitored. The ShoeScrubR package also includes logging of all image-in image-out operations using attributes - each time a transformation is performed, the transformation and the parameters are appended to the running operations log. This should make it possible to track the provenance of an object through the set of transformations (and potentially un-do them in some cases). Film and Powder Images The images are challenging to do basic statistical analysis on because the shoe print is made up of tiny particles (e.g. it is not a solid object), and there are areas of smudged particles outside the image (fingerprints, etc.) that can be hard to automatically remove. In addition, the film backing has subtle variations in color. The prints are not rotationally aligned, that is, they are taken at a variety of angles (usually \\(\\pm 15^\\circ\\) from vertical) which vary due to individual differences in walking style, the orientation of the film, and changes in experimental protocol. Experimental protocol changes included resolution changes for the scans over time: the resolution of the last set of images is about 2 times higher than the resolution of the first 3 check-ins worth of images. Thus, parameters need to be automatically selected based on the resolution of the image. The initial use of templates to clean up the image requires addressing the alignment of the print and the template. As this experiment contains 8 total shoe model and size combinations, it is possible to create a template for each shoe model and use that template to isolate the region of the image which contains a set of features most likely to be a shoe (rather than random noise). Solving the template problem by aligning the mask and template yields an additional benefit: the resulting cleaned up image is roughly aligned relative to the template and, presumably, to other images. Rough Alignment of Template Mask and Image: Clean images, do a rough alignment between the image and the corresponding shoe mask (per model and size). Rotationally align image and mask using principal components on the non-background pixels in the image Gross align the non-background pixels in the image and mask Use a “mask-ified” version of the shoe print that encloses most of the shoe region in a single region (see below for explanation) Default to trimming the actual print by 5% on each dimension to minimize the effect of page borders and creases Alignment method options: center of mass - this fails if a print does not account for the whole shoe center of narrowest width - ideally, this is approximately the arch of the shoe. This works better than center of mass in most cases, but occasionally computation fails because it relies on finding a local minimum near the center of the image; if the derivative calculation fails, the algorithm falls back to the center of narrowest width. Pad the image and mask so that the centers are aligned and the image and mask are the same size Set any pixels outside the mask to background Before: After: These steps are wrapped into the rough_align function in the ShoeScrubR package. Cleaning the image and exaggerating into a mask In several of the methods applied to this data, it has been useful to have an exaggerated version of the image to function as a mask - this exaggerated image has a center of mass similar to the actual mask, for instance. Creating this mask requires parameter tuning (done the old way); a new method was developed in order to reduce the number of parameters which depended on image resolution and other similar items. In the old method, there were several parameters necessary - gaussian blur diameter for image cleaning, threshold for binarization, gaussian blur diameter for mask creation, threshold for image cleaning, diameter for opening the mask, diameter for closing the mask. The new steps are as follows: Use the EM algorithm to cluster the intensities of the points into three normally distributed categories: signal, intermediate, and background. The normality assumption is highly questionable, but the method works pretty well. Use the calculated pdf values for each point to construct a likelihood ratio of P(signal)/P(intermediate + background). Binarize based on the value of this likelihood ratio - if it’s over 10, the pixel is signal. Each shoe is shown in three separate images, corresponding to the pdf value for each point based on the EM algorithm clustering distribution fitted values. White pixels are highly likely to belong to the group in question - signal, intermediate, and noise. Binarized versions of each shoe The binarized image from the EM algorithm is cleaned slightly using parameters that should be robust to different ppi images (diameters &lt; 10 pixels) - this gets rid of speckling induced by the EM segmentation. Initial cleaning - dilation and erosion at very small pixel values Labeling disjoint regions (different colors indicate different regions) Removing any blobs which are in the 50px square corner region and which do not involve more than 10% of the image. At this point we can use the mask to clean the image: The binarized image is exaggerated using parameters which depend only on the size of the image. In general, size \\(s\\) here is the square root of the number of pixels in the image, that is, the side length of the image if it were square. A gaussian blur is applied to the image (diameter \\(s\\)/50) Any pixel intensity less than the median is determined to be shoe All disjoint regions are labeled The largest region is selected as the best mask for the shoe Some cleaning is done to this mask - any holes are filled in and then it is opened by a diameter of approximately \\(s\\)/5 (must be odd) At this point we can use the mask to get a “clean” image: And proceed with the alignment as normal: Aligning each image to the template corresponding to the shoe model and size provides a good first step towards aligning the images to one another, but additional work is necessary in order to ensure that we can compare two images from the same shoe (or two images from different shoes). Fine Alignment of two images using RNiftyReg The RNiftyReg package is intended for alignment of brain scans and other MRI data and uses a symmetric block-matching approach(Modat et al. 2014). It only allows for registration of images up to 2048x2048, though, which means we will have to align images at a lower resolution and then modify the transformation matrix accordingly. One definite positive feature is that it allows for provision of a mask for both the source and target images so that only pixels within the mask are used for alignment. Affine transformations are a type of image transformation that encompasses translation, resizing, rotation, and skew operations. Affine transformations preserve collinearity and ratios of distances: parallel lines remain parallel after the transformation. A subset of affine transformations are so-called “rigid body” transformations, which only allow translation and rotation. RNiftyReg allows for linear (rigid-body, affine), and nonlinear transformations, but for the moment, we are only interested in rigid-body transformations - while the shoes may have some slight distortion due to the wearer and kinematics of walking, this is minor and should not interfere too greatly with a gross alignment. RNiftyReg’s niftyreg.linear function returns a 4x4 transformation matrix describing the composition of multiple image transformation operations. A 3x3 matrix is necessary for 3-dimensional image rotation, resizing, and skew operations; it is then augmented by a row of zeros on the bottom and a column ending with a 1 that describes the x, y, and z translation coordinates. In this way, a 4x4 matrix can represent the composition of all relevant image transformation operations in 3 dimensions. In two dimensions, many of the cells in this matrix are 0. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Cells 1, 2, 5, 6 describe the rotation operation; the angle of rotation can be recovered using trigonometry. Cells 4, 8 describe the translation operation, e.g. the row (4) and column(8) offset from one matrix to another. For 2D rigid transformations, these are the only cells which matter. Cells 11 and 16 are 1, and 3, 7, 9, 10, 12, 13, 14, 15 are all 0 under these constraints. In order to use RNiftyReg for registration of the original-size images, we have to scale cells 4 and 8, but the rest of the cells remain unscaled. Note that EBImage uses what is essentially the transposed version of the matrix used by RNiftyReg, reduced to elements 1, 2, 5, 6, 4, 8 in a 3 row by 2 column matrix. 1 5 2 6 4 8 Starting with a set of 6 shoes observed at 3 timepoints each, with 2 different “modes” of print capture, we can align both prints taken on the same date for each shoe. The original images: (1 and 2 are taken at the same time point, 3 and 4, etc. 1-6 are the same physical shoe over time and so on) The images are first aligned to masks, which are used to clean the images to reduce noise. Then reduced-size versions of each image pair are aligned, producing a transformation matrix: 0.9999730 -0.0073485 0 30.48815 0.0073485 0.9999730 0 -10.16105 0.0000000 0.0000000 1 0.00000 0.0000000 0.0000000 0 1.00000 (this is one example) The coordinates in bold have been scaled according to the size reduction, so that they can be applied to the full-size images. The resulting aligned images are shown above; black pixels are areas where both images agree; pink and blue pixels represent areas found in one image but not the other. Some images which have many bubbles (air between the film and backing) or which contain blurred or double prints do not align accurately; this is really not avoidable with automatic alignment solutions. The alignment process itself (without any image preprocessing) is extremely fast, about 1s per image pair, and seems to be similar even when image size is increased by a factor of 16. Due to the scaling process and necessary modification of the transformation matrix, the alignments may be off by 1-5 pixels in some cases. This might be handled by aligning a smaller subset of the image at full resolution. Efforts to align sub-images produced less accurate alignments, likely due to the kinematic distortion of the sole during the walking process, resulting in situations where the local alignment estimate is not a good estimate of the global alignment. New attempts to find consensus alignment might include keypoint-based alignment methods, trying to identify the shoe border precisely to align that, and possibly use of Hough transforms to identify specific reproducible features that might serve as global keypoints. Given how well RNiftyReg works, the next question is whether all of the preprocessing is even necessary. In the image below, the first row is alignment with the full image, the second is alignment with a binarized version of the output image (so masked + thresholded), and the third is automatically cropped around the mask which is aligned to the image using the rough-align process. The third row maintains most of the information and produces better alignments than the other rows; thus, we can conclude that it’s necessary to combine the preprocessing and rough alignment in order to get the best results from RNiftyReg. Fine Alignment of two images using FFT Basic Process: Pad images so they’re the same size (Let’s say that \\(A\\) and \\(B\\) are the two images) Downweight the edges of the images using a Hann function This is necessary because Fourier transforms assume images go on forever, so abrupt changes (e.g. edges) don’t work out that well. Computationally, the image is “wrapped” around itself to be considered “infinite”. The strength of the Hann function can be adjusted by a power argument Use the Fast Fourier Transform on each image. Center the FFT’d images so that low frequencies are in the center (exchange top left quadrant with bottom right, and top right quadrant with bottom left). Call the FFT’d, centered images \\(F(A)\\) and \\(F(B)\\) Recover the angle between the two images: In Fourier space, a rotated image produces the same signal in Fourier space, but rotated. To recover this information, we can use the magnitude of the FFT’d images as a new image. Call these images \\(|F(A)|\\) and \\(|F(B)|\\). Then, Polar transform the image, so that a rotation of the original image is a linear shift of the transformed image (\\(P(|F(A)|)\\) and \\(P(|F(B)|)\\)). Apply Hann functions to the polar-transformed images. b. Use FFT-based alignment (see steps 3, 5) to pick out the $\\theta, \\rho$ which best align the two polar images c. Discard $\\rho$ and keep $\\theta$. Transform the original image (not the FFT&#39;d image) to get $\\tilde{B}_\\theta$ and recompute the FFT on the rotated image ($F(\\tilde{B}_\\theta)$). Recover the shift between the two images: Compute the cross-power spectrum: \\(S = F(A) \\times F(\\tilde{B}_\\theta)*\\), where \\(F(x)*\\) is the complex conjugate of \\(F(x)\\) (e.g. \\(a + bi\\) becomes \\(a - bi\\)) Compute the magnitude of the cross-power spectrum: \\(|S| = \\sqrt{S \\times S*}\\) Use (a) and (b) to get the normalized cross power spectrum: \\(S/|S|\\) Take the inverse FFT of \\(S/|S|\\): \\(A = F^{-1}(S/|S|)\\) Find the maximum of the \\(A\\) matrix. The row and column where the maximum is found are the shift to align the two images: \\((\\Delta x, \\Delta y)\\). Apply the shift to image \\(\\tilde{B}_\\theta\\) to get \\(\\tilde{B}_{\\theta,\\Delta x, \\Delta y}\\). There are some fiddly details (e.g. converting shifts which are most of the image into negative shifts), but FFT-based alignment works pretty well (and pretty quickly) for shoes. Original Images: Angle-aligned Images: FFT-Aligned Result: (still working out the kinks) Combining information from multiple images With aligned images, we can then combine the information in each of the two prints taken at the same timepoint to get more reliable images. Because the methodology for acquiring the prints changed from timepoint to timepoint, the analysis of each type of print doesn’t make sense - it is completely conflated with the observation date. However, the multiple prints aren’t wasted, because while each print may be missing information, the combination of both of the prints (when properly aligned) provides a much more reliable assessment of the shoe (and the wear of that shoe at the proper timepoint). There are at least 3 options to combine these images (note that white = 1, black = 0): Take the pixel-by-pixel minimum: this keeps all the noise from both images (but there’s relatively little noise in the cropped images, so that may not matter too much). If images are misaligned, this results in a “double” print (or smeared print). This method is also sensitive to the lightness of the print - a complete print would still be ignored if the “dark” part of the print was lighter than the “light” part of the second print. Take the pixel-by-pixel maximum: this keeps only points which appear strongly in both images (losing a lot of data but also a lot of noise in the process) Take the pixel-by-pixel mean: This would result in lighter regions where one image has “gaps” but would reduce noise and make it clear where there is less sure information. Of these options, the first seems like the best, but the last provides some idea of whether the alignment was decent to begin with. The alignment method seems to fail more frequently when one of the images is very light. It may be advantageous to invest more effort into adaptive histogram normalization. Alignment Fails: Alignment Success: Wear Characterization Ideas: average intensity of cleaned image length of border/edges detected 7.2 Shoe Scanner - Passive Shoe Recognition 7.2.1 NIJ Grant Grant scope: Build the shoe scanner, develop an automatic recognition algorithm for geometric design elements, test the scanner in locations around Ames. 7.2.1.1 Hardware 2 molds made: Fiber cement held, modified cement is still under testing Fiberglass in the ground being hydro-tested Big mold - modified concrete, comes out of the mold tomorrow morning (2020-06-02) Indoor ramp constructed - can hold 900 lbs safely Electronics - waiting on plexiglass - proximity sensors aren’t working in daylight with reg plexiglass. New plexiglass ordered wednesday to diffuse top-light better 7.2.1.2 Software 7.2.1.2.1 Whole-Shoe Object Detection Project Overview Explore the use of RCNNs for whole-shoe object detection Preliminary results (using ~1000 previously classified images to train and then predict several thousand other images in a semi-supervised approach): 7.2.1.2.2 Restoring Shoe Images Overview Use CNNs to “upsample” manually degraded shoe images (or pairs of ideal and realistic images), restoring as much of the original pattern as possible. Proof of concept (with awful image scaling to 120x120 px… sorry) Low light enhancement with CNNs 7.2.2 CoNNOR: Convolutional Neural Network for Outsole Recognition Project Overview Label images of shoes according to geometric classification scheme Use convolutional base of pretrained CNN VGG16 and train a new classifier on labeled features Eventually, acquire real data passively and use CoNNOR to assess feature similarities and frequencies Link to submitted Creative Component on CoNNOR Github repository for paper submitted to Forensic Science International Exploring new directions: Truncate convolutional base and train random forest on features Could replace fully connected layers of neural net as classifier Importance score can filter/reduce the number of features Training the random forest requires too much memory for Bigfoot (CSAFE server), and still takes over two weeks on the HPC. We’re setting this goal aside for now, but could try subsampling the features for a random forest in the future. Spatial integration Model is currently set up to take in 256x256 pixels Try taking in full shoe using a sliding window of size 256x256 View class predictions spatially Fully convolutional networks (FCNs) Unsupervised segmentation to assess current classification scheme Handle whole shoe image of any size (instead of only 256x256 pixel images) References for CNNs and FCNs Stack Exchange post explaining patchwise training “Learning Hierarchical Features for Scene Labeling”: describes an application of multi-scale CNNs and image pyramids “Pyramid methods in image processing”: classic paper from 1984 explaining pyramid methods “Fully Convolutional Networks for Semantic Segmentation” “W-Net: A Deep Model for Fully Unsupervised Image Segmentation” 7.2.2.1 Spatial integration The overhead costs of going fully convolutional are high; CNN papers are opaque, and many supervised techniques require fully labeled data for semantic segmentation (i.e., label every pixel). Moreover, complex models (for both supervised and unsupervised tequniques) are often only available in Python, and there are a large number of GitHub repositories of mixed quality and reliability. Filtering for quality, understanding code structures, and implementing them on HPC are all enormous tasks on their own. In the meantime, it is much easier (relatively speaking) to use our existing framework of 256x256 square pixel images, for which we have generated thousands of labeled images and have already trained and improved domain-specific models. Currently, I have code working to automatically crop image borders, chop the image into 256x256 pixels (padding the image when appropriate) and equalize the contrast on the individual images. Pad the left and top of the image with a pre-specified offset, then chop the image into 256x256 pixel pieces Equalize the contrast channel for each piece of the image Use the trained model to predict each piece of the image for each class Repeat the above process for different cuts of the original image, and aggregate predictions Updates/In progress: Presented on CoNNOR at ISU’s 3-Minute Thesis competition Preliminaries: 8 heats with 10-12 participants each Runner up to a guy making better yogurt Implementing semantic segmentation model on HPC Learning HPC structure, command line syntax, using Python in practice Documentation assumes high baseline of knowledge :( Improving spatial integration process Remove predictions for blank (and mostly blank) edge images Smooth predictions for overlapping image pieces geom_density2d(), stat_contour(), creating brush overlay with EBImage Shiny app demo 7.3 Maximum Clique Matching 7.4 Project Tread (formerly Cocoa Powder Citizen Science) Project Tread, modified from Leverhulme Institute’s Sole Searching, is a developing CSAFE project with the goals of engaging community participation in forensic research and acquiring shoe print data that may be useful in future analyses. In progress: Review procedures and IRB documents written by James Volunteers are testing modified procedure (below) Be involved in set up of data collection site (through CSSM) Develop plan for data analysis and use (to inform procedure modifications) 7.4.0.1 Comparing the procedures Procedure Leverhulme CSAFE (initial) CSAFE (proposed) ‘Before’ Views 4 per shoe 5 per shoe 2 per shoe ‘Before’ Pictures 1 per view 3 per view 1 per view Total ‘before’ pics 4 per shoe 15 per shoe 2 per shoe Paper Letter (larger) 8.5x11 or 8.5x14 8.5x11 or 8.5x14 Actions Run, jump, walk Step, hop Step, hop Replicates 6 per action 9 per action 3 per action ‘After’ Pictures 1 pic per rep. 3 pics per rep. 1 pic per rep. Total # prints 18 per shoe 18 per shoe 6 per shoe Total # print images 18 per shoe 54 per shoe 6 per shoe Requested # shoes Not specified Both shoes One, 2nd optional Total # images taken 22 or 44 138 8 or 16 Recommended procedure modifications: Ask for one high-quality image of each shoe angle (‘before’) and print (‘after’), instead of three replicate images of each If Project Tread submission website will be set up like current CSSM system (which takes community submissions for environmental images), intermediate quality control can help filter out bad images Only ask for one shoe, and make second shoe optional E.g., always ask for left shoe, and make right shoe optional Ask for 3 replicates per shoe instead of 6-9 Longitudinal shoe study only uses 2-3 replicates per shoe Make written instructions clearer, close loopholes 7.4.0.1 Pilot procedure testing Feedback from pilot testing: Materials Used 1/4 - 1/2 tsp of cocoa powder, not a “dime-size”\" Test/specify cocoa powder vs hot chocolate powder Procedure Sift cocoa powder onto paper (brushing damages print) More pictures of what things should look like Fewer “before” pictures: bottom and side only Mention won’t see oil if done correctly Don’t stack completed prints until after imaging/scanning Analysis Use printable scale/ruler for later alignment of images QR code (like handwriting study) to identify action/replicate/meta info Collect shoe demographics (brand, size, measurements of length/width) Get “mask” by tracing around shoe A few images from pilot testing 7.5 3d Shoe Recognition Some background info: Shoe impressions are often left at crime scenes. In an ideal investigation, an impression left atthe scene of the crime is matched to the suspect’s shoe, placing them at the scene. However, it is not always that easy. An additional problem when dealing with shoes in the forensic discipline isshoes wear over time. When time elapses between the crime and recovery of the shoe, this processmay be more difficult: identifying marks may have worn away, or new marks may have been acquired due to additional wear. In order to understand the changes to an outsole of a shoe that may occur during the wear process, it is necessary to collect longitudinal data from across the shoe’s lifetime. These studies are needed to establish statistical models for shoe wear patterns and accumulated damage. In order to analyze data collected from these studies, it is imperative to develop a statistical pipeline to align and compare successive 3D scans over time. Manual alignment methods are time consuming, and do not scale effectively to large studies, because it is necessary to align hundreds of scans, observed at multiple time points, before any statistical analysis or modeling can be performed. To practically use longitudinal studies, the best approach is to develop statistical tools, to provide a pipeline in which to align and analyze two shoe scans. The set up: Ititally, the idea of the of the study was to use the data from the longitudial study in order to determine if one could predict wear, given pressure point matts and amount of steps and so on that was all part of the longitutidal data collection done previously. This idea has someone shifted. There is a large difference between the scans of the longitudial study that were conducted by hand scanning and those conducted using the turn table. The hand scan soles of the shoes have a wide variation not only on quality, but also on amount of shoe and direction of the scan. In order to simplify the inital processes and determining a pipeline for alignment, instead of the longitudinal shoe scans, the processes below are using a shoe that is scanned soly using the turn table to have some consistancy in the processing as we start to have a pipe line for at least alignment, then later using the longitudinal study add the extra information into the processing; amount of steps, weight,ect. Steps: The shoe scans start as an stl file, however R does not work with stl files directly so they need to be transformed into a mesh object, in this specific case a triangle mesh object: Mesh Objects: A triangle mesh object is made up of the following parts, sufaces, polygons, faces, edges and vertices. Looking at a mesh object at first: Some background: At first we wanted to try to simplify the mesh object to just the basic shape in order have simplicity when aligning. The idea of allignment is to take the most basic features, allign them, then by adding complexity of the soul, further allign the two different shoes. The idea is that we want to take a lower detail scan of shoe, allign it, then by adding complexity we can further allign the shoes. However the problem was realized that we are using a prediction to allign rather than the shoe, which ran into some quick problems. The next “attempt” at alignment came from a package called morph. In order to use this package, we needed to transform the data from a mesh object into a plane of points. In order to do this, we greated the function shoe_coord. This function takes a triangle mesh object, and looks at the vertices that make up each triangle. With a specific amount of vertices specified using the specification vert, shoe_coord, takes these vertices and maps them onto a x,y,z coordinate plane. Then using this coordinate plane, the allignment process began. When working with coordinate systems, there was still not aligning, as each shoe was set on its own coordinate plane Some attempts that have not really turned out well - Transforming the mesh objects to points aligned by the center of mass to overlay them detecting difference. - Problems - Isnt aligning properly as you can see Principal Component Analysis: The basic idea of principal component analysis is to take a dataset with many variables, and simplify that dataset by turning the original variables into a smaller number of “Principal Components”. These principal components are the underlying structure in the data. Taking that data and giving a vector in directions where there is the most variance. Taking these egienvectors as the data, we can get a “flattened” version of our origianl shoe. This is done using singular decomposition: If the \\(N \\times p\\) matrix X has rank r then it has a so-called singular value decomposition as \\(X_{N \\times p} = U_{N\\times r} D_{r \\times r} V&#39;_{r\\times p}\\) where U has orthonormal columns spanning the column space of X, V has orthonormal columns spanning the colum space of X’ and D is a diaginal matrix of singular values. For ordinary principal components, the columns of V are called the principal component directions in \\(\\Re^3\\) . Once a shoe has been turned into coordinates from the verticies of a mesh object, we can use the function prcomp to get a transformation matrix. Some notes on this: use vertices of 7 to decrease amount of points to match if you have too few points also doesnt work turn table works the best Sony are the best matches (the shoes I personally have scanned) Point cloud alignment First looking at shoes with all vertices included: Now the same shoe with fewer vertices included in the allignment With these point clous alignments, the next step is to take the rotation matrix given by prcomp and apply it to the original mesh object since rather than comparing points that may or may not exsisit in comparison to each other shoes, as the vertices are not exactly the same on a mesh object from scan to scan of even the same shoe. prcomp gives a matrix that can be applied to a mesh object using a function called transform3d Once this was tried, it seems that the function prcomp isnt actually centering the data, although according to the code it should. So in order to overcome this issue when initially turning the stl file into a mesh object, a centering is performed using the barycenter of the shoe scan. In astronomy, the barycenter is the center of mass of two or more bodies that orbit one another and is the point about which the bodies orbit, since we are obviously only dealing with one object, the barycenter can be thought of as the center of mass for the shoe scan (the center of mass of a distribution of mass in space is the unique point where the weighted relative position of the distributed mass sums to zero). This puts the point (0,0,0) in the center of the shoe rather than the center being close to the 300s as before Centering The blue shoe is the original shoe grab, where the green shoe is now the shoe that has been centered on the center of mass. Now we can start with two shoes that at least have the same “center” point before principal component analysis is completed: Once the shoes themselves have been centered, then PCA can be compleated on the shoes themselves: PCA after centering The Green shoe is the original shoe grab centered, where the red shoe is now after PCA has been applied. Then using the PCA for both of the two scans we can get the following map of the two shoes with their given principal component rotation matrix applied: As you can see although they are on the same plane, they are not exactly allign. In order to get them aligned a matrix of 1s and -1 needs to be applied in some order, minimizing the RMSE Once the PCA has done the initial alignment, then a secondary alignment needs to be done using Itterative Closest Point, to do this a landmark must be established. One cant just use the PCA, the way that the initial scans are only of the soles, which change as they wear, so we are trying to allign something that is not there anymore. The idea landmark will give a spot in the shoe that will not change, the arch or a large indent, in order to find a proposed landmark, we are attempting to use the maximum distance in z. Automated Alignment: In order to align the shoes through an automatized process, we need to use the following steps. We begin with the two mesh objects of the same shoe at different time points, say time i and time j.Then using the barycenter, or center of mass, we center the shoes on the same x,y,z, coordinate system. After the shoes are then centered, they are transformed into a point cloud made up of the verities of the triangle mesh object. Using these two coordinate systems. Principal component analysis is performed. This gives us a 3X3 rotation matrix in which to transform the original mesh object by the PCA rotation matrix. PC1 PC2 PC3 x 0.5603136 -0.2618793 0.78579127 y 0.3038740 0.9475434 0.09910676 z 0.7705253 -0.1832507 -0.61049973 Due to the nature of PCA, the rotation matrix that we are using is not unique. Further more it often times has a negative determinate. If this happens, we need to rotate the mesh shoe object by a matrix whose diaginal is (1,1,-1). Leaving us with a positive determinate. After this is done, there is no guarantee that the two shoes although sharing the same space in x,y,z where x is the length of the shoe, y the width and z the depth, will be in the same direction. In order to correct for this we need to rotate the shoes by 180 degrees in x,y,z or a combination of those directions. There are 8 possibilities of those rotations: rotation around the x-axis 180 degrees rotation around the y-axis 180 degrees rotation around the z-axis 180 degrees rotation around the x-axis 180 degrees and the y-axis 180 degrees rotation around the x-axis 180 degrees and the z-axis 180 degrees rotation around the y-axis 180 degrees and the z-axis 180 degrees rotation around the x-axis 180 degrees and the y-axis 180 degrees and the z-axis 180 degrees No rotation is needed there are only 4 unique matrices to deal with Matrix Rotations In order to choose which one of these matrices, the first shoe scan in time is set as the base scan. Then each of the four matrices described above is applied to second scan. After each rotation, the distance between the first and second scan is measured. The matrix with the smallest distance is used as the rotation matrix, and is applied to the second shoe. For example, with scan 1 and scan 2, the four matrices lead to the following 4 distances: 2.153458, 9.124214, 9.117095, and 6.234845. Thus scan 2 is rotated by the matrix of the diag(1,1,1). Once the shoes are initially alligned, ICP is applied with an iteration of the aligners choosing. Once ICP has been performed, the distance is again measured and recorded. Here the final distance is 0.9791678 cm How distance is being measured: In this process, we set the first scan as a base scan (here scan 1). This scan does not move through the process. Once this scan is set, using a K-D tree method the shoe is divided up into sections until there is only one vertex in each section. This process is done to the second shoe as well. K-D construction is as follows divide the points in half by a line perpendicular to one of the axes, in each tree. Then recursively construct k-d trees for the two sets of points, the two points corresponding to the verities of the triangles. Think of it as this, we first choose a shoe to be set. In this case shoe scan 1. Then for each one of the verticies of a triangle we draw a ball with radius then we start epsilon to be 0 and expand it till you hit the closest section with a specified point. Each point is then pair with the closest euclidean distance corresponding point.For two vectors or two vectors (x_1,y_1,z_1) and (x_2,y_2,z_2), the euclidean distance is found by: \\[ \\sqrt(\\sum^n_{i=1}(q_i-p_i)^2) \\] This is repeated for the set iterations, or until epsilon is deemed small enough. Each time the distance is measured, but the K-D tree sectioning remains the same through the process. This gives us N distances for the number of iterations. We return the final N iteration’s distances from each set of verities. The mean of these distances is then computed, giving us a distance metric. This is used to find the magnitude of the distance between the two shoes. This distance is consistent for each processing of two shoes (i.e. if you run scan 1 and scan 2 through this process, you will still get the same distance of 0.98. However, if you switch the positions of the shoe, making scan 2 the base you will get a different result, 2.243478) Understanding the distance: Once we have the distance, we want to understand where the wear is occurring, plotting the distance on the base scan of scan 1, we see the distance to scan 2 looks like the following: Some Issues through the process: Inconsistency of scans: As we have seen the cropping of the scan is inconsistent, i.e. where the scans were cut How ICP is being measured: It is unclear how much ICP is changing after each iteration. For example, in our scan1 vs scan 2 shoes if we take the mean distance after 1 iteration, then 2, up to 20 iterations we get the same mean. (next step is to go further out to see if it changes, or look at the distribution of distances after each iteration and not just the mean, to see if the tails are changing.) 7.6 Shoe outsole matching using image descriptors 7.6.1 Method 1: Edge detection and MC (MC-COMP) Under the revision by Journal of Applied Statistics, Comments by reviewers Compare your predictive accuracies on the number of circles (more or less than 3). In the previous matching, we found three circular regions in red. As reviewer requests, we selected the additional three more circles in blue. We tried the same set of mated and non-mated matching in training and testing set. We get the average of the similarity values by the number of circles. The random forest is trained on the set of results summarised by the number of circles. When there are five and six circles found in the questioned shoe, then the AUC is the largest as 0.96 in the test set. 7.6.2 Method 2: SURF detection and MC on degraded impressions Accepted by Statistical Analysis and Data Mining in Feb. 2020. Comments by reviewers Generalization (Fingerprints, Photos of faces, … ) Other classifiers, besides the random forest. Variable importance 7.6.3 Research 1: Features Previously, features such as edge, corner, SURF were extracted to match shoeprints. The goal of this project is to find other image descriptors as image features for shoe print matching. Image descriptors SURF(Speeded Up Robust Features)- blobs KAZE - blobs ORB(Oriented FAST and Rotated BRIEF)- corners Image matching CSAFE data - Nike size of 10.5 and Adidas size of 10 will be used to construct mated and non-mated matching Features will be combination of strong 100 points of KAZE, ORB, SURF. 7.6.4 Matching on clean and full images with several features Mates : Image (1) and (2) Non-mates : Image (1) and (3), Image (2) and (3) Performance evaluation using SURF 500 KAZE 500 ORB 500 SURF 100 + KAZE 100 + ORB 100 SURF 200 + KAZE 200 + ORB 200 POC (Phase-only correlation) FMTC (Fourier Mellon transformation correlation) Example graphs of mates: Example graphs of non-mates: Example matching table using SURF 500: Class Clique size Rotation angle % Overlap Median distance of OP Mates 18 2.11 0.5646 0.78 Non-mates 9 6.43 0.1208 1.39 Density plots: ROC curves on test comparisons: 7.6.5 Matching on degraded and partial images Example of degraded images: Example crime-scene like images on dust from the data by Speir’s group: Density of mates and non-mates by features: ROC curves by features: AUC values by features: Relationship among similarities (% Overlap) by feature-types Overlap feature (% overlap after the alignment) in the degradation level, 0 Overlap feature (% overlap after the alignment) in the degradation level, 10 Random forest training for the variable importance Trained the RF, in the degradatino level of 0 and 10, on the class with the variables of all similarities by each feature type. Degradation level 0 Degradation level 10 7.6.6 Research 2: Impact of weight to outsole scans from EverOS 2D scanner Analysis setup: Shoes: 5 pairs of Nike Winflow 4, size 10.5 (Brand-new shoes) Participants: Person 1 (weight aa lb)and person 2 (bb lb) Weights: 2 weight vests (20 lbs and 12 lbs) Weight variations: W1(P1), W2(P1 with one vest), W3(P1 with two vests), W4(P2), W5(P2 with one vest), W6(P2 with two vests) W1 \\(=\\) 155.2 lb, W2 \\(=\\) 173.8 lb, W3 \\(=\\) 187.6 lb, W4 \\(=\\) 178 lb, W5 \\(=\\) 197.4 lb, W6 \\(=\\) 211 lb Result1: Shoe is fixed to left side of shoe1. Result2: Purple, green: comparison between repeated replicates from the same shoe, W2-W2, W4-W4. Comparison between weights W2 and W4: weights are almost the same. Yellow: comparison when shoe is fixed to shoe1-L. Red: comparison between shoe1 – shoe2-5. Shoes are never used. Discrimination among groups Let’s simplify the question; Is the person effect significant? Initial analysis Cluster analysis; hcluter Principal component analysis (PCA) Are groups significantly different? Multivariate Analysis of Variance (MANOVA) Multi-Response Permutation Procedurs(MRPP) Analysis of Group Similarities (ANOSIM) How do groups differ? Discriminant Analysis (DA) Classification and Regression Trees (CART) Logistic Regression (LR) 7.6.7 Research 3: MC + CNN Idea: Align two images using MC. Calculate similarity features. Ask CNN to learn similarity features. Combine 3 and 4. Use ResNet-50 to get a final similarity value. ### Additional features from descriptors Much of the outsole matching already done at CSAFE is based primarily on the geometric locations of SURF features. While this is apparently useful, the utility of SURF and other computer vision features (such as BRISK, ORB, and KAZE) can go far beyond location. Most of the work done has also been on images produced using the EverOS 2D scanner. Location features from these images are relatively easy to align, but with different image types -such as those made using fingerprint powder- alignment becomes much more difficult. When a SURF (or other feature type) is detected in an image, several features are extracted. Each SURF is a detected “blob” from an image. Features from a SURF include Sign of Laplacian, orientation, location, metric, and a 1x64 vector of descriptors. Essentially these several features describe characteristics of the blob like color intensity, change in intensity, change in pattern, blob direction, etc. Between two different images if two SURFs have a matching sign of Laplacian, the distance between the two 1x64 descriptor vectors is measured. If the distance is below a specified threshold the two SURFs are considered matching. Sign of Laplacian 64 descriptors When SURFs are matched, a matrix is returned with indices of matching SURFs and the measured distance. Matched features are illustrated here. Clearly many of these “matched” SURFs -based on metric- are not actually corresponding parts between images. To make more precise which SURFs match, I use the maximum clique from the shoeprintr packages to align the SURF points based on location. Once aligned, Euclidean distance is used to find the closest point(s) in image 2 to each point in image 1. Now there are two sets of matching indices. The index pairs which overlap between the two sets are considered certain matches. The number of “certain” matching SURFs and the metric between SURF descriptors can be used as two additional features for image pair comparison. Useable features Clique Size Rotation Angle % Overlap Median distance of OP Certain SURF matches Max SURF match corr We’ve trained 2 different random forests using these features from image pairs. The first random forest uses 115 pairs of images using matching soles with the same stepping style and 116 pair of images of the same shoes with different stepping style. The random forest classification accuracy using leave one out cross validation is 92% Different step type average variable values Class Clique size Rotation angle % Overlap Median distance of OP Certain SURF matches Max SURF match corr Same steps 8 3.97 0.1992 1.21 13 0.9523 Dif steps 6 14.04 0.0169 0.99 0.487 0.2236 RF Variable importance Variable Importance Score Certain SURF matches 100.000 % Overlap 14.951 Max SURF match corr 14.059 Rotation angle 4.484 Median distance of OP 2.359 Clique size 0.000 The second rf is trained from 115 images of matching shoes with the same stepping style and 116 images of non-matching shoes with the same stepping style. The random forest classification accuracy was also 92% Match v. non-match average variable values Class Clique size Rotation angle % Overlap Median distance of OP Certain SURF matches Max SURF match corr Mates 8 3.97 0.1992 1.21 13 0.9523 Non-mates 6 9.26 0.0164 1.24 0.836 0.2821 RF Variable importance Variable Importance Score % Overlap 100.000 Certain SURF matches 34.044 Max SURF match corr 26.937 Rotation angle 2.221 Median distance of OP 2.213 Clique size 0.000 Improvements to be made When two images using fingerprint powder are are from the same shoe and with the same stepping style, the alignment using maximum clique is generally good. However, when the sole images are not mates the alignment is occasionally not good. The alignment is especially poor when the step style changes. So far, other existing methods haven’t shown any more effectiveness in aligning shoes than the maximum clique. It would be useful if features could be made from the SURF descriptors which could be used to compare shoes without needing to align the images. References "],
["theoretical-foundations.html", "Chapter 8 Theoretical foundations 8.1 Explaining SLR behavior 8.2 Common Source vs Specific Source Comparison via Information Theory 8.3 Score-based Likelihood Ratios are not Fundamentally “Incoherent” 8.4 Copper Wire Synthetic Data 8.5 Optimal matching problem 8.6 SRL behaviour and dependence", " Chapter 8 Theoretical foundations 8.1 Explaining SLR behavior This project involves explaining the behavior of an SLR w.r.t. the true LR. Here is a summary of the main results: \\(|log(LR) - log(SLR)|\\) is likely to be unbounded and has to do with the fact that the LR and the SLR typically do not share the same invariances (the contour lines of the LR and SLR differ). The example in the paper shows how this works for univariate Gaussian data and squared Euclidean distance score function. Large discrepancies between the LR and the SLR are probably even with univariate data The most likely and largest discrepancies between the SLR and LR tend to be when both the LR and SLR are very large or very small. Bounds on the tail probabilities of the LR given a score Bounded LR implies bounded SLR LRs are always larger than the SLR in expectation under the prosecution hypothesis and smaller in expectation under the defense hypothesis (technically the latter statement should be that the inverse of the LR is larger in expectation than the inverse of the SLR) We submitted this to JRSS Series A, but it was returned to us with the option to resubmit. The biggest issue seemed to be a misunderstanding about whether independence under the prosecution hypothesis is reasonable. Otherwise, the first reviewer seemed to focus heavily on the LR paradigm generally, and they also seemed to misunderstand that our definition of “LR” was of the true distributions from which data are sampled under both hypotheses. To address these misunderstandings, we are completely rewriting the intro to move the focus from LRs to SLRs and correcting some specific lines that may have helped lead to some confusion. We are also taking more time in the paper to discuss that, the specific source problem conditions on the source of the evidence under the prosecution hypothesis, and thus the “dependence” that we think the reviewers are thinking should exist is lost. That is, the fact that the unknown source and known source data should be more similar under \\(H_p\\) than any random two pieces of evidence is already conditioned upon. See here for the current draft of the paper from this work. 8.2 Common Source vs Specific Source Comparison via Information Theory Please note that this project has changed somewhat significantly, but this information might be relevant for others in the future. See here for the current draft of the paper from this work. 8.2.1 Introduction Central Goals continue work started by Danica and Peter Vergeer on the analysis of likelihood ratios study the differences between specific source (SS) and common source (CS) likelihood ratios (LRs) in an information theoretic way does the CS or SS LR have more “information”? does the data (or the score) have more “information” about the SS or the CS hypothesis? can be the CS or SS hypotheses (prosecution or defense) be formally compared in terms of being easier to “prove” or “disprove”? General Notation Let \\(X \\in \\mathbb{R}^{q_x}\\) and \\(Y \\in \\mathbb{R}^{q_y}\\) be two random vectors with joint distribution \\(P\\) and corresponding density \\(p\\). : \\(\\mathbb{H}(X) = -\\int{p(x) \\log p(x) dx}\\) : \\(\\mathbb{H}(X|Y) = \\mathbb{E}_{Y}\\left[-\\int{p(x|y) \\log p(x|y) dx}\\right]\\) : \\(\\mathbb{H}_{2}(X|Y) = -\\int{p(x|y) \\log p(x|y) dx}\\) : \\(\\mathbb{I}(X;Y) = \\mathbb{H}(X) - \\mathbb{H}(X|Y)\\) Proof that Mutual Information is always positive: \\[\\begin{align*} \\mathbb{I}(X;Y) &amp;= \\mathbb{H}(X) - \\mathbb{H}(X|Y) \\\\ &amp;= -\\int{p(x) \\log p(x) dx} + \\int{\\int{p(x|y)p(y) \\log p(x|y) dx} dy} \\\\ &amp;= -\\int{\\int{p(x,y) \\log p(x) dx}dy} + \\int{\\int{p(x,y) \\log p(x|y) dx} dy} \\\\ &amp;= -\\int{\\int{p(x,y) \\log p(x) dx}dy} + \\int{\\int{p(x,y) \\log \\frac{p(x,y)}{p(y)} dx} dy} \\\\ &amp;= \\int{\\int{p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} dx}dy} \\\\ &amp;= KL(P||P_{X} \\times P_{Y}) \\\\ &amp;\\geq 0 \\end{align*}\\] 8.2.2 Common Source vs Specific Source LR The “common source” problem is to determine whether two pieces of evidence, both with unknown origin, have the same origin. One might be interested in this problem if two crimes were suspected to be linked, but no suspect has yet been identified. Alternatively, the “specific source” problem is to determine whether a fragment of evidence coming from an unknown source, such as evidence at a crime scene, has the same origin as a fragment of evidence of known origin, such as evidence collected directly from a suspect. Basic Setup \\(H \\in \\{ H_p, H_d \\}\\) as the random variable associated with the CS hypothesis. \\(A\\) and \\(B\\) are discrete r.v.’s representing two “sources” of evidence distributions for \\(A\\) and \\(B\\) defined conditionally based on the hypothesis SS hypothesis is represented by the conditional random variable \\(H|A\\) \\(X\\) is data coming from \\(A\\), \\(Y\\) is data coming from \\(B\\) compare information contained in \\((X,Y)\\) about \\(H\\) and \\(H|A\\) join density can be written as \\(p(X,Y,A,B,H) = p(X,Y|A,B)p(B|A,H)p(A|H)p(H)\\) Is there more information in a CS or SS LR? Let us examine this question in two different ways. Is the posterior entropy (given \\((X,Y)\\)) in the common source hypothesis smaller than that of the specific source hypothesis? In other words, would observing the specific value of \\(A\\) as well as the data make you more certain about \\(H\\) than just observing the data? Is the posterior entropy (given \\((X,Y)\\)) in the common source hypothesis smaller than the average (over possible values for \\((X,Y,A)\\)) posterior entropy of the specific source hypothesis? In other words, do you expect that, on average, observing the value of \\(A\\) as well as the data make you more certain about \\(H\\) than just observing the data? Answering the first question/interpretation, to me, requires proving that \\[ \\mathbb{H}_{2}(H|X,Y) - \\mathbb{H}_{2}(H|X,Y, A) \\geq 0 \\]. Answering the second question requires proving that \\[ \\mathbb{H}(H|X,Y) - \\mathbb{H}(H|X,Y, A) \\geq 0 \\]. Luckily, the second question is true due to the fact that \\[\\begin{align*} \\mathbb{H}(H|X,Y) - \\mathbb{H}(H|X,Y,A) &amp;= \\mathbb{E}_{(X,Y)} \\left[ - \\int{p(h,a|x,y) \\log p(h|x,y) d(h,a)} + \\int{p(h,a|x,y) \\log p(h|x,y,a) d(h,a)} \\right] \\\\ &amp;= - \\int{p(h,a|x,y)p(x,y) \\log \\frac{p(h,a|x,y)}{p(a|x,y)p(h|x,y)} d(h,x,y,a)} \\\\ &amp;= \\mathbb{E}_{(X,Y)} \\left[ KL(P_{(H,A)|(X,Y)}||P_{H|(X,Y)} \\times P_{A|(X,Y)}) \\right] \\geq 0 \\end{align*}\\] Whether or not \\(\\mathbb{H}_{2}(H|X,Y) - \\mathbb{H}_{2}(H|X,Y, A) \\geq 0\\) is not obvious. We have that \\[\\begin{align*} \\mathbb{H}_{2}(H|X,Y) - \\mathbb{H}_{2}(H|X,Y, A) &amp;= \\int{-p(h|x,y)\\log p(h|x,y) dh} - \\int{-p(h|x,y,a) \\log p(h|x,y,a) dh} \\\\ &amp;= \\frac{p(a)}{p(a|x,y)}\\int{-p(h|x,y,a)\\log p(h|x,y) dh} + \\int{p(h|x,y,a) \\log p(h|x,y,a) dh}\\\\ &amp;??? \\end{align*}\\] We can try and understand the value of \\(\\mathbb{H}_{2}(H|X,Y) - \\mathbb{H}_{2}(H|X,Y, A)\\) in terms of \\(\\frac{p(a)}{p(a|x,y)}\\). For example, if \\(\\frac{p(a)}{p(a|x,y)} \\geq 1\\), then \\(\\mathbb{H}_{2}(H|X,Y) - \\mathbb{H}_{2}(H|X,Y, A) \\geq 0\\). If \\(\\frac{p(a)}{p(a|x,y)} \\leq 1\\), then it is hard to say much about the value of \\(\\mathbb{H}_{2}(H|X,Y) - \\mathbb{H}_{2}(H|X,Y, A)\\). Is there more information in the data about the CS or SS hypothesis? Under the second scenario, we can study this question by looking at 8.2.3 Other notions of information Information in \\(Y\\) about \\(X\\): \\(\\int{p(x|y) \\log \\frac{p(x|y)}{p(x)} dx}\\) nonnegative Equal to zero when \\(X \\perp Y\\) needn’t integrate over \\(Y\\) (?) as opposed to entropy, information in a random variable requires another random variable to be “predicted”… this is fine in our situation as we have a natural candidate: \\(H_p\\) or \\(H_d\\) 8.2.4 Information Theoretic Specific Source Score Sufficiency Metric Consider the specific source problem. The following derivations are very similar to those in the “infinite alternative population” situation considered in the paper that Danica, Alicia, Jarad, and I submitted. Assuming \\(X \\perp Y|A,B\\) and both \\(X \\perp B|A\\) and \\(Y \\perp A|B\\), the LR is \\[\\begin{align*} LR &amp;= \\frac{p(x,y|A = a,B = a)}{p(x,y|A = a,B \\neq a)} \\\\ &amp;= \\frac{p(x|A = a)p(y|A = a, B = a)}{p(x|A = a)p(y|A = a, B \\neq a)} \\\\ &amp;= \\frac{p(y|A = a, B = a)}{p(y|A = a, B \\neq a)}. \\end{align*}\\] Thus, the LR depends only on the evidence from the unknown source, \\(Y\\). For a given score, \\(s\\), we can also write the LR in the following way, \\[\\begin{align*} LR = \\frac{p(y|A = a, B = a)}{p(y|A = a, B \\neq a)} &amp;= \\frac{p(s|y, A = a)p(y|A = a, B = a)}{p(s|y, A = a)p(y|A = a, B \\neq a)} \\\\ &amp;= \\frac{p(s|y, A = a, B = a)p(y|A = a, B = a)}{p(s|y, A = a, B \\neq a)p(y|A = a, B \\neq a)} \\\\ &amp;= \\frac{p(s,y|A = a, B = a)}{p(s,y|A = a, B \\neq a)} \\\\ &amp;= \\frac{p(y|s,A = a, B = a)p(s|A = a, B = a)}{p(y|s,A = a, B \\neq a)p(s|A = a, B \\neq a)}. \\end{align*}\\] Because \\(S|Y,A\\) is a function only of the known source evidence, \\(X\\), and because \\(X \\perp B|A\\), we have that \\(S \\perp B | Y, A\\). This means that \\(p(s|y, A = a, B = a) = p(s|y, A = a, B \\neq a)\\). Using these facts, we can then decompose the KL divergence of the data under the specific source prosecution hypothesis in the following way, \\[\\begin{align*} KL(P(X,Y|A &amp;= a, B = a)||P(X,Y|A = a, B \\neq a)) = E_{(X,Y)}\\left[ \\log \\frac{p(x,y|A = a,B = a)}{p(x,y|A = a,B \\neq a)} | A = a, B = a \\right] \\\\ &amp;= E_{Y}\\left[ \\log \\frac{p(y|A = a,B = a)}{p(y|A = a,B \\neq a)} | A = a, B = a \\right] \\\\ &amp;= E_{S}\\left[ E_{Y}\\left[ \\log \\frac{p(y|A = a,B = a)}{p(y|A = a,B \\neq a)} |s, A = a, B = a \\right] \\right] \\\\ &amp;= E_{S}\\left[ E_{Y}\\left[ \\log \\frac{p(y|s,A = a,B = a)}{p(y|s,A = a,B \\neq a)} + \\log \\frac{p(s|A = a, B = a)}{p(s|A = a, B \\neq a)} |s, A = a, B = a \\right] \\right] \\\\ &amp;= E_{S}\\left[ E_{Y}\\left[ \\log \\frac{p(y|s,A = a,B = a)}{p(y|s,A = a,B \\neq a)}|s, A = a, B = a \\right] \\right] + E_{S} \\left[ \\log \\frac{p(s|A = a, B = a)}{p(s|A = a, B \\neq a)} \\right] \\\\ &amp;= E_{S} \\left[ KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \\neq a)) \\right] + KL(P(S|A = a, B = a)||P(S|A = a, B \\neq a)). \\end{align*}\\] This implies that \\(KL(P(X,Y|A = a, B = a)||P(X,Y|A = a, B \\neq a)) \\geq KL(P(S|A = a, B = a)||P(S|A = a, B \\neq a))\\). An additional consequence is that larger values of \\(KL(P(S|A = a, B = a)||P(S|A = a, B \\neq a))\\) imply smaller values of \\(E_{S} \\left[ KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \\neq a)) \\right]\\). Because \\(KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \\neq a))\\) is a nonnegative function in terms of \\(S\\), small values of \\(E_{S} \\left[ KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \\neq a)) \\right]\\) imply small values (in some sense) of \\(KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \\neq a))\\). For example, if the expectation is zero, then the (conditional) KL divergence is zero almost everywhere. Zero KL divergence implies that \\(P(Y|S,A = a,B = b) = P(Y|S,A = a,B \\neq b)\\), i.e. \\(S\\) is sufficient for the specific source hypothesis. All of this means that \\(KL(P(S|A = a, B = a)||P(S|A = a, B \\neq a))\\) and \\(KL(P(S|A = a, B \\neq a)||P(S|A = a, B = a))\\) are measures of the usefulness of the score which have direct ties to sufficiency. Estimates of these are always computable in practice, and they are intuitive targets to maximize. For example, if the score is a predicted class probability for “match”, the more discriminative the classifier, the more sufficient the score. 8.3 Score-based Likelihood Ratios are not Fundamentally “Incoherent” Concern has been raised in the literature on LRs about a desirable property supposedly inherently absent from specific-source SLRs. The property, dubbed “coherence”, intuitively says that given two mutually exhaustive hypotheses, \\(H_A\\) and \\(H_B\\), the likelihood ratio used to compare hypothesis A to hypothesis B should be the reciprocal of that used to compare hypothesis B to hypothesis A. I will argue that the claims about the inherent incoherency of SLRs is a result of thinking about SLRs too narrowly. Specifically, I will show that the arguments as to why SLRs are incoherent arise through the inappropriate comparison of SLRs based on different score functions. When one appropriately considers a single score function, incoherency is impossible. 8.3.1 Coherence Denote by \\(E \\in \\mathbb{R}^{n}\\) the vector of random variables describing all of the observed evidence or data which will be used to evaluate the relative likelihood of the two hypotheses. Define by \\(LR_{i,j} \\equiv \\frac{p(E|H_i)}{p(E|H_j)}\\) the likelihood ratio of hypothesis \\(i\\) to hypothesis \\(j\\). The coherency principal is satisfied if \\[ LR_{i,j} = \\frac{1}{LR_{j,i}} \\]. Likelihood ratios are fundamentally coherent, but what about score-based likelihood ratios? Denote by \\(s: \\mathbb{R}^n \\rightarrow \\mathbb{R}^{q}\\) a score function mapping the original data to Euclidean space of dimension \\(q\\) (typically \\(q = 1\\)). Similar to LRs, denote by \\(SLR_{i,j} \\equiv \\frac{p(s(E)|H_i)}{p(s(E)|H_j)}\\) the score-based likelihood ratio comparing hypothesis \\(i\\) to hypothesis \\(j\\). Clearly, in this general context SLRs are also coherent. 8.3.2 Problems with arguments showing SLRs are incoherent Let us examine the arguments presented in [REFS] to the incoherence of SLRs. These arguments stem from an example where there are two known sources of evidence say, source \\(A\\) and source \\(B\\), each producing data \\(e_A\\) and \\(e_B\\), respectively. Furthermore, assume that we have a third piece of evidence of unknown origin, \\(e_u\\), which must have come from either \\(A\\) or \\(B\\). We then wish to evaluate the support of the data for \\(H_A\\) or \\(H_B\\) defined as follows \\[\\begin{array}{cc} H_A: &amp; e_u \\text{ was generated from source } A \\\\ H_B: &amp; e_u \\text{ was generated from source } B. \\end{array}\\] In this case, we have \\(LR_{A,B} = \\frac{p(e_A, e_B, e_u|H_A)}{p(e_A, e_B, e_u|H_B)}\\). We make use of all available data in the formulation of the numerator and denominator densities. Under the assumptions that each fragment of evidence is independent under both hypothesis \\(A\\) and \\(B\\) as well as that \\(p(e_A,e_B|H_A) = p(e_A,e_B|H_B)\\), the LR reduces to \\(LR_{A,B} = \\frac{p(e_u|H_A)}{p(e_u|H_B)}\\). The second assumption is generally acceptable as the source of \\(e_u\\) ought to have no impact on the distribution of the evidence with known source. [REFS] then consider possible SLRs for this example. However, they make an assumption that the score is explicitly a function only of two fragments of evidence. That is, assuming the dimension of \\(e_i\\), \\(dim(e_i) = k\\), is constant for \\(i = A,B,u\\), their score maps \\(s:\\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}\\). An common example of such a score is Euclidean distance, i.e. \\(s(x,y) = \\left[ \\sum_{i = 1}^{k}(x_i - y_i)^2 \\right]^{1/2}\\). Such a score makes perfect sense in a typical specific-source problem context in which only two fragments of evidence are considered: one from the known source and one from the unknown source. However, when one desires to create an SLR based on this score in this particular example, it is tempting to suggest that the natural SLR is \\(SLR_{A,B} = \\frac{p(s(e_A,e_u)|H_A)}{p(s(e_A,e_u)|H_B)}\\). Yet, the natural SLR if the hypotheses were reversed is \\(SLR_{B,A} = \\frac{p(s(e_B,e_u)|H_B)}{p(s(e_B,e_u)|H_A)}\\). Neither of these SLRs is the reciprocal of the other, and so the specific source SLR appears to be “incoherent”. This approach, however, should raise a red flag immediately. Why, in the full LR case, do we require that (simplifying model assumptions aside) the numerator and denominator densities be functions of all available data, but the score is not? Furthermore, if we consider these SLRs in the more general context of scores depending on all available data, we see that, in fact, what [REFS] define to be \\(SLR_{A,B}\\) and \\(SLR_{B,A}\\) turn out to be two different SLRs depending on two different scores. For clarity, we will use \\(s(\\cdot)\\) to denote scores which are explicitly functions of all observed data, and we will use \\(\\delta (\\cdot)\\) to denote score functions which are only a function of two fragments of evidence/data. Specifically, the score in \\(SLR_{A,B}\\) is \\(s_1(e_u,e_A,e_B) = \\delta(e_u,e_A)\\) and the score in \\(SLR_{B,A}\\) is \\(s_2(e_u,e_A,e_B) = \\delta(e_u,e_B)\\). While the functional form of the score in the two SLRs appears to be the same, clearly \\(s_1(e_u,e_A,e_B) \\neq s_2(e_u,e_A,e_B)\\). Thus, the two SLRs are simply two distinct quantities whose relationship needn’t be expected to be related anymore than if one had decided to use two different function forms of \\(\\delta(\\cdot,\\cdot)\\) in the two separate SLRs. One might ask how to reasonably construct an SLR which utilizes a (univariate) score other than a similarity metric for two fragments of evidence. One such example in this case would be \\(s(e_u, e_A, e_B) = \\frac{\\delta(e_u,e_A)}{\\delta(e_u,e_B)}\\). Intuitively, under \\(H_A\\), the numerator should be larger than the denominator, while under \\(H_B\\), the opposite should be true. 8.3.3 Example of a coherent SLR in the two source problem Suppose that our hypotheses are defined such that \\[ \\begin{array}{cc} H_A: &amp; e_u \\sim N(\\mu_A, \\sigma^2), e_A \\sim N(\\mu_A, \\sigma^2), e_B \\sim N(\\mu_B, \\sigma^2) \\\\ H_B: &amp; e_u \\sim N(\\mu_B, \\sigma^2), e_A \\sim N(\\mu_A, \\sigma^2), e_B \\sim N(\\mu_B, \\sigma^2), \\end{array} \\] where \\(e_u\\), \\(e_A\\), \\(e_B\\) are mutual independent under both \\(H_A\\) and \\(H_B\\). We will examine three different SLRs: \\(SLR^{(A)} \\equiv \\frac{p(s_1(E)|H_A)}{p(s_1(E)|H_B)}\\), \\(SLR^{(B)} \\equiv \\frac{p(s_2(E)|H_A)}{p(s_2(E)|H_B)}\\), and \\(SLR^* \\equiv \\frac{p(s_3(E)|H_A)}{p(s_3(E)|H_B)}\\), where \\[\\begin{align*} E &amp;= (e_u, e_A, e_B)^{\\top} \\\\ s_1(E) &amp;= \\log \\lVert e_u - e_A \\rVert^2 \\\\ s_2(E) &amp;= \\log \\lVert e_u - e_B \\rVert^2 \\\\ s_3(E) &amp;= \\log \\frac{\\lVert e_u - e_A \\rVert^2}{\\lVert e_u - e_B \\rVert^2} \\end{align*}\\] LR versus SLR scatterplots under hypothesis A and B using three types of SLRs: “coherent”, “incoherent” considering hypothesis A first, and “incoherent” considering hypothesis B first. RMSE Exp.Cond.KL score.KL true.KL type hypothesis 3.79 2.71 1.76 4.47 coherent A 3.86 2.77 1.74 4.51 coherent B 4.64 3.37 1.10 4.47 incoherent A A 3.94 3.22 1.29 4.51 incoherent A B 3.93 3.23 1.24 4.47 incoherent B A 4.73 3.44 1.07 4.51 incoherent B B 8.3.4 Possible Generalizations of Coherent SLRs to the Multisource Case It might be nice to, in general, be able to construct a reasonable score given a “similarity” score, \\(\\delta(\\cdot, \\cdot)\\) defined in terms of two pieces of evidence. I’ll propose a couple ways of doing this. First, suppose that instead of two sources, we now have \\(K\\) sources, one of which is the source of the evidence from an unknown source. The task is to compare the hypothesis that the unknown source evidence was generated by a specific source \\(A = a_x \\in \\mathcal{S} \\equiv \\{1, 2, ..., K\\}\\) to the hypothesis that the unknown source evidence was generated by any one of the other sources \\(B = b \\in \\mathcal{S} \\setminus a_x\\). Mathematically, \\[ \\begin{array}{cc} H_A: &amp; e_u \\text{ generated by } a_x \\\\ H_B: &amp; e_u \\text{ generated by some } b \\in \\mathcal{S} \\setminus a_x. \\end{array} \\] Let’s consider two possible scores, both of which will be based off of an accepted dissimilarity metric, \\(\\delta(\\cdot, \\cdot) \\geq 0\\). The first score that we will consider is \\[ S_1(e_u, e_1, ..., e_K) = \\log \\frac{\\delta(e_u, e_{a_x})}{ \\min_{b \\in \\mathcal{S} \\setminus a_x} \\delta(e_u, e_b) }. \\] The second score that we will consider is \\[ S_2(e_u, e_1, ..., e_K) = \\log \\frac{\\delta(e_u, e_{a_x})}{ \\sum_{b \\in \\mathcal{S} \\setminus a_x} w(b)\\delta(e_u, e_b) }, \\] where \\(w(b)\\) are weights with \\(\\sum_{b \\in \\mathcal{S} \\setminus a_x} w(b) = 1\\). Intuitively, the first score should perform well. The dissimilarity in the numerator should be compared with the smallest dissimilarity in $ a_x$. In the absence of other prior information, only the relative size of the numerator dissimilarity to the smallest dissimilarity of \\(b \\in \\mathcal{S} \\setminus a_x\\) should matter. The second score would likely be easier to study in terms of mathematical properties. For example, it might be possible to assume \\(E \\left[ \\delta(e_u, e_i) \\right] = \\mu_1 &lt; \\infty\\) if the source of \\(e_u\\) is that of \\(e_i\\) but that \\(E \\left[ \\delta(e_u, e_i) \\right] = \\mu_2 &lt; \\infty\\) if the sources are different. One might be able to show some type of consistency property if, instead of one copy of \\(E = (e_u, e_1, ..., e_K)^{\\top}\\), we now have \\(N\\) iid copies \\(E_i = (e_u, e_1, ..., e_K)^{\\top}_i\\). Then, using \\(\\frac{1}{N} \\sum_{i = 1}^{N} \\delta(e_{u_i}, e_{j_i})\\) in place of \\(\\delta(e_u, e_j)\\) yields the ability to use the law of large numbers. This may be impractical in any real life situation, but I consider the score here nonetheless. In more generality, there seems to be no reason why a multisource score couldn’t be constructed using an arbitrary summary statistic of the “similarity” scores computed between the unknown source evidence and the alternative population. 8.3.5 Multisource example For simplicity, we will again assume that all evidence is generated from independent, univariate Gaussian distributions. Specifically, \\[ \\begin{array}{ccc} H_p: &amp; e_u \\sim N(\\mu_K, \\sigma^2), &amp; e_i \\sim N(\\mu_i, \\sigma^2), i \\in \\{ 1,..., K \\} \\\\ H_d: &amp; e_u \\sim GMM(\\{\\mu_k\\}_{k = 1}^{K - 1}, \\{ \\pi_k \\}_{k = 1}^{K - 1}, \\sigma^2), &amp; e_i \\sim N(\\mu_i, \\sigma^2), i \\in \\{ 1,...,K \\} \\end{array}. \\] where all random variables are assumed to be independent conditional on each hypothesis. We will further assume that \\(\\mu_i \\stackrel{iid}{\\sim} N(0, \\tau^2), i \\in \\{ 1,..., K-1 \\}\\). log-LR versus log-SLR scatterplots under hypothesis P and D using three types of SLRs which correspond to using different statistics to aggregate dissimilarity scores in the alternative source population. We try min, average, and max, corresponding to rows 1-3, respectively.Results are based on 10,000 observations for each hypothesis. RMSE Exp.Cond.KL KL true.KL type hypothesis 3.1371 2.1718 2.2813 4.4527 min P 8.4669 5.1396 1.8487 6.9865 min D 3.4835 2.3965 2.0568 4.4527 avg P 7.6549 4.2336 2.7529 6.9865 avg D 3.8587 2.7198 1.7333 4.4527 max P 7.0711 4.1623 2.8250 6.9865 max D log-LR versus log-SLR scatterplots under hypothesis P and D using four types of SLRs. The first three scores correspond to using different statistics to aggregate dissimilarity scores in the alternative source population. The fourth score is essentially the predicted probability of Hypothesis P being true given the first three scores based on a sparse Gaussian process model. We try min, average, and max, corresponding to rows 1-3, respectively. Results are based on 1,000 observations for each hypothesis due to training time for the sparse Gaussian process. RMSE Exp.Cond.KL KL true.KL type hypothesis 1 3.0605 2.0616 2.3206 4.3809 min P 3 3.2725 2.0710 2.3111 4.3809 avg P 5 3.7826 2.5398 1.8424 4.3809 max P 7 2.6095 1.5380 2.8442 4.3809 gp P RMSE Exp.Cond.KL KL true.KL type hypothesis 2 7.9189 4.7271 2.1101 6.8364 min D 4 7.1000 3.8640 2.9724 6.8364 avg D 6 6.6734 4.0090 2.8274 6.8364 max D 8 6.9872 3.3887 3.4414 6.8364 gp D Aggregating scores via the sparse GP results in a final score that uniformly beats each of the other scores under both the prosecution and defense hypotheses in terms of the score KL divergence. 8.3.6 Other Possible Viewpoints? I have assumed in the previous section that the order of consideration of hypotheses should not affect the ordering of the data vector \\(E = (e_u,e_A,e_B)\\) or of the ordering of these arguments to the score function. This seems reasonable, but perhaps [REFS] would argue that considering \\(H_A\\) first, \\(E = (e_u, e_A, e_B)\\) and \\(s(E) = s(e_u, e_A, e_B)\\), but considering \\(H_B\\) first, \\(E = (e_u, e_B, e_A)\\) and \\(s(E) = s(e_u, e_B, e_A)\\). In this case, \\(SLR_{A,B} \\neq \\frac{1}{SLR_{B,A}}\\) because we switch the order of arguments to the score from one SLR to the other. Note that, however, if we relax the independence assumptions of independence under either \\(H_A\\) or \\(H_B\\), then even the LR becomes “incoherent” because \\(\\frac{p(e_u, e_A, e_B|H_A)}{p(e_u, e_A, e_B|H_B)} \\neq \\frac{p(e_u, e_B, e_A|H_A)}{p(e_u, e_B, e_A|H_B)}\\) in general. It is true that the LR depends only on the evidence of the unknown source in this specific scenario, but that is a consequence of modeling assumptions and not of LR paradigmatic principals. 8.4 Copper Wire Synthetic Data Score KL divergences and Monte Carlo standard errors for five randomly generated synthetic copper wire data sets under \\(H_p\\) and \\(H_d\\). 8.5 Optimal matching problem 8.5.1 Two groups case. Suppose there are two groups \\(\\pi_{1}\\) and \\(\\pi_{2}\\) with densities \\(f_{1}(x)\\) and \\(f_{2}(x)\\) on the support \\(x \\in T\\). Let \\(p_{1}\\) and \\(p_{2}\\) be the prior probabilities of groups \\(\\pi_{1}\\) and \\(\\pi_{2}\\), respectively. There are new observations \\(\\mbox{obs}_{1}\\) and \\(\\mbox{obs}_{2}\\) with measurements \\(x_{1}\\) and \\(x_{2}\\), respectively. The goal is to distinguish there the new observations are from the same group or not. That is to partition the space \\(T \\times T\\) in to \\(T_{m} \\cup T_{nm}\\), where we conclude \\(\\mbox{obs}_{1}\\) and \\(\\mbox{obs}_{2}\\) are from the same group if \\((x_{1}, x_{2})\\) falls into \\(T_{m}\\); and otherwise if \\((x_{1}, x_{2}) \\in T_{nm}\\). The two type errors: Matching error: \\((x_{1}, x_{2}) \\in T_{m}\\) if \\(\\mbox{obs}_{1} \\in \\pi_{1}, \\mbox{obs}_{2} \\in \\pi_{2}\\) or \\(\\mbox{obs}_{1} \\in \\pi_{2}, \\mbox{obs}_{2} \\in \\pi_{1}\\); Unmatching error: \\((x_{1}, x_{2}) \\in T_{um}\\) if \\(\\mbox{obs}_{1}, \\mbox{obs}_{2} \\in \\pi_{1}\\) or \\(\\mbox{obs}_{1}, \\mbox{obs}_{2} \\in \\pi_{2}\\). The probability of errors are: \\[P(\\mbox{Matching error}) = \\int_{T_{m}} \\{f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})\\}p_{1}p_{2}dx_{1}dx_{2},\\] \\[P(\\mbox{Unmatching error}) = \\int_{T_{um}} \\{f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2}\\}dx_{1}dx_{2}.\\] Consider the unweighted sum of those two error probabilities \\(P(\\mbox{error}) = P(\\mbox{Matching error}) + P(\\mbox{Unmatching error})\\). We have \\[P(\\mbox{error}) = \\int_{T_{m}} \\big[\\{f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})\\}p_{1}p_{2} - \\{f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2}\\}\\big]dx_{1}dx_{2} + C,\\] where \\(C\\) is a constant. The minimum of this error probability with respect to \\(T_{m}\\) occurs when \\[\\begin{equation} T_{m} = \\bigg\\{(x_{1}, x_{2}): \\frac{[f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})]p_{1}p_{2}}{f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2}} &lt; 1 \\bigg\\}. \\label{eq:Optimalrule1} \\end{equation}\\] This decision region is the optimal matching rule to minimize the probability of the matching errors. Note that \\[f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2} - [f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})]p_{1}p_{2} = \\{f_{1}(x_{1})p_{1} - f_{2}(x_{1})p_{2}\\}\\{f_{1}(x_{2})p_{1} - f_{2}(x_{2})p_{2}\\}.\\] The optimal region \\(T_{m}\\) in () is equivalent to \\[\\begin{eqnarray} \\frac{f_{1}(x_{1})}{f_{2}(x_{1})} &lt; \\frac{p_{2}}{p_{1}} &amp;\\mbox{and}&amp; \\frac{f_{1}(x_{2})}{f_{2}(x_{2})} &lt; \\frac{p_{2}}{p_{1}} \\ \\mbox{ or } \\nonumber \\\\ \\frac{f_{1}(x_{1})}{f_{2}(x_{1})} &gt; \\frac{p_{2}}{p_{1}} &amp;\\mbox{and}&amp; \\frac{f_{1}(x_{2})}{f_{2}(x_{2})} &gt; \\frac{p_{2}}{p_{1}}, \\label{eq:Optimalrule2} \\end{eqnarray}\\] which corresponds to the optimal classification rule. From (), the optimal matching rule is equivalent to the optimal classification rule as long as we conclude the observations matched from one group if they are classified to the same group. Normal distribution. As an example, assume \\(\\pi_{1}\\) and \\(\\pi_{2}\\) are from normal distributions with mean \\(\\mu_{1}\\) and \\(\\mu_{2}\\), and covariance \\(\\Sigma\\). Further assume the prioir probabilities are the same \\(p_{1} = p_{2} = 1 / 2\\). The optimal decision is to classify \\(x_{1}\\) and \\(x_{2}\\) into the same group if \\[\\begin{equation} \\frac{\\exp\\big[ \\{x_{2} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big] + \\exp\\big[ \\{x_{1} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big]} {1 + \\exp\\big[ \\{x_{1} + x_{2} - (\\mu_{1} + \\mu_{2})\\}&#39; \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big]} &lt; 1. \\label{eq:OptimalruleNormal1} \\end{equation}\\] It can be shown that the above inequality is equivalent to \\[\\begin{eqnarray} \\exp\\big[ \\{x_{2} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big] &lt; 1 &amp;\\mbox{and}&amp; \\exp\\big[ \\{x_{1} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big] &lt; 1 \\ \\mbox{ or } \\nonumber \\\\ \\exp\\big[ \\{x_{2} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big] &gt; 1 &amp;\\mbox{and}&amp; \\exp\\big[ \\{x_{1} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\big] &gt; 1 \\label{eq:OptimalruleNormal2} \\end{eqnarray}\\] For discriminat analysis, it is well known that the optimal classification rule under normal distribution is to classify \\(x_{1}\\) and \\(x_{2}\\) to \\(\\pi_{1}\\) if \\(\\{x_{1} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\leq 0\\) and \\(\\{x_{2} - (\\mu_{1} + \\mu_{2}) / 2\\} \\Sigma^{-1} (\\mu_{2} - \\mu_{1}) \\leq 0\\) respectively, and classify them to \\(\\pi_{2}\\) if otherwise. Feature difference is a method to solve the matching problem via classification. Take \\(d = x_{1} - x_{2}\\) as the pairwise difference between two observations. It is clear that \\(d \\sim N(0, 2 \\Sigma)\\) if \\(x_{1}\\) and \\(x_{2}\\) are both from either \\(\\pi_{1}\\) or \\(\\pi_{2}\\), and \\(d \\sim N(\\mu_{1} - \\mu_{2}, 2 \\Sigma)\\) or \\(d \\sim N(\\mu_{2} - \\mu_{1}, 2 \\Sigma)\\) if \\(x_{1}\\) and \\(x_{2}\\) are from different groups. Let \\(f_{m}(d)\\) and \\(f_{um}(d)\\) be the density of \\(d\\) if two observations are from the same group and different groups, respectively. Then, \\(f_{m}(d)\\) is the normal density with mean \\(0\\) and covariance \\(2 \\Sigma\\), and \\(f_{um}(d)\\) is the mixture normal \\(0.5 N(\\mu_{1} - \\mu_{2}, 2 \\Sigma) + 0.5 N(\\mu_{2} - \\mu_{1}, 2 \\Sigma)\\). The optimal discriminant rule is to classify \\(d\\) into the unmatching case if \\[\\frac{f_{um}(d)}{f_{m}(d)} = \\frac{\\exp\\{-(d - \\mu_{1} + \\mu_{2})&#39; \\Sigma^{-1} (d - \\mu_{1} + \\mu_{2}) / 4\\} + \\exp\\{-(d + \\mu_{1} - \\mu_{2})&#39; \\Sigma^{-1} (d + \\mu_{1} - \\mu_{2}) / 4\\}}{2 \\exp(-d&#39; \\Sigma^{-1} d / 4)} &gt; \\frac{p_{1}^{2} + p_{2}^{2}}{2 p_{1} p_{2}}.\\] Let \\(\\mu_{d} = \\mu_{1} - \\mu_{2}\\). The above inequality is equivalent to \\[\\exp(d&#39; \\Sigma^{-1} \\mu_{d} / 2) + \\exp( - d&#39; \\Sigma^{-1} \\mu_{d} / 2) &gt; \\exp(\\mu_{d}&#39; \\Sigma^{-1} \\mu_{d} / 4) \\frac{p_{1}^{2} + p_{2}^{2}}{p_{1}p_{2}},\\] which is approximately equivalent to \\[|d&#39; \\Sigma^{-1} \\mu_{d}| / 2 &gt; \\mu_{d}&#39; \\Sigma^{-1} \\mu_{d} / 4 + \\log (p_{1}^{2} + p_{2}^{2}) - \\log(p_{1}p_{2}).\\] As an illustration, consider one dimensional feature space. Take \\(\\mu_{1} = 1, \\mu_{2} = -1, \\Sigma = 1\\), and \\(p_{1} = p_{2} = 0.5\\). Figure 1 shows the optimal matching rule and the optimal rule based on feature difference. We see that in this example the matching region from the feature difference method only overlaps a small fraction of that from the optimal matching rule, and there is a missing alignment for the feature difference method in the two small triangles at the origin. We also note that even though most of the pink area and the blue area in Figure 1 don’t overlap, the probabilities that the pair of data \\((x_{1}, x_{2})\\) falling into those non-overlapping regions could be small, especially if the absolute value of either coordinate is large. See the contours of multivariate normal distribution in Figure 1. ## Warning: package &#39;mvtnorm&#39; was built under R version 4.0.2 Figure 8.1: Matching regions from the optimal rule (in pink) and the method based on feature difference (in blue). The contours of multivariate normal distribution with means \\((1, -1)\\) (unmatching case) and \\((1, 1)\\) (matching case) are marked in black and red, respectively, where the covariance is identity. Comparison with random forest. We also conducted a small scale simulation to compare the optimal matching rule with the random forest method applied on the feature difference. The traning data include 50 observations from \\(N(1, 1)\\) and \\(N(-1, 1)\\). There are additional 10 observations from each of the group serving as the testing data. We evaluate the percentage of the matching errors on the pairs of the testing data. We repeated the whole simulation 100 times. The accuracy rates are 0.762 and 0.708 for the optimal matching rule and the random forest applied on the differences of the measurements, respectively. 65% out of the 100 repetitions, the former method has higher accuracy than the latter method. 8.5.2 Topics needs exploration How to quantify the matching error rates when the training data only inlcude a small part of many potential groups? How training errors change as more and more features are collected (dimension \\(p\\) increases), where only a small fraction of those features carry useful signals (feature selection). 8.5.3 Brief introduction to optimal rule Suppose that there are multiple sources denoted by \\(\\pi_g\\) for \\(g = 1,\\ \\cdots,\\ G\\), and observations from each \\(\\pi_g\\) follow a distribution denoted by \\(f(\\cdot|\\pi_g)\\). As an example, the figure below shows three different normal distributions by source. Let’s say we have \\(n\\) observations from those souces but do not know which sources they actually belong to. Our goal is to identify if two observations represented by \\(x\\) and \\(y\\) would have come from the same sources or not. For \\(n\\) independent observations, \\({n \\choose 2}\\) pairwise comparisons occur. When we sampled 300 observations from each density in the plot above, the following plot illustrates whether each pair actually come from the same sources or not. Blue dots represent the pairs from the same sources, and red ones indicates the pairs from the different sources. If we can predict the blue area accurately as much as possible, our goal to predict the membership of pairwise observations will be attained. Figure 8.2: True membership of pairs The following figure displays the optimal rule’s matching results using the data from the previous plot. Likewise, the blue dots represent the matched zone by optimal rule where the pairs are predicted to be from the same sources, and the red ones represent the unmatched zone by optimal rule where the pairs are predicted to be from different sources. Figure 8.3: Membership of pairs predicted by optimal rule Now, let \\(T_{SS}\\) and \\(T_{DS}\\) denote the same source zone, and a different source zone, respectively where \\(T_{SS} \\cup T_{DS} = T \\times T\\), \\(T\\) is the support of observations, and \\(T_{SS} \\cap T_{DS} = \\emptyset\\). With the notation, if a pair, \\((x,y) \\in T_{SS}\\), then they are said to be matched. Otherewise, \\((x,y) \\in T_{DS}\\), which means that the pair is not matching, unmatched. In addition, let \\(\\pi_X\\) and \\(\\pi_Y\\) denote the sources for \\(x\\), and \\(y\\), two observations in a pair. Our prior belief for sources is denoted by \\(p_g\\) for a source, \\(\\pi_g\\), such that the probability of an observation belonging to \\(\\pi_g\\) is equal to \\(p_g\\), and \\(\\sum_{g=1}^G p_g = 1\\) where \\(G\\) is the total number of sources considered. Optimal rule is constructed to minimize the total error occuring when matching the memberships of two observations. The total error probability can be written as the following: \\(\\begin{aligned} &amp;P(\\mbox{total error}) \\\\ &amp;=P(\\mbox{Incorrectly matching pairs})\\\\ &amp;=P(\\mbox{A pair of observations matched when they are from different sources}) \\ +\\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ P(\\mbox{A pair of observations unmatched when they are from the same sources})\\\\ &amp;= P\\left((X,Y) \\in T_{SS}|\\pi_X \\ne \\pi_Y\\right)P\\left(\\pi_X \\ne \\pi_Y \\right) + P\\left((X,Y) \\in T_{DS}|\\pi_X = \\pi_Y\\right)P\\left(\\pi_X = \\pi_Y \\right) \\\\ &amp;=\\int_{T_{SS}} \\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_{g_1}p_{g_2} dx dy + \\sum_{g=1}^G p_g^2 - \\int_{T_{SS}}\\sum_{g=1}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_g^2 dxdy \\\\ &amp;= \\int_{T_{SS}} \\left[\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_{g_1}p_{g_2}\\right] - \\left[\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) p_g^2\\right] dx dy + C \\\\ &amp;\\mbox{where } C = \\sum_{g=1}^G p_g^2 = P(\\pi_X = \\pi_Y) \\mbox{ is a constant over } T_{SS}. \\end{aligned}\\) If we could collect every \\((x,y) \\in T \\times T\\) such that \\[\\left[\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_{g_1}p_{g_2}\\right] - \\left[\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) p_g^2\\right] &lt; 0,\\] then over the set denoted by \\(T_{SS}^{Opt}\\), the intergral will attain its minimum, so does \\(P(\\mbox{total error})\\). That is, \\[T_{SS}^{Opt} = \\underset{T_{SS}}{\\mathrm{argmin}} \\ P(\\mbox{total error}).\\] Note above that the parameters for true densities are unknown in practice, so parameters will be replaced with sample estimates. (e.g., sample mean, and sample variance) 8.5.4 Optimal rule for matching problems We would like to statistically tell if two observations would have com from the same source or not, which is said to be matching problems. The optimal rule is designed to deal with matching problems, and to minimize the matching error rate defined as the unweighted or weighted sum of the probabilities related to mismatching the memberships of two observations. Let \\(X\\) and \\(Y\\) be two independent random variables associated with the observations \\(x\\) and \\(y\\), respectively, and \\(T\\) be the support of them. i.e., \\(x \\in T\\), \\(y \\in T\\), and \\((x,y)\\in T \\times T\\). Suppose that there are \\(G\\) known sources denoted by \\(\\pi_1,\\ \\pi_2,\\ \\cdots ,\\ \\pi_G\\) with corresponding prior probabilities denoted by \\(p_g \\in (0,1)\\) for \\(g = 1,\\ \\cdots,\\ G\\) where \\(\\sum_{g=1}^G p_g = 1\\). Next, let \\(\\pi_X\\) and \\(\\pi_Y\\) denote the source of \\(X\\) and \\(Y\\). Then, the conditional pdf of \\(X\\) if it is from a source \\(\\pi_g\\) can be defined as \\(f_X(x|\\pi_g)\\) with the prior probability, \\(p_g = P(\\pi_X = \\pi_g)\\). Likewise, the conditional pdf of \\(Y\\) if it is from a source \\(\\pi_{g&#39;}\\) is written as \\(f_Y(y|\\pi_{g&#39;})\\) with the prior probability, \\(p_{g&#39;} = P(\\pi_Y = \\pi_{g&#39;})\\). Lastly, let \\(T_{SS}\\) and \\(T_{DS}\\) be the zone for the same sources and zone for the different sources such that \\(T_{SS} \\cup T_{DS} = T \\times T\\), and \\(T_{SS} \\cap T_{DS} = \\emptyset\\). Now, we can define two errors occurring in matching problems using the notation above: The same source error: \\(\\pi_X = \\pi_g\\) and \\(\\pi_Y = \\pi_{g&#39;}\\) for \\(g \\ne g&#39;\\), but \\((x,y) \\in T_{SS}\\); Different source error: \\(\\pi_X = \\pi_Y = \\pi_g\\) for some \\(g\\), but \\((x,y) \\in T_{DS}\\). It follows that the probabilities of those errors above are given as the following: \\(\\pi_X = \\pi_g\\) and \\(\\pi_Y = \\pi_{g&#39;}\\) for \\(g \\ne g&#39;\\), but \\((x,y) \\in T_{SS}\\): \\(\\begin{aligned} P(\\mbox{The same source error}) &amp;= P\\left((X,Y) \\in T_{SS}|\\pi_X \\ne \\pi_Y\\right)P\\left(\\pi_X \\ne \\pi_Y \\right)\\\\ &amp;= \\sum_{g_1 \\ne g_2}^G \\int_{T_{SS}} f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_{g_1}p_{g_2} dx dy \\\\ &amp;= \\int_{T_{SS}} \\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y| \\pi_{g_2}) p_{g_1}p_{g_2} dx dy, \\end{aligned}\\) \\(\\pi_X = \\pi_Y = \\pi_g\\) for some \\(g\\), but \\((x,y) \\in T_{DS}\\): \\(\\begin{aligned} P(\\mbox{different source error}) &amp;= P\\left((X,Y) \\in T_{DS}|\\pi_X = \\pi_Y\\right)P\\left(\\pi_X = \\pi_Y \\right)\\\\ &amp;= \\sum_{g=1}^G \\int_{T_{DS}} f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) p_g^2 dx dy \\\\ &amp;=\\sum_{g=1}^G p_g^2 \\int_{T_{DS}} f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) dx dy \\\\ &amp;=\\sum_{g=1}^G p_g^2 \\left[1 - \\int_{T_{SS}} f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) dx dy \\right]\\\\ &amp;=\\sum_{g=1}^G p_g^2 - \\int_{T_{SS}}\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) p_g^2 dx dy. \\\\ \\end{aligned}\\) Then, the total error probability as the unweighted sum of the two probabilities is given as the following: \\(\\begin{aligned} &amp;P(\\mbox{total error}) \\\\ &amp;= P(\\mbox{The same source error}) + P(\\mbox{different source error})\\\\ &amp;=\\int_{T_{SS}} \\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_{g_1}p_{g_2} dx dy + \\sum_{g=1}^G p_g^2 - \\int_{T_{SS}}\\sum_{g=1}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_g^2 dxdy \\\\ &amp;= \\int_{T_{SS}} \\left[\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y|\\pi_{g_2}) p_{g_1}p_{g_2}\\right] - \\left[\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) p_g^2\\right] dx dy + C \\\\ &amp;\\mbox{where } C = \\sum_{g=1}^G p_g^2 = P(\\pi_X = \\pi_Y) \\mbox{ is a constant over } T_{SS}. \\end{aligned}\\) Recall that \\(T_{SS} \\subset T \\times T\\) is the set of all pairs of the form of \\((x,y)\\) from the same sources, called the same source zone, and the optimal rule’s mechanism is to minimize the total error probability. Note in the equation above that \\(P(\\mbox{total error})\\) attains its minimum with respect to \\(T_{SS}\\) if \\(T_{SS}\\) is the set of all possible \\((x,y) \\in T \\times T\\) satisfying the following: \\[\\left[\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y| \\pi_{g_2}) p_{g_1}p_{g_2}\\right] - \\left[\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g})p_g^2 \\right] &lt; 0.\\] That’s because such \\(T_{SS}\\) is the collection of all \\((x,y)\\) such that the intergrand is negative. Therefore, the optimal rule is defined as \\[T_{SS}^{Opt} = \\left\\{(x,y):\\ \\frac{\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y| \\pi_{g_2}) p_{g_1}p_{g_2}}{\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g}) p_g^2} &lt; 1 \\right\\}.\\] Additionally, We can also consider the weighted total error probability as the weighted sum of the two error probabilities: for a given \\(w \\in [0,1]\\), \\[P(\\mbox{weighted total error}) = (1-w)\\cdot P(\\mbox{The same source error}) + w\\cdot P(\\mbox{different source error}).\\] It can be shown that \\(P(\\mbox{weighted total error})\\) is equal to \\(\\int_{T_{SS}} (1-w)\\left[\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y| \\pi_{g_2}) p_{g_1}p_{g_2}\\right] - w\\left[\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g})p_g^2 \\right] dx dy + w\\cdot C\\). Therefore, by the same logic, the generalized optimal rule designed to minimize \\(P(\\mbox{weighted total error})\\) can be defined as the following: \\[T_{SS}^{Opt, w} = \\left\\{(x,y):\\ \\frac{\\sum_{g_1 \\ne g_2}^G f_X(x|\\pi_{g_1}) f_Y(y| \\pi_{g_2}) p_{g_1}p_{g_2}}{\\sum_{g=1}^G f_X(x| \\pi_{g}) f_Y(y| \\pi_{g})p_g^2} &lt; \\frac{w}{1-w} \\right\\}.\\] 8.5.5 Simulation plan 8.5.5.1 Matching techniques For matching problems, techniques using feature differences have been commonly used. A feature difference as a measure of similarity (discrepancy) can be defined to be the difference between two observations in a pair. Then, traditional classification methods can be applied to the feature differences in order to predict if two observations having such similarity or discrepancy would have belonged to the same sources. In this report, we compare the performance of common classification methods based on feature differences with that of optimal rule. Note here that optimal rule works based on the entire data instead of feature differences. A list of the methods compared is shown below. Optimal rule Linear discriminant analysis (LDA) based on feature differences Quadratic discriminant analysis (QDA) based on feature differences Randomforest based on feature differences 8.5.5.2 ROC curve for comparison ROC curve is a diagnostic plot of binary classifiers, and with varying appropriate thresholds for a predictive model, ROC curve can be plotted, which is a great way to figure out predictive performance. The matching techniques we considered above are binary in that their prediction results for each pair of data represent whether the two observations are from the same sources or not rather than which source each observation would have come from. Thus, ROC curve will be used for comparing the matching techniques in this report. Besides, threshold is different depending on matching methods, and varying threshold values lead to trade-off between true positive rate (TPR), and false positive rate (FPR). For optimal rule, the weight \\(w\\) in \\(T_{SS}^{Opt, w}\\) plays a role of threshold in ROC curve. We use prior probabilities as threshold for LDA and QDA based on feature differences where the prior indicates the probabilities of two groups—the same sources, and different sources—applied to the discriminant rules. Lastly, cutoff values for the two groups are used as threshold for randomforest so that the “winning” group for each pair of observations is the one with the maximum ratio of its vote share to cutoff. 8.5.5.3 Set-up for simulations Assume that the conditional distribution, \\(f(\\cdot | \\pi_g)\\), is a normal distribution for any \\(g = 1,\\ \\cdots,\\ G\\). The distributions will be used to generate data below. They may have different parameters for mean, variance, and covariance by a source, and some of them may share common parameters. For prior probabilities, \\(p_g = 1/G\\) is used so that the number of observations per a source is equal. Step 1: According to the distributions for sources, 50 observations per a source are generated for a training set, and additional 10 observations per a source are generated for a test set. i.e., \\(50 \\times G\\) observations in a training set and \\(10 \\times G\\) observations in a test set. Step 2: Pair all obesrvations in the test set. Hence, there will be the matching results for \\({10G \\choose 2}\\) pairs in the test set so that whether the two observations in each pair would have come from the same sources or not are predicted. Step 3: For a matching technique selected, using different thresholds, compute the false positive rate (FPR), and the true positive rate (TPR) based on the prediction results with the test set. Here, FPR indicates the estimated probability of the same source error and TPR indicates 1 minus the estimated probability of the different source error. Step 4: By plotting the values of TPR over corresponding values of FPR, a single ROC curve is constructed. Step 5: Repeat the step 1 to 4 100 times. Step 6: Incorporate 100 ROC curves into a single monotonic ROC curve—e.g., incorporation by averaging out. 8.5.5.4 How to average multiple ROC curves For the step 6, there are some issues of averaging multiple ROC curves by the values of FPR because no matter how densely the coordinates for (FPR, TPR) are formed, some ROC curves may not share common values of FPR. With that approach, the average ROC curve may be non-monotonic ROC curve which is not a desirable property of a ROC curve. The figure below illustrates the result. Hence, we consider two differnt ways to average out, and the description and examples are shown below: Average of the values of TPRs and FPRs by threshold value Average of cumulatively largest vause of TPR in each bin for FPR. Like above, two types of average ROC curves will be provided for comparing the performance of several methods. 8.5.6 Simulation results 8.5.6.1 Univariate Result 1 ## Error in loadNamespace(name): there is no package called &#39;rlist&#39; 8.5.6.2 Univariate Result 2 ## Error in loadNamespace(name): there is no package called &#39;rlist&#39; 8.5.6.3 Univariate Result 3 ## Error in loadNamespace(name): there is no package called &#39;rlist&#39; 8.5.6.4 Univariate Result 4 ## Error in loadNamespace(name): there is no package called &#39;rlist&#39; 8.5.6.5 Univariate Result 5 ## Error in loadNamespace(name): there is no package called &#39;rlist&#39; 8.5.6.6 Univariate Result 6 ## Error in loadNamespace(name): there is no package called &#39;rlist&#39; 8.5.6.7 Multivariate Result 1; G = 2, N = 2 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 10&amp;-10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 16&amp;-4\\\\-4&amp;25 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;1 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 9&amp;4\\\\4&amp;36 \\end{bmatrix}\\] 8.5.6.8 Multivariate Result 2 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 10&amp;-10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 16&amp;-4\\\\-4&amp;25 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;1 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 9&amp;4\\\\4&amp;36 \\end{bmatrix}\\] \\[3\\] \\[\\begin{bmatrix} 20&amp;-20 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 16&amp;-9\\\\-9&amp;25 \\end{bmatrix}\\] 8.5.6.9 Multivariate Result 3 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 1&amp;10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 1&amp;-1\\\\-1&amp;4 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;12 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 1&amp;0.5\\\\0.5&amp;1 \\end{bmatrix}\\] \\[3\\] \\[\\begin{bmatrix} 0&amp;10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 1&amp;-1.5\\\\-1.5&amp;4 \\end{bmatrix}\\] \\[4\\] \\[\\begin{bmatrix} 2&amp;8 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 1&amp;-0.1\\\\-0.1&amp;4 \\end{bmatrix}\\] \\[5\\] \\[\\begin{bmatrix} -2&amp;7 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;-2\\\\-2&amp;4 \\end{bmatrix}\\] 8.5.6.10 Multivariate Result 4 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 3&amp;10&amp;5 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;2&amp;-0.3\\\\2&amp;25&amp;8\\\\-0.3&amp;8&amp;9 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;20&amp;15 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;2&amp;-3\\\\2&amp;49&amp;2\\\\-3&amp;2&amp;9 \\end{bmatrix}\\] \\[3\\] \\[\\begin{bmatrix} -5&amp;30&amp;25 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;1&amp;-0.3\\\\1&amp;36&amp;1\\\\-0.3&amp;1&amp;9 \\end{bmatrix}\\] 8.5.6.11 Multivariate Result 5 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 10&amp;-10&amp;1&amp;8&amp;5 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 16&amp;-4&amp;3&amp;3&amp;3\\\\-4&amp;25&amp;3&amp;3&amp;3\\\\3&amp;3&amp;4&amp;2&amp;-0.3\\\\3&amp;3&amp;2&amp;25&amp;8\\\\3&amp;3&amp;-0.3&amp;8&amp;9 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -5&amp;5&amp;-1&amp;12&amp;9 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 9&amp;4&amp;3&amp;3&amp;3\\\\4&amp;36&amp;3&amp;3&amp;3\\\\3&amp;3&amp;4&amp;2&amp;-3\\\\3&amp;3&amp;2&amp;49&amp;2\\\\3&amp;3&amp;-3&amp;2&amp;9 \\end{bmatrix}\\] \\[3\\] \\[\\begin{bmatrix} 4&amp;-4&amp;0&amp;20&amp;2 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 16&amp;-9&amp;3&amp;3&amp;3\\\\-9&amp;25&amp;3&amp;3&amp;3\\\\3&amp;3&amp;4&amp;1&amp;-0.3\\\\3&amp;3&amp;1&amp;36&amp;1\\\\3&amp;3&amp;-0.3&amp;1&amp;9 \\end{bmatrix}\\] 8.5.6.12 Multivariate Result 6 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 10&amp;-10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 25&amp;-4\\\\-4&amp;25 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;1 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] 8.5.6.13 Multivariate Result 7 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 10&amp;-10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 25&amp;-9\\\\-9&amp;36 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;1 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] \\[3\\] \\[\\begin{bmatrix} 20&amp;-20 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] 8.5.6.14 Multivariate Result 8 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} -5&amp;10 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;-2\\\\-2&amp;6 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;6 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] \\[3\\] \\[\\begin{bmatrix} 0&amp;10 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] \\[4\\] \\[\\begin{bmatrix} 2&amp;8 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] \\[5\\] \\[\\begin{bmatrix} -2&amp;7 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] 8.5.6.15 Multivariate Result 9 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 1&amp;8&amp;5 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;0.5&amp;-1\\\\0.5&amp;9&amp;1\\\\-1&amp;1&amp;4 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} -1&amp;12&amp;9 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] \\[3\\] \\[\\begin{bmatrix} 0&amp;20&amp;2 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] 8.5.6.16 Multivariate Result 10 \\[\\mbox{Source, }g\\] \\[\\mu_g\\] \\[\\Sigma_g\\] \\[1\\] \\[\\begin{bmatrix} 1&amp;10&amp;1&amp;8&amp;5 \\end{bmatrix}&#39;\\] \\[\\begin{bmatrix} 4&amp;-2&amp;-1&amp;-1&amp;-1\\\\-2&amp;6&amp;-1&amp;-1&amp;-1\\\\-1&amp;-1&amp;4&amp;0.5&amp;-0.3\\\\-1&amp;-1&amp;0.5&amp;25&amp;0.5\\\\-1&amp;-1&amp;-0.3&amp;0.5&amp;9 \\end{bmatrix}\\] \\[2\\] \\[\\begin{bmatrix} 2&amp;8&amp;-1&amp;12&amp;9 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] \\[3\\] \\[\\begin{bmatrix} 1&amp;12&amp;0&amp;20&amp;2 \\end{bmatrix}&#39;\\] \\[&#39;&#39;\\] 8.5.7 Real data for simulations For the simulations in the previous section, data were generated from normal distributions, and optimal rule outperformed the other methods under the condition. To examine if it works well in practice, we compare it with the other techniques on the glass data published in this paper where the data are referred to as Dataset 3. 8.5.7.1 Data description We use the data set consisting of 31 and 17 glass panes (AA, AB, …, AAQ, AAR / BA, BB, …, BP, BR) provided from company A and B, and 22 to 24 fragments were randomly sampled frome each source, a glass pane. A single fragment as an observational unit has the average amount of concentration for 18 chemical elements such as Li7, Na23, and Mg25 where the mean was caluated from repeatedly measured values of concentration. Hence, each row of the data at our analysis level is made up of a 18-dimensional vector whose coordinates indicate the average amount of concentration for the 18 chemical elements, and extra information like which source fragments came from. 8.5.7.2 Simulation plan for glass data The basic scheme is the same as the explanation in Simulation plan, the section above, but there are some differences. For the glass data, 20 random splits of a dataset will be generated into test sets so that an average ROC curve aggregated from 20 ROC curves can be provided. Specifically, 6 observations per source are assigned to a test set, and the rest is assigned to a training set. Then, 16 to 18 fragments per a source remain in a training set, which is not enough to estimate 18 by 18 covariance matrix for each source, and covariance estimation is a necessary process for optimal rule. Alternatively, we use pooled covariance estimate for optimal rule with the glass data. One way is to use a single pooled covariance across all sources, and another one is apply separate pooled covariance estimate for each company. The latter is based on our prior belief that covariance structure could differ by company, and could be similar within the same company while there is the possibility that a single sample covariance across all sources is more reliable than separate ones. Besides, unlike the other methods, randomforest method needs to train its model while the other methods just computes their matching rule based on summary statistics. Since feature differences are calculated from all pairwise observations in a training set, a serious imbalance to the number of pairs from the same sources, and that from different sources happens as the number of sources and the number of observations increase. To address some potential problems due to the imbalance, we also employ downsampling technique to randomforest method where twice the number of pairs from the same sources is randomly selected from different sources instead of utilizing all pairs from different sources. Lastly, subsets of the whole data will be used to compare mathing techniques where a subset of data is determined by choosing different glass panes. For example, we can apply matching methods to a dataset with 6 different panes (AA, AB, AC, BA, BB, and BC) as the whole data. In this report, a scenario will be based on a subset of the entire glass data, and several scenarios will be presented. To sum up, two kinds of average of 20 ROC curves will be presented for each scenario: average of the values of TPRs and FPRs by threshold value, and average of cumulatively largest vause of TPR in each bin for FPR. Optimal rule’s performance will be compared with LDA, QDA, and randomforest based on feature differences as before, but optimal rule and randomforest have two different scenarios. For optimal rule, one is to apply a single pooled covariance across all sources, and the other is to use separate pooled covariance for each company. Randomforest will present a regualr one without downsampling, and one with downsampling. Consequently, there will be 6 methods compared. 8.5.8 Simulation results using glass data 8.5.8.1 Case 1: 6 sources Panes used: AA, AB, AC BA, BB, BC 8.5.8.2 Case 2: 6 sources Panes used: AAM, AAQ, AAR BO, BP, BR 8.5.8.3 Case 3: 10 sources Panes used: AD, AE, AF, AG, AH BD, BE, BF, BG, BH 8.5.8.4 Case 4: 20 sources Panes used: AAD, AAF, AAH, AAI, AAJ, AAK, AAL, AAM, AAQ, AAR BH, BI, BJ, BK, BL, BM, BN, BO, BP, BR 8.5.8.5 Case 5: All sources (48) Panes used: 31 panes from company A 17 panes from company B 8.5.8.6 Case 6: 6 sets of sources combined by dates 8.5.8.7 Case 7: 10 sets of sources combined by dates 8.5.8.8 Case 8: All sets (26) of sources combined by dates 8.5.9 Extension to openset problems This is motivated to consider the situations where no training data are given for questioned evidence such as glass fragments. In earlier work, we assumed finite known sources corresponding to a closed-set situation. The assumption would break as a questioned evidence does not belong to those known sources. To improve this weakness, and move to openset framework, we used simple hierarchical structure first. 8.5.9.1 Initial set-up \\[X|\\mu, Y|\\mu \\sim N(\\mu, \\Sigma) \\mbox{ where }\\mu \\sim N(M, V), \\ X, Y \\mbox{ are conditionally independent}\\] \\[\\begin{align*} &amp;P(\\mbox{the same source error}) \\\\ &amp;= \\iint_{T_{SS}}\\iint_{\\mathbb{R}^2} f_X(x|\\mu_X)f_Y(y|\\mu_Y)h(\\mu_X)h(\\mu_Y) d\\mu_X d\\mu_Y dx dy \\\\ &amp;= \\iint_{T_{SS}}\\iint_{\\mathbb{R}^2} (2\\pi)^{-1}\\Sigma^{-1}\\exp{\\left(-\\frac{(x - \\mu_X)^2}{2\\Sigma}\\right)}\\exp{\\left(-\\frac{(y - \\mu_Y)^2}{2\\Sigma}\\right)}\\times \\\\ &amp;(2\\pi)^{-1}V^{-1}\\exp{\\left(-\\frac{(\\mu_X - M)^2}{2V}\\right)}\\exp{\\left(-\\frac{(\\mu_Y - M)^2}{2V}\\right)}d\\mu_X d\\mu_Y dx dy\\\\ &amp;= \\iint_{T_{SS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\ M\\end{matrix}\\right), \\left(\\begin{matrix}\\Sigma + V &amp; 0 \\\\ 0 &amp; \\Sigma + V \\end{matrix} \\right) \\right) dxdy \\end{align*}\\] \\[\\begin{align*} &amp;P(\\mbox{different source error}) \\\\ &amp;=\\iint_{T_{DS}}\\iint_{\\mathbb{R}^2} f_X(x|\\mu)f_Y(y|\\mu)h(\\mu)d\\mu d\\mu_Y dx dy \\\\ &amp;=\\iint_{T_{DS}}\\iint_{\\mathbb{R}^2} (2\\pi)^{-1}\\Sigma^{-1}\\exp{\\left(-\\frac{(x - \\mu)^2}{2\\Sigma}\\right)}\\exp{\\left(-\\frac{(y - \\mu)^2}{2\\Sigma}\\right)} \\times\\\\ &amp;(2\\pi)^{-1/2}V^{-1/2}\\exp{\\left(-\\frac{(\\mu - M)^2}{2V}\\right)}d\\mu_X d\\mu_Y dx dy \\\\ &amp;=\\iint_{T_{DS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), \\left(\\begin{matrix}\\Sigma + V &amp; V \\\\ V &amp; \\Sigma + V \\end{matrix} \\right) \\right) dxdy \\end{align*}\\] \\[\\begin{align*} &amp;P(\\mbox{the same source error})(1-w) + P(\\mbox{different source error})w\\\\ &amp;=\\iint_{T_{SS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), \\left(\\begin{matrix}\\Sigma + V &amp; 0 \\\\ 0 &amp; \\Sigma + V \\end{matrix} \\right) \\right) dxdy (1-w)\\\\ &amp;+ \\iint_{T_{DS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), \\left(\\begin{matrix}\\Sigma + V &amp; V \\\\ V &amp; \\Sigma + V \\end{matrix} \\right) \\right) dxdy w \\\\ &amp;= \\iint_{T_{SS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), \\left(\\begin{matrix}\\Sigma + V &amp; 0 \\\\ 0 &amp; \\Sigma + V \\end{matrix} \\right) \\right) dxdy (1-w) + \\\\ &amp;w - \\iint_{T_{SS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), \\left(\\begin{matrix}\\Sigma + V &amp; V \\\\ V &amp; \\Sigma + V \\end{matrix} \\right) \\right) dxdy w \\\\ &amp;= w + \\iint_{T_{SS}} \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), G_1 \\right)(1-w) - \\mbox{MVN}\\left(\\left(\\begin{matrix}M \\\\M\\end{matrix}\\right), G_2 \\right) w dxdy\\\\ &amp;\\mbox{where } G_1 = \\left(\\begin{matrix}\\Sigma + V &amp; 0 \\\\ 0 &amp; \\Sigma + V \\end{matrix} \\right), G_2 = \\left(\\begin{matrix}\\Sigma + V &amp; V \\\\ V &amp; \\Sigma + V \\end{matrix} \\right) \\end{align*}\\] \\[\\begin{align*} T_{SS}^{Opt} = \\left\\{(x,y):\\ \\exp{\\left(-\\frac{1}{2} \\left(\\begin{matrix}x - M \\\\y - M \\end{matrix}\\right)&#39;\\left(G_1^{-1} - G_2^{-1}\\right)\\right)}\\left(\\begin{matrix}x - M \\\\y - M \\end{matrix}\\right) &lt; \\frac{w}{1-w} \\left|G_1\\right|^{1/2}\\left|G_2\\right|^{-1/2}\\right\\} \\end{align*}\\] , which is valid for both univariate and multivariate cases. 8.5.9.2 An example based on normal populations Set-up: \\(X|\\mu, Y|\\mu \\sim \\mbox{N}(\\mu, \\Sigma = 1)\\) where \\(\\mu \\sim \\mbox{N}(M = 0, V =1)\\), which results in \\(X, Y \\sim N(0,2)\\). 8.5.9.3 Application to glassdata Assumption: \\[X|\\mu, Y|\\mu \\sim N(\\mu, \\Sigma) \\mbox{ where }\\mu \\sim N(M, V), \\ X, Y \\mbox{ are conditionally independent.}\\] Here, \\(\\Sigma\\) is replaced with a pooled sample covariance based on training data. For \\(M\\) and \\(V\\), we use the overall sample mean, and the sample covariance of sample means by source based on training data as well. Actually, the sample means and covariance matrices are so different by company that produced glass panes. The names of panes started with letters A, B indicates their production from the company A and B, respectively. Training: AA(1/3), AC(1/4), AE(1/5), AG(1/6) Testset: AB(1/3), AD(1/4), AF(1/5), AH(1/6) Training: BA(12/5), BK(12/12), BN(12/15), BP(12/16) Testset: BB(12/5), BL(12/12), BO(12/15), BR(12/16) Training: AA(1/3), AC(1/4), AE(1/5), AG(1/6) Testset: AI(1/7), AJ(1/7), AK(1/8), AL(1/8) Training: AA(1/3), AC(1/4), AE(1/5), AG(1/6), BA(12/5), BK(12/12), BN(12/15), BP(12/16) Testset: AB(1/3), AD(1/4), AF(1/5), AH(1/6), BB(12/5), BL(12/12), BO(12/15), BR(12/16) Training: AA(1/3), AC(1/4), AE(1/5), AG(1/6)) Testset: BA(12/5), BK(12/12), BN(12/15), BP(12/16) Training: AI(1/7), AJ(1/7), AK(1/8), AL(1/8) Testset: AV(1/13), AY(1/15), AAA(1/16), AAC(1/17) 8.6 SRL behaviour and dependence Draft: August 2020 8.6.1 Introduction. Statistical analysis in forensic science can be used to help guide decision making in a criminal case. In a general court case, a ruling regarding the innocence of a suspect can be thought as if you are contrasting two hypotheses: The prosecutor hypothesis \\((H_p)\\), that the suspect is guilty of the crime. The defense hypothesis \\((H_d)\\), that the suspect is innocent. Suppose that some evidence \\(E\\) was found at the crime scene. Formalizing the previous statement, we can think about the two competing hypotheses and how likely they are. Using Bayes Theorem, we can compute: \\[\\begin{equation} \\label{eq:LR} \\frac{P(H_p/E)}{P(H_d/E)}= \\underbrace{\\frac{P(E/H_p) }{P(E/H_d) }}_{\\text{Likelihood ratio}} \\underbrace{\\frac{P(H_p)}{P(H_d)}}_{\\text{Prior odds}} \\end{equation}\\] The first component is the ratio of competing hypothesis. Following cite{Stern2017} and cite{Cook1998}, it should be noted that decision regarding guilt or innocence is something that is up to the judge and potentially outside of the scope of expertise of the forensic expert. In general, forensic testimony would concern lower level proposition, addressing, for instance, the common source hypothesis. Reframing under this tier of propositions, we will address the hypothesis that the trace found in the crime scene \\((E_x)\\) is a match with the suspect \\((E_y)\\) \\((H_p)\\) or that evidence came from a different source \\((H_d)\\). The Prior odds reflects a pre evidence belief, and the Likelihood ratio or Bayes factor acts as an update of those beliefs into posterior probabilities. In essence, the likelihood ratio allows us to assess the relative probability of observing the evidence under the competing hypothesis. It is a means to quantifying the strength of the evidence. Consider the following example of DNA trace from cite{Stern2017}. DNA was found in a crime scene \\((E_x)\\), a suspect \\(S\\) is arrested and we can obtain a sample from the suspect (\\(E_y\\)). The likelihood ratio can be written as: \\[\\begin{equation} \\label{eq:LR_DNA2} LR= \\frac{Pr(E_x,E_y|H_p)}{Pr(E_x,E_y|H_d)}. \\end{equation}\\] In the case of DNA trace, under some simplifying assumptions, the calculation for the numerator and denominator can be performed studying the match at each j location in a DNA profile. And the denominator can be interpreted as the probability of a random match or that a random person shares the same DNA profile, this refers to the different source hypothesis. Note that the previous calculation required a thorough knowledge of the population frequencies and some simplifying assumptions. In most cases, this information might not be available, or constructing formal probability models can be challenging, so a similarity score can be used instead. SRL has been featured in statistical forensic work with different degrees of emphasis (e.g.: handwriting cite{Hepler2012},cite{Davis2012},firearms: cite{Dong2019},cite{Riva2017},…). Suppose there is a method to compute the similarity between the crime scene evidence and the one collected from the suspect \\(S\\), \\(S(E_x,E_y)\\), (e.g., RF score, Cross-correlation, CMS). Then, the likelihood ratio can be written as, \\[\\begin{equation} SLR= \\frac{Pr(S(E_x,E_y)=s|H_p)}{Pr(S(E_x,E_y)=s|H_d)} \\end{equation}\\] Now we can interpret the numerator as the likelihood that the scores being \\(s\\) under the prosecutor hypothesis and the denominator the likelihood \\(s\\) under the defense hypothesis As noted by cite{Stern2017}, the distribution will depend on the evidence found at the crime scene \\((E_x)\\) so we can formalize this by: \\[\\begin{equation} SLR= \\frac{Pr(S(E_x,E_y)=s|E_x,H_p)}{Pr(S(E_x,E_y)=s|E_x,H_d)}. \\end{equation}\\] SRL allows to use a univariate distribution instead of a more complex probability model that might depend on multiple parameters. Even if the probability model can be formulated, the true parameters are unknown and likely challenging to estimate cite{Hepler2012} To find an estimated value of the SRL (\\(\\hat{SLR}\\)) we can implement a two-step approach that has been used in the literature before cite{…}. Find \\(\\hat{Pr}(S(E_x,E_y)|E_x,H_p)\\) , \\(\\hat{Pr}(S(E_x,E_y)|E_x,H_d)\\) Evaluate \\(\\hat{Pr}(S(E_x,E_y)=s|E_x,H_p)\\) and \\(\\hat{Pr}(S(E_x,E_y)=s|E_x,H_d)\\) at \\(s\\) observed to determine which one is more likely. For the first steps, researchers have used dataset of pairwise comparison or similarity scores. If the hypothesis that the empirical distribution follows a known unimodal distribution cannot be rejected, this distribution is used, and its parameters are estimated using the data. Alternatively, KDE has been used to get an estimate of the PDF. In the literature, \\(\\hat{Pr}(S(E_x,E_y)|E_x,H_p)\\) has also been denoted as within score and \\(\\hat{Pr}(S(E_x,E_y)|E_x,H_d)\\) as between scores. Different issues regarding the use of Score Likelihood ratio in forensic have been discuses in the literature cite{Garton, others}. In particular, we are interested in the results obtained from these techniques are different compared to their LR counterparts, how dependent are the results to the training data, and if there exist other dependence structures. 8.6.2 A pipeline for testing forensic algorithm. The following sections address a simulation approach to assess empirically the SLR and their behavior. The first section describes the step for simulating realistic data, then the steps implemented to construct LR and SLR, and tools for comparison. 8.6.3 Simulating realistic data. 8.6.4 Forensic set up For assessing the behavior of our SRL under realistic data, we begin with data glass data collected by Dr. Joan Buscaglia, which is publicly available and has been previously used in related analysis cite{Aitken and lucy, Ommen}. The data set provides the elemental composition of 62 windows pane, 5 fragments each, evaluation of the composition of 4 elements. Calcium (CA), Potassium (K), Silicone (Si) and Iron (Fe). As previously noted cite{..}, the features provided are actually the log ratio of elements, hence for each window fragment, we have actually 3 features \\(log(CA/K)\\),\\(log(CA/SI)\\),\\(log(CA/Fe)\\). In their database, the windows pane are organized in 3 groups, 16 window pane in groups 1 and 2, 30 on group 3. This grouping will provide the starting point for the pipeline. To set up the forensic experiment, suppose, a crime has been committed, and a glass fragment was recovered from the crime scene. We assume the fragment comes from a particular window \\(W_1\\). A suspect is arrested and a second fragment is found on their clothing, the fragments are assumed to come from window \\(W_2\\). Under the prosecutor and defense hypothesis framework, the prosecutor hypothesis or \\(H_p\\) states that the fragments share the same source, meaning that \\(W_1\\) and \\(W_2\\) are the same window. On the other hand, the defense states that the windows are not the same. Note that we assume that only one fragment is found in the crime scene and another on the suspect for simplicity at first. 8.6.5 Model set up We will also assume that the following random effects model generated the data. \\(y_{ijk}=\\mu_k+a_{ik}+w_{ijk}\\) Where \\(y_{ijk}\\) denotes the 3 dimensional vector of features measured for the \\(j^{th}\\) fragment of the \\(i^{th}\\) window on group \\(k\\). Within each group we have \\(m\\) (\\(i=1,...,m\\)) windows and \\(n\\) fragments (\\(j=1,...,n\\)), in total the group size would be \\(N=nm\\). For example, if we have 10 window panes, with 5 fragments sampled each we would have \\(N=50\\). In the previous model \\(\\mu_k\\) is the population mean for group \\(k\\), and \\(a_{ik}\\) is a random effect associated with the window \\(i\\) in group \\(k\\), \\(w_{ikj}\\) is the RE associated for glass fragment. Assuming that the RE follows a 3 dimensional Gaussian distribution, \\(a_{ik} \\sim N(0,\\Sigma_b)\\) and \\(w_{ijk} \\sim N(0,\\Sigma_w)\\) where \\(\\Sigma_b\\) is the between source covariance matrix and \\(\\Sigma_w\\) is the within source covariance matrix, Hence we will have the following parameter to estimate, \\(\\mu_k\\) , \\(\\Sigma_W\\) and \\(\\Sigma_b\\). Aitken and Lucy proposed the following way to estimate the parameters. For each group we can compute the mean vector for the features as, \\[\\bar{y}_{..k}= \\frac{1}{mn} \\sum_{j=1}^{n} \\sum_{i=1}^{m} y_{ijk},\\] Under this model, we assume that there is two source of variation. The variation that is between the sources or windows, and the variation among the fragments of the same window or in other word within source variation. Lucy and aitken propose a way to compute them when there is only one group, to make our notation consistent suppose that we are dealing with only one group \\(k\\) for now, We can compute the within-group covariance matrix \\(\\hat{\\Sigma}_{wk}=U_k\\) using as:, \\[U_{k}=S_{wk}/(N-m),\\] where \\(S_{wk}= \\sum_{i=1}^{m} \\sum_{j=1}^{n} (y_{ijk}-\\bar{y}_{.ik}) (y_{ijk}-\\bar{y}_{.ik})&#39;\\) The between group covariance matrix \\(\\hat{\\Sigma}_{bk}=C_k\\) can be estimated by \\[C_k=S_k^*/(m-1)-S_{wk}/n(N-m),\\] with \\(S_k^*=\\sum_{i=1}^m (\\bar{y}_{.ik}-\\bar{y}_{y}) (\\bar{X_{i}}-\\bar{X})&#39;\\), Since we have 3 groups, we used the previous steps to find \\(\\hat{U}_{k}\\) and \\(\\hat{C_k}\\) and compute the average to find \\(\\hat{\\Sigma}_{w}\\) and \\(\\hat{\\Sigma}_{b}\\) that is going to be used in the following steps. 8.6.6 Simulation basics. After estimating \\(\\hat{\\mu}_k\\) \\((k=1,2,3)\\), \\(\\hat{\\Sigma}_{b}\\) and \\(\\hat{\\Sigma}_{w}\\) we can simulate 3 groups as follows. For k=1 to 3. Select number of windows \\(m\\) and fragment \\(n\\) within the window We first sample \\(m\\) sources \\(z_{ik} \\sim N(\\hat{\\mu}_k,\\hat{\\Sigma}_{b})\\) We sample \\(n\\) fragments from each source \\(y_{ijk} \\sim N(z_{ik},\\hat{\\Sigma}_{w})\\) For our simulation study, we choose \\(m=10\\) and \\(n=5\\) hence each group will have 50 total fragments, and 150 rows will compose our database. The following plots show the distribution for just one sample, where each point is a fragment within a window and group. The shape of the point denotes the group of origin and the color the Source ID within the group. 8.6.7 SLR. Different scores have been proposed to measures similarity between all pairwise comparisons of trace evidence, in our case, the fragments. BolcK Ni and Lopakta (2015) used distances for measuring different impurities in drugs. Suppose you are making comparisons of two elements, \\(x\\), and \\(y\\), then if you measure \\(p\\) characteristics, the following distances are used. Euclidean: \\(s_e=\\sqrt{ \\sum_{i=1}^{p}(x_i-y_i)^2}\\) City : \\(s_c= \\sum_{i=1}^{p}|x_i-y_i|\\) In forensic glass data analysis, Random Forest (RF) generated scores have been proposed as a way to address the same source problem cite{Carriquiry Park}, cite{Park}. Beginning from all possible pairwise combinations in a data frame, we took the absolute difference for each ratio as our features. The pair of fragments are designated as a known match (KM) if they come from the same windows and non-match (KNM) if not. Since the KM observation clearly outnumber the KNM observations, we use downsampling during the training stage to improve the performance of the classifier as proposed by Carriquiry and Park The prediction of this trained RF is the third score \\(s_{RF}\\) we compare in this document. 8.6.8 LR Since we know the source of the simulated data for each pair, we can compute the common source LR \\[\\lambda_{CS}=\\frac{f(u_x,u_y|\\theta_a,M_p)} {f(u_x|\\theta_a,M_d)f(u_y|\\theta_a,M_d)}\\] \\(u_x\\) is the vector of features for the first item in the pair, and \\(u_y\\) the vector for the second item. Since we sample from a multivariate normal distribution, we use \\(f\\) the density of a multivariate normal whose parameters are \\(\\theta_a=\\{\\hat{\\mu_k},\\hat{\\Sigma_b},\\hat{\\Sigma_w}\\}\\). Under the prosecutor hypothesis the two items share the same source, while in the defense hypotesis has different source. Details of Block covariance… 8.6.9 Train- Test - Validation. Previous papers have used the same data for training their SLR, comparing them to LR. Inspired by machine learning k-fold, we follow a train-test-Validation validation approach as follows. In the Traing Group: Train a RF using Downsampling. Compute the following scores over the Test set: RF score, Euclidean distance, City Distance. For the KM in the Test set, estimate the KDE of each score. For the KNM in the Test set, estimate the KDE of each score. Compute the following scores over the Validation set: RF score, Euclidean distance, City Distance. Plug in the Scores in the KDE found in step 3. This will act as the numerator in each SLR Plug in the Scores in the KDE found in step 4. This will act as the denominator in each SLR Compute the ratio of points 6 and 7. This will act as the SLR. Compute the true LR that will serve as a basis for comparison. The assignment of groups 1,2 or 3 into training, testing and validation set allows to do 3-fold validation permutating the order in which they were assigned, hence for each simulation, we have actually 6 permutations. This allows us to compare the sensitivity of using different training and testing set over the validation results. Permutation order 8.6.10 Metrics for comparing results Cite(Meuwly _2017) address different evaluation metric. Rate of misleading evidence. Recall that the LR is used to update the prior belief of the decision-maker, a ratio larger than 1 would indicate that the evidence leans towards prosecutor hypothesis, that the traces compare belong to the same source. On the opposite direction, a ratio smaller than 1 would support the defense hypothesis. Since our validation process tracks the pairs that genuinely belong to the same source (KM)and those who do not (KNM), we can assess if the ratios found could potentially be misleading. In the case of a KNM, we would expect the ratio to be smaller than 1. The rate of misleading evidence for KNM is then the percentage of pairs that present a ratio larger than 1 but are actually KNM. In the case of a KM, we would expect the opposite, that the ratio is bigger that 1 supporting the prosecutor hypothesis. The rate of misleading evidence for KNM is then the percentage of pairs that present a ratio smaller than 1 but are actually KM. Comparison with LR Since our primary goal is to study the behavior of Score likelihood ratio, we can compare each of them computed for any pairs to the LR counterpart described previously. Scatter plots have been used previously to compare these results, however, do to the domain of the ratio \\((0,\\infty)\\) we follow a discretization for the ratios that follow the scale proposed by Evett et al. (2000). LR range Evidence supporting Prosecutor 1&lt;LR&lt;10 Limited 10&lt;LR&lt;100 Moderate 100&lt;LR&lt;1000 Moderately strong 1000&lt;LR&lt;10000 Strong LR&gt;10000 Very Strong cite{Garton}, defines the following collections of set as a way to compare the ratios. \\[B=\\{(0,10^{-4}),(10^{-4},10^{-3}),...,(1,10),...,(10^4,\\infty) \\} \\] From this discretization, we create a tile plot, mapping the number of times a pair fall in one of the bins for the LR and the corresponding SLR. Ideally, we expect that the values fall within the diagonal showing an agreement between the LR and the SLR Agreements between Validation sets. The Train Test Validation followed allows for testing the agreement between different permutations that have the same groups as Validation set. Consider, for instance the first permutation and the third, although the validation set is the same, we are using different train and test sets. In this case the LR will agree since they do not depend on the train and test, however, we can potentially see difference in the SLR scores. In the distance-based scores, a source of difference is introduced as we use a different Test set to get the KDE. While the RF we exchange the order in which the train and test set are used. Note that also, a third source of difference is introducing in the RF case since we are also implementing a downsampling step. As in the previous comparison, we introduce the following collection of sets as a way to study the difference. \\[D=\\{(-\\infty,-10^4),...,(-10,0),(0,10),...,(10^4,\\infty) \\} \\] If the difference belongs to the 5 or 6 sets, meaning that the difference is close to 0, we can say that the score agrees between the different permutations. 8.6.11 Results 8.6.12 From a single permutation within a simulation The following only illustrates partial results for the first permutation to discuss the challenges faced by the algorithms. First, we report the RF OOB for this permutation is 8.5%, the specific class error is 8% for the KM and 9% for the KNM in the training set. The KDE over the test Set also allows to assess the performance of the RF in discriminating between the two populations. We also present the KDE over the Test set using the Euclidean and the City distance. It shows that a smaller distance is more likely to be observed if we are dealing with a KM pair, although some overlapping is seen. Next we compute the Rate of misleading evidence for KM and KME. We see that in this case, the RF SLR is closer to the rate of misleading evidence for the LR in the case of the KM and the KNM. Table 8.1: Rate of Misleading evidence KM Permutation RME KM Euclidean RME KM City RME KM RF RME KM LR 1 5 2 3 0 Table 8.1: Rate of Misleading evidence KNM Permutation RME KNM Euclidean RME KNM City RME KNM RF RME KNM LR 1 20.5 18.7 6.58 6.58 8.6.13 From all permutation in simulation First we present the RME for each permutatio within this particular sample. We can see that the RME for LR is the same in ‘paired’ permutation since it does not depend on the train-test set. Table 8.2: Rate of Misleading evidence KM Permutation RME KM Euclidean RME KM City RME KM RF RME KM LR 1 5 2 3 0 2 7 7 14 1 3 13 7 2 0 4 4 3 8 0 5 12 9 10 1 6 4 2 4 0 Table 8.2: Rate of Misleading evidence KNM Permutation RME KNM Euclidean RME KNM City RME KNM RF RME KNM LR 1 20.50 18.70 6.580 6.58 2 12.60 10.20 0.444 1.16 3 14.30 9.96 7.640 6.58 4 13.70 9.07 1.070 1.78 5 8.89 7.64 1.510 1.16 6 13.20 9.60 4.440 1.78 Next, the following plot shows the difference between the ‘paired’ permutation. All the difference for the LR belongs to group ‘5’ since the difference is 0 as previously mentioned. In this particular sample, the distance-based SLR shows that most cases fall within groups 5 or 6, meaning the difference is small. This is true for both KM and KNM. In the case of the RF SLR, in this particular sample, the difference is not symmetric. Leaning towards negative values. It is interesting to note that in the KM, the larger percentage does not fall in group 5. Difference in 1st Sample 8.6.14 From all permutation in all simulation The following results are for 500 samples, and in each one we have 6 permutations. First, we present the boxplot for RME for each sample X permutation, approximately 3000 rates for each ratio. (note some “NA” present) LR is the smaller as expected, followed by RF, their range reasonably similar. However, it should be noted that in the case of KM RF present large outliers. Boxplot RME The next plot shows the differences between ‘paired’ permutations across all 500 samples. In the case of the City distance, for KM most cases fall in groups 5 or 6 wichs suggest low sensitivity to changes in train and test sets. On the other hand, RF shows the opposite behavior suggesting that this score based likelihood ratio is more sensitive to changes in train and test sets. Difference City Difference Euclidean Difference RF Regarding the match between the LR and the SLR… Tiles City Tiles Euclidean Tiles RF "],
["outreach-activities.html", "Chapter 9 Outreach activities 9.1 Book on Forensic Science and Statistics", " Chapter 9 Outreach activities CSAFE has several ongoing outreach projects. 9.1 Book on Forensic Science and Statistics Project members: Kiegan Rice Alicia Carriquiry Hal Stern (UCI) General book outline: Chapter 1: Introduction Chapter 2: Analysis of Forensic Evidence Drafted. In the editing stage. Chapter 3: The Path to the Witness Stand Drafted. In the editing stage. Chapter 4: Communicating Evidence in the Courtroom Not drafted. In the writing stage. Chapter 5: Conclusions #Chapter on Statistics for Judges Project members: - Eryn Blagg - Alicia Carriquity Topics the Chapter outlines - Basic Probability - Collecting Data - Describing Data - Statistical Inference - uncertainty Status: final edits "],
["reproducibility-in-research.html", "Chapter 10 Reproducibility in Research 10.1 Computational Reproducibility", " Chapter 10 Reproducibility in Research We are working on improving CSAFE’s reproducibility in research. 10.1 Computational Reproducibility CSAFE’s work is driven by a goal of providing open-source research, including open-source data and open-source algorithms. We believe in transparency and honesty in research, and allowing others to see ``under the hood\" of algorithms that may be used to analyze evidence. In the process of furthering that goal, many of our researchers implement methods and data analysis in R, an open-source scripting language often used in data science and statistics. R and python are both languages which encourage (and rely on) user-developed packages to enhance the language’s ability to perform specific data manipulation and modeling tasks. Many software packages are still under development; that is, functions and methods that exist in a package are subject to changes over time by the developers. This affects our work as statistics researchers in three major ways: - Code to run data analyses and obtain quantitative results may be affected by package updates. This can lead to differing quantitative results and lack of reproducibility of a method. - Researchers’ own packages in development (e.g., bullet pipeline, shoe analysis pipeline) are vulnerable to changes in any package dependencies. - Group collaboration efforts on a developing package can lead to miscommunications, including changes in code that affect others’ results (sorry again, Charlotte!) So, how does this practically affect CSAFE’s research teams, and how can we manage it? For example, consider the bullet analysis ``pipeline\" and some of the R packages that are involved at different steps of the process (this is not exhaustive): Just to go from a two raw x3p files to a quantitative result of a similarity score between the two bullet LEAs, we need a lot of R packages. When something in those packages change (e.g., default requirements for a tidyverse function), it can break our code! In addition, different people on the project may be running the same script to analyze bullets but be using different versions of packages. Because packages like bulletxtrctr and the methodology for different steps (e.g., groove ID, crosscut ID) are still changing and new approaches are being added, we need to be thoughtful and careful about reporting results and working to make sure our results are reproducible by other team members and outside users. This is not isolated to just the bullet project, either. Let’s consider three major projects at CSAFE and their associated packages: bulletxtrctr and x3ptools for the bullet project, handwriter for the handwriting project, and shoeprintr for the shoe project. A total of 109 different packages are involved!!!!! This is a LOT of packages, and it leaves “open forensic science in R” very vulnerable to changes in other packages. To remedy this, we are developing a package to help manage dependencies and track when functions change. It is ongoing work, and by the next spotlight I should be able to share some use cases, examples, and functionality that will majorly assist our package development workflow. 10.1.1 Computational Reproducibility in Team-based Collaboration In order to investigate just how vulnerable our teams are to differences in code across machines, we can perform a small case study of the CSAFE bullet team and each team member’s package inventories. I asked each member of the bullet project to use the manager tools we have developed to take an inventory of their package versions when they use the bulletxtrctr package (take_inventory(packages = c(\"bulletxtrctr\"))), and send me their inventory. I received: 9 total inventories 6 unique users (one user on 3 machines, one user on 2 machines) We first want to look at all the CSAFE R packages that are used when data analysis is done. The four packages developed by the team at CSAFE interact with one another during the bullet data analysis process: Most of these packages are under active development, which means that one or more people on the team are adding or changing functionality. This means that across the bullet team we can have multiple versions of the packages in question. We can look at the versions of the bullet-related packages that the team has: This only accounts for the four bullet packages we are actively developing; however, there are a lot more packages implicitly involved when we call the bulletxtrctr package. We can take a look at a sampling of those package versions: We can already see a lot of differences in package versions, but we are most interested in where functions and objects actually differ between packages. Just because two users or machines have two different versions of a package doesn’t mean there are any functional differences between the functions or objects employed in a script in question. There is some inconsistency! There are only two pairs of users who have the same exact sets of functions, which means any other two users who run an analysis may end up with different results. However, we have also quickly identified specific functions which differ across the team, and can now focus on identifying whether these functions are used in a script in question, or for a particular analysis. Kiegan Summer 2020 Plans: Work on CSAFE Statistics and Forensics Book Chapter 1, Intro: outline and draft Chapter 2, Analysis of Forensic Evidence: written and edited Chapter 3, The Path to the Witness Stand: written, partially edited Chapter 4, Communicating Evidence in the Courtroom: draft and edit Chapter 5, Conclusions: outline and draft Publication of Research Work Groove ID Methods paper with Nate Garton. Aiming for FSI. editing and submission Bullet scanning variability study Forensic community-facing paper, study design and key findings Statistics community-facing paper, statistical methodology and application Computational Reproducibility Package, manager Documentation and cleaning up, CRAN submission (?) Written paper about tools with case studies Presentation of Research Work Symposium on Data Science and Statistics (SDSS) Personal: Submit final dissertation, find a job "],
["references.html", "References", " References "]
]
