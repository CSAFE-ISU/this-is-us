[
["theoretical-foundations.html", "Chapter 7 Theoretical foundations 7.1 Nate’s Updates 9/5/2019", " Chapter 7 Theoretical foundations 7.1 Nate’s Updates 9/5/2019 Currently in Virginia RA for this semester (year?) is under Danica Central Goals: continue work started by Danica and Peter Vergeer on the analysis of likelihood ratios study the differences between specific source (SS) and common source (CS) likelihood ratios (LRs) in an information theoretic way does the CS or SS LR have more “information”? can be the CS or SS hypotheses (prosecution or defense) be formally compared in terms of being easier to “prove” or “disprove”? Basic Setup \\(H_p\\), \\(H_d\\) are CS prosecution and defense hypotheses upon which we will place priors \\(A\\) and \\(B\\) are discrete r.v.’s representing two “sources” of evidence distributions for \\(A\\) and \\(B\\) defined conditionally based on the hypothesis SS hypothesis is represented by the conditional random variable \\(H_p|A\\) \\(X\\) is data coming from \\(A\\), \\(Y\\) is data coming from \\(B\\) compare information contained in \\((X,Y)\\) about \\(H_p\\) and \\(H_p|A\\) this is what I’m working on now important quantities: Kullback-Leibler divergences, entropy, mutual information "]
]
