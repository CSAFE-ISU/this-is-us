# Theoretical foundations

## Common Source vs Specific Source Comparison via Information Theory

### Introduction
**Central Goals**

- continue work started by Danica and Peter Vergeer on the analysis of likelihood ratios
- study the differences between specific source (SS) and common source (CS) likelihood ratios (LRs) in an information theoretic way
- does the CS or SS LR have more "information"?
- does the data (or the score) have more "information" about the SS or the CS hypothesis?
- can be the CS or SS hypotheses (prosecution or defense) be formally compared in terms of being easier to "prove" or "disprove"?

  
**General Notation**
Let $X \in \mathbb{R}^{q_x}$ and $Y \in \mathbb{R}^{q_y}$ be two random vectors with joint distribution $P$ and corresponding density $p$. 


- \textbf{Entropy}: $\mathbb{H}(X) = -\int{p(x) \log p(x) dx}$
- \textbf{Conditional Entropy}: $\mathbb{H}(X|Y) = \mathbb{E}_{Y}\left[-\int{p(x|y) \log p(x|y) dx}\right]$
- \textbf{Type II Conditional Entropy}: $\mathbb{H}_{2}(X|Y) = -\int{p(x|y) \log p(x|y) dx}$
- \textbf{Mutual Information}: $\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X|Y)$


Proof that Mutual Information is always positive:

\begin{align*}
\mathbb{I}(X;Y) &= \mathbb{H}(X) - \mathbb{H}(X|Y) \\
&= -\int{p(x) \log p(x) dx} + \int{\int{p(x|y)p(y) \log p(x|y) dx} dy} \\
&= -\int{\int{p(x,y) \log p(x) dx}dy} + \int{\int{p(x,y) \log p(x|y) dx} dy} \\
&= -\int{\int{p(x,y) \log p(x) dx}dy} + \int{\int{p(x,y) \log \frac{p(x,y)}{p(y)} dx} dy} \\
&= \int{\int{p(x,y) \log \frac{p(x,y)}{p(x)p(y)} dx}dy} \\
&= KL(P||P_{X} \times P_{Y}) \\
&\geq 0
\end{align*}
  
### Common Source vs Specific Source LR
The "common source" problem is to determine whether two pieces of evidence, both with unknown origin, have the same origin. One might be interested in this problem if two crimes were suspected to be linked, but no suspect has yet been identified. Alternatively, the "specific source" problem is to determine whether a fragment of evidence coming from an unknown source, such as evidence at a crime scene, has the same origin as a fragment of evidence of known origin, such as evidence collected directly from a suspect.

**Basic Setup**

- $H \in \{ H_p, H_d \}$ as the random variable associated with the CS hypothesis. 
- $A$ and $B$ are discrete r.v.'s representing two "sources" of evidence
- distributions for $A$ and $B$ defined conditionally based on the hypothesis
- SS hypothesis is represented by the conditional random variable $H|A$
- $X$ is data coming from $A$, $Y$ is data coming from $B$
- compare information contained in $(X,Y)$ about $H$ and $H|A$
- join density can be written as $p(X,Y,A,B,H) = p(X,Y|A,B)p(B|A,H)p(A|H)p(H)$

**Is there more information in a CS or SS LR?**

Let us examine this question in two different ways. 

1. Is the posterior entropy (given $(X,Y)$) in the common source hypothesis smaller than that of the specific source hypothesis?
     + In other words, would observing the specific value of $A$ as well as the data make you _more_ certain about $H$ than just observing the data?
2. Is the posterior entropy (given $(X,Y)$) in the common source hypothesis smaller than the average (over possible values for $(X,Y,A)$) posterior entropy of the specific source hypothesis?
     + In other words, do you expect that, on average, observing the value of $A$ as well as the data make you _more_ certain about $H$ than just observing the data?

Answering the first question/interpretation, to me, requires proving that 

\[ \mathbb{H}_{2}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A) \geq 0 \].

Answering the second question requires proving that 

\[ \mathbb{H}(H|X,Y) - \mathbb{H}(H|X,Y, A) \geq 0 \].

\noindent Luckily, the second question is true due to the fact that 

\begin{align*}
\mathbb{H}(H|X,Y) - \mathbb{H}(H|X,Y,A) &= \mathbb{E}_{(X,Y)} \left[ - \int{p(h,a|x,y) \log p(h|x,y) d(h,a)} + \int{p(h,a|x,y) \log p(h|x,y,a) d(h,a)} \right] \\
&= - \int{p(h,a|x,y)p(x,y) \log \frac{p(h,a|x,y)}{p(a|x,y)p(h|x,y)} d(h,x,y,a)} \\
&= \mathbb{E}_{(X,Y)} \left[ KL(P_{(H,A)|(X,Y)}||P_{H|(X,Y)} \times P_{A|(X,Y)}) \right] \geq 0
\end{align*}

Whether or not $\mathbb{H}_{2}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A) \geq 0$ is not obvious. We have that 

\begin{align*}
\mathbb{H}_{2}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A) &= \int{-p(h|x,y)\log p(h|x,y) dh} - \int{-p(h|x,y,a) \log p(h|x,y,a) dh} \\
&=  \frac{p(a)}{p(a|x,y)}\int{-p(h|x,y,a)\log p(h|x,y) dh} + \int{p(h|x,y,a) \log p(h|x,y,a) dh}\\ 
&???
\end{align*}

We can try and understand the value of $\mathbb{H}_{2}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A)$ in terms of $\frac{p(a)}{p(a|x,y)}$. For example, if $\frac{p(a)}{p(a|x,y)} \geq 1$, then $\mathbb{H}_{2}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A) \geq 0$. If $\frac{p(a)}{p(a|x,y)} \leq 1$, then it is hard to say much about the value of $\mathbb{H}_{2}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A)$.

**Is there more information in the data about the CS or SS hypothesis?**
Under the second scenario, we can study this question by looking at 

<!-- \begin{align*} -->
<!-- \mathbb{I}(H_{x};\delta) - \mathbb{I}(H;\delta) &= \begin{multlined}[t] \mathbb{H}(H|A) - \mathbb{H}(H|\delta, A) - \\ -->
<!-- \left[ \mathbb{H}(H) - \mathbb{H}(H|\delta) \right] \end{multlined} \\ &= \begin{multlined}[t] \mathbb{H}(H|A) - \mathbb{H}(H) - \\  -->
<!-- \left[ \mathbb{H}(H|\delta, A) - \mathbb{H}(H|\delta) \right] \end{multlined} \\ -->
<!-- &= \mathbb{I}(H;A|\delta) - \mathbb{I}(H;A)  -->
<!-- \end{align*} -->

### Other notions of information
- Information in $Y$ about $X$:
    - $\int{p(x|y) \log \frac{p(x|y)}{p(x)} dx}$
    - nonnegative
    - Equal to zero when $X \perp Y$
    - needn't integrate over $Y$ (?)
    - as opposed to entropy, information in a random variable requires another random variable to be "predicted"... this is fine in our situation as we have a natural candidate: $H_p$ or $H_d$
    
### Information Theoretic Specific Source Score Sufficiency Metric
Consider the specific source problem. The following derivations are very similar to those in the "infinite alternative population" situation considered in the paper that Danica, Alicia, Jarad, and I submitted. Assuming $X \perp Y|A,B$ and both $X \perp B|A$ and $Y \perp A|B$, the LR is 

\begin{align*}
LR &= \frac{p(x,y|A = a,B = a)}{p(x,y|A = a,B \neq a)} \\
&= \frac{p(x|A = a)p(y|A = a, B = a)}{p(x|A = a)p(y|A = a, B \neq a)} \\
&= \frac{p(y|A = a, B = a)}{p(y|A = a, B \neq a)}.
\end{align*}

Thus, the LR depends only on the evidence from the unknown source, $Y$. For a given score, $s$, we can also write the LR in the following way,

\begin{align*}
LR = \frac{p(y|A = a, B = a)}{p(y|A = a, B \neq a)} &= \frac{p(s|y, A = a)p(y|A = a, B = a)}{p(s|y, A = a)p(y|A = a, B \neq a)} \\
&= \frac{p(s|y, A = a, B = a)p(y|A = a, B = a)}{p(s|y, A = a, B \neq a)p(y|A = a, B \neq a)} \\
&= \frac{p(s,y|A = a, B = a)}{p(s,y|A = a, B \neq a)} \\
&= \frac{p(y|s,A = a, B = a)p(s|A = a, B = a)}{p(y|s,A = a, B \neq a)p(s|A = a, B \neq a)}.
\end{align*}

Because $S|Y,A$ is a function only of the known source evidence, $X$, and because $X \perp B|A$, we have that $S \perp B | Y, A$. This means that $p(s|y, A = a, B = a) = p(s|y, A = a, B \neq a)$. 

Using these facts, we can then decompose the KL divergence of the data under the specific source prosecution hypothesis in the following way,

\begin{align*}
KL(P(X,Y|A &= a, B = a)||P(X,Y|A = a, B \neq a)) = E_{(X,Y)}\left[ \log \frac{p(x,y|A = a,B = a)}{p(x,y|A = a,B \neq a)} | A = a, B = a \right] \\
&= E_{Y}\left[ \log \frac{p(y|A = a,B = a)}{p(y|A = a,B \neq a)} | A = a, B = a \right] \\
&= E_{S}\left[ E_{Y}\left[ \log \frac{p(y|A = a,B = a)}{p(y|A = a,B \neq a)} |s, A = a, B = a \right] \right] \\
&= E_{S}\left[ E_{Y}\left[ \log \frac{p(y|s,A = a,B = a)}{p(y|s,A = a,B \neq a)} + \log \frac{p(s|A = a, B = a)}{p(s|A = a, B \neq a)} |s, A = a, B = a \right] \right] \\
&= E_{S}\left[ E_{Y}\left[ \log \frac{p(y|s,A = a,B = a)}{p(y|s,A = a,B \neq a)}|s, A = a, B = a \right] \right] + E_{S} \left[ \log \frac{p(s|A = a, B = a)}{p(s|A = a, B \neq a)} \right] \\
&= E_{S} \left[ KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \neq a)) \right] + KL(P(S|A = a, B = a)||P(S|A = a, B \neq a)).
\end{align*}

This implies that $KL(P(X,Y|A = a, B = a)||P(X,Y|A = a, B \neq a)) \geq KL(P(S|A = a, B = a)||P(S|A = a, B \neq a))$.

An additional consequence is that larger values of $KL(P(S|A = a, B = a)||P(S|A = a, B \neq a))$ imply smaller values of $E_{S} \left[ KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \neq a)) \right]$. Because $KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \neq a))$ is a nonnegative function in terms of $S$, small values of $E_{S} \left[ KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \neq a)) \right]$ imply small values (in some sense) of $KL(P(Y|S,A = a,B = b) || P(Y|S, A = a, B \neq a))$. For example, if the expectation is zero, then the (conditional) KL divergence is zero almost everywhere. Zero KL divergence implies that $P(Y|S,A = a,B = b) = P(Y|S,A = a,B \neq b)$, i.e. $S$ is sufficient for the specific source hypothesis. 

All of this means that $KL(P(S|A = a, B = a)||P(S|A = a, B \neq a))$ and $KL(P(S|A = a, B \neq a)||P(S|A = a, B = a))$ are measures of the usefulness of the score which have direct ties to sufficiency. Estimates of these are always computable in practice, and they are intuitive targets to maximize. For example, if the score is a predicted class probability for "match", the more discriminative the classifier, the more sufficient the score. 

## Score-based Likelihood Ratios are not Fundamentally "Incoherent"
Concern has been raised in the literature on LRs about a desirable property supposedly inherently absent from specific-source SLRs. The property, dubbed "coherence", intuitively says that given two mutually exhaustive hypotheses, $H_A$ and $H_B$, the likelihood ratio used to compare hypothesis A to hypothesis B should be the reciprocal of that used to compare hypothesis B to hypothesis A. I will argue that the claims about the inherent incoherency of SLRs is a result of thinking about SLRs too narrowly. Specifically, I will show that the arguments as to why SLRs are incoherent arise through the inappropriate comparison of SLRs based on different score functions. When one appropriately considers a single score function, incoherency is impossible. 

### Coherence
Denote by $E \in \mathbb{R}^{n}$ the vector of random variables describing _all_ of the observed evidence or data which will be used to evaluate the relative likelihood of the two hypotheses. Define by $LR_{i,j} \equiv \frac{p(E|H_i)}{p(E|H_j)}$ the likelihood ratio of hypothesis $i$ to hypothesis $j$. The coherency principal is satisfied if 

\[ LR_{i,j} = \frac{1}{LR_{j,i}} \].

Likelihood ratios are fundamentally coherent, but what about score-based likelihood ratios? Denote by $s: \mathbb{R}^n \rightarrow \mathbb{R}^{q}$ a score function mapping the original data to Euclidean space of dimension $q$ (typically $q = 1$). Similar to LRs, denote by $SLR_{i,j} \equiv \frac{p(s(E)|H_i)}{p(s(E)|H_j)}$ the score-based likelihood ratio comparing hypothesis $i$ to hypothesis $j$. Clearly, in this general context SLRs are also coherent.

### Problems with arguments showing SLRs are incoherent
Let us examine the arguments presented in [REFS] to the incoherence of SLRs. These arguments stem from an example where there are two known sources of evidence say, source $A$ and source $B$, each producing data $e_A$ and $e_B$, respectively. Furthermore, assume that we have a third piece of evidence of unknown origin, $e_u$, which must have come from either $A$ or $B$. We then wish to evaluate the support of the data for $H_A$ or $H_B$ defined as follows 

$$\begin{array}{cc}
H_A: & e_u \text{ was generated from source } A \\
H_B: & e_u \text{ was generated from source } B.
\end{array}$$

In this case, we have $LR_{A,B} = \frac{p(e_A, e_B, e_u|H_A)}{p(e_A, e_B, e_u|H_B)}$. We make use of _all_ available data in the formulation of the numerator and denominator densities. Under the assumptions that each fragment of evidence is independent under both hypothesis $A$ and $B$ as well as that $p(e_A,e_B|H_A) = p(e_A,e_B|H_B)$, the LR reduces to $LR_{A,B} = \frac{p(e_u|H_A)}{p(e_u|H_B)}$. The second assumption is generally acceptable as the source of $e_u$ ought to have no impact on the distribution of the evidence with known source.

[REFS] then consider possible SLRs for this example. However, they make an assumption that the score is explicitly a function only of two fragments of evidence. That is, assuming the dimension of $e_i$, $dim(e_i) = k$, is constant for $i = A,B,u$, their score maps $s:\mathbb{R}^k \times \mathbb{R}^k \rightarrow \mathbb{R}$. An common example of such a score is Euclidean distance, i.e. $s(x,y) = \left[ \sum_{i = 1}^{k}(x_i - y_i)^2 \right]^{1/2}$. Such a score makes perfect sense in a typical specific-source problem context in which only two fragments of evidence are considered: one from the known source and one from the unknown source. 

However, when one desires to create an SLR based on this score in this particular example, it is tempting to suggest that the natural SLR is $SLR_{A,B} = \frac{p(s(e_A,e_u)|H_A)}{p(s(e_A,e_u)|H_B)}$. Yet, the natural SLR if the hypotheses were reversed is $SLR_{B,A} = \frac{p(s(e_B,e_u)|H_B)}{p(s(e_B,e_u)|H_A)}$. Neither of these SLRs is the reciprocal of the other, and so the specific source SLR appears to be "incoherent". 

This approach, however, should raise a red flag immediately. Why, in the full LR case, do we require that (simplifying model assumptions aside) the numerator and denominator densities be functions of all available data, but the score is not? Furthermore, if we consider these SLRs in the more general context of scores depending on all available data, we see that, in fact, what [REFS] define to be $SLR_{A,B}$ and $SLR_{B,A}$ turn out to be two different SLRs depending on two different scores. 

For clarity, we will use $s(\cdot)$ to denote scores which are explicitly functions of _all_ observed data, and we will use $\delta (\cdot)$ to denote score functions which are only a function of two fragments of evidence/data. Specifically, the score in $SLR_{A,B}$ is $s_1(e_u,e_A,e_B) = \delta(e_u,e_A)$ and the score in $SLR_{B,A}$ is $s_2(e_u,e_A,e_B) = \delta(e_u,e_B)$. While the functional form of the score in the two SLRs _appears_ to be the same, clearly $s_1(e_u,e_A,e_B) \neq s_2(e_u,e_A,e_B)$. Thus, the two SLRs are simply two distinct quantities whose relationship needn't be expected to be related anymore than if one had decided to use two different function forms of $\delta(\cdot,\cdot)$ in the two separate SLRs. 

One might ask how to reasonably construct an SLR which utilizes a (univariate) score other than a similarity metric for two fragments of evidence. One such example in this case would be $s(e_u, e_A, e_B) = \frac{\delta(e_u,e_A)}{\delta(e_u,e_B)}$. Intuitively, under $H_A$, the numerator should be larger than the denominator, while under $H_B$, the opposite should be true. 

### Example of a coherent SLR in the two source problem
Suppose that our hypotheses are defined such that 

\[
\begin{array}{cc}
H_A: & e_u \sim N(\mu_A, \sigma^2), e_A \sim N(\mu_A, \sigma^2), e_B \sim N(\mu_B, \sigma^2) \\
H_B: & e_u \sim N(\mu_B, \sigma^2), e_A \sim N(\mu_A, \sigma^2), e_B \sim N(\mu_B, \sigma^2),
\end{array}
\]

where $e_u$, $e_A$, $e_B$ are mutual independent under both $H_A$ and $H_B$. We will examine three different SLRs: $SLR^{(A)} \equiv \frac{p(s_1(E)|H_A)}{p(s_1(E)|H_B)}$, $SLR^{(B)} \equiv \frac{p(s_2(E)|H_A)}{p(s_2(E)|H_B)}$, and $SLR^* \equiv \frac{p(s_3(E)|H_A)}{p(s_3(E)|H_B)}$, where 

\begin{align*}
E &= (e_u, e_A, e_B)^{\top} \\
s_1(E) &= \log \lVert e_u - e_A \rVert^2 \\
s_2(E) &= \log \lVert e_u - e_B \rVert^2 \\
s_3(E) &= \log \frac{\lVert e_u - e_A \rVert^2}{\lVert e_u - e_B \rVert^2}
\end{align*}

![LR versus SLR scatterplots under hypothesis A and B using three types of SLRs: "coherent", "incoherent" considering hypothesis A first, and "incoherent" considering hypothesis B first.](images/foundations/nate/incoherent_coherent_slr.png)

| RMSE| Exp.Cond.KL| score.KL| true.KL|type         |hypothesis |
|----:|-----------:|--------:|-------:|:------------|:----------|
| 3.79|        2.71|     1.76|    4.47|coherent     |A          |
| 3.86|        2.77|     1.74|    4.51|coherent     |B          |
| 4.64|        3.37|     1.10|    4.47|incoherent A |A          |
| 3.94|        3.22|     1.29|    4.51|incoherent A |B          |
| 3.93|        3.23|     1.24|    4.47|incoherent B |A          |
| 4.73|        3.44|     1.07|    4.51|incoherent B |B          |

### Possible Generalizations of Coherent SLRs to the Multisource Case
It might be nice to, in general, be able to construct a reasonable score given a "similarity" score, $\delta(\cdot, \cdot)$ defined in terms of two pieces of evidence. I'll propose a couple ways of doing this. First, suppose that instead of two sources, we now have $K$ sources, one of which is the source of the evidence from an unknown source. The task is to compare the hypothesis that the unknown source evidence was generated by a specific source $A = a_x \in \mathcal{S} \equiv \{1, 2, ..., K\}$ to the hypothesis that the unknown source evidence was generated by any one of the other sources $B = b \in \mathcal{S} \setminus a_x$. Mathematically,  

\[
\begin{array}{cc}
H_A: & e_u \text{ generated by } a_x \\
H_B: & e_u \text{ generated by some } b \in \mathcal{S} \setminus a_x.
\end{array}
\]

Let's consider two possible scores, both of which will be based off of an accepted dissimilarity metric, $\delta(\cdot, \cdot) \geq 0$. The first score that we will consider is 

\[ S_1(e_u, e_1, ..., e_K) = \log \frac{\delta(e_u, e_{a_x})}{ \min_{b \in \mathcal{S} \setminus a_x} \delta(e_u, e_b) }.
\]

The second score that we will consider is 

\[ S_2(e_u, e_1, ..., e_K) = \log \frac{\delta(e_u, e_{a_x})}{ \sum_{b \in \mathcal{S} \setminus a_x} w(b)\delta(e_u, e_b) },
\]

where $w(b)$ are weights with $\sum_{b \in \mathcal{S} \setminus a_x} w(b) = 1$. Intuitively, the first score should perform well. The dissimilarity in the numerator should be compared with the smallest dissimilarity in $ \mathcal{S} \setminus a_x$. In the absence of other prior information, only the relative size of the numerator dissimilarity to the smallest dissimilarity of $b \in \mathcal{S} \setminus a_x$ should matter. 

The second score would likely be easier to study in terms of mathematical properties. For example, it might be possible to assume $E \left[ \delta(e_u, e_i) \right] = \mu_1 < \infty$ if the source of $e_u$ is that of $e_i$ but that $E \left[ \delta(e_u, e_i) \right] = \mu_2 < \infty$ if the sources are different. One might be able to show some type of consistency property if, instead of one copy of $E = (e_u, e_1, ..., e_K)^{\top}$, we now have $N$ iid copies $E_i = (e_u, e_1, ..., e_K)^{\top}_i$. Then, using $\frac{1}{N} \sum_{i = 1}^{N} \delta(e_{u_i}, e_{j_i})$ in place of $\delta(e_u, e_j)$ yields the ability to use the law of large numbers. This may be impractical in any real life situation, but I consider the score here nonetheless.

In more generality, there seems to be no reason why a multisource score couldn't be constructed using an arbitrary summary statistic of the "similarity" scores computed between the unknown source evidence and the alternative population.

### Multisource example
For simplicity, we will again assume that all evidence is generated from independent, univariate Gaussian distributions. Specifically, 

\[
\begin{array}{ccc}
H_p: & e_u \sim N(\mu_K, \sigma^2), & e_i \sim N(\mu_i, \sigma^2), i \in \{ 1,..., K \} \\
H_d: & e_u \sim GMM(\{\mu_k\}_{k = 1}^{K - 1}, \{ \pi_k \}_{k = 1}^{K - 1}, \sigma^2),  & e_i \sim N(\mu_i, \sigma^2), i \in \{ 1,...,K \}
\end{array}.
\]

where all random variables are assumed to be independent conditional on each hypothesis. We will further assume that $\mu_i \stackrel{iid}{\sim} N(0, \tau^2), i \in \{ 1,..., K-1 \}$.

![log-LR versus log-SLR scatterplots under hypothesis P and D using three types of SLRs which correspond to using different statistics to aggregate dissimilarity scores in the alternative source population. We try min, average, and max, corresponding to rows 1-3, respectively.](images/foundations/nate/multisource_slr_vs_lr.png)

|   RMSE| Exp.Cond.KL|     KL| true.KL|type |hypothesis |
|------:|-----------:|------:|-------:|:----|:----------|
| 3.1371|      2.1718| 2.2813|  4.4527|min  |P          |
| 8.4669|      5.1396| 1.8487|  6.9865|min  |D          |
| 3.4835|      2.3965| 2.0568|  4.4527|avg  |P          |
| 7.6549|      4.2336| 2.7529|  6.9865|avg  |D          |
| 3.8587|      2.7198| 1.7333|  4.4527|max  |P          |
| 7.0711|      4.1623| 2.8250|  6.9865|max  |D          |

### Other Possible Viewpoints?
I have assumed in the previous section that the order of consideration of hypotheses should not affect the ordering of the data vector $E = (e_u,e_A,e_B)$ or of the ordering of these arguments to the score function. This seems reasonable, but perhaps [REFS] would argue that considering $H_A$ first, $E = (e_u, e_A, e_B)$ and $s(E) = s(e_u, e_A, e_B)$, but considering $H_B$ first, $E = (e_u, e_B, e_A)$ and $s(E) = s(e_u, e_B, e_A)$. In this case, $SLR_{A,B} \neq \frac{1}{SLR_{B,A}}$ because we switch the order of arguments to the score from one SLR to the other. Note that, however, if we relax the independence assumptions of independence under either $H_A$ or $H_B$, then even the LR becomes "incoherent" because $\frac{p(e_u, e_A, e_B|H_A)}{p(e_u, e_A, e_B|H_B)} \neq \frac{p(e_u, e_B, e_A|H_A)}{p(e_u, e_B, e_A|H_B)}$ in general.

It is true that the LR depends only on the evidence of the unknown source _in this specific scenario_, but that is a consequence of modeling assumptions and not of LR paradigmatic principals. 

## Optimal matching problem

### Two groups case.

Suppose there are two groups $\pi_{1}$ and $\pi_{2}$ with densities $f_{1}(x)$ and $f_{2}(x)$ on the support $x \in T$. 
Let $p_{1}$ and $p_{2}$ be the prior probabilities of groups $\pi_{1}$ and $\pi_{2}$, respectively.
There are new observations $\mbox{obs}_{1}$ and $\mbox{obs}_{2}$ with measurements $x_{1}$ and $x_{2}$, respectively. The goal is to distinguish there the new observations are from the same group or not. That is to partition the space $T \times T$ in to $T_{m} \cup T_{nm}$, where we conclude $\mbox{obs}_{1}$ and $\mbox{obs}_{2}$ are from the same group if $(x_{1}, x_{2})$ falls into $T_{m}$; and otherwise if $(x_{1}, x_{2}) \in T_{nm}$.
The two type errors:

- Matching error: $(x_{1}, x_{2}) \in T_{m}$ if $\mbox{obs}_{1} \in \pi_{1}, \mbox{obs}_{2} \in \pi_{2}$ or $\mbox{obs}_{1} \in \pi_{2}, \mbox{obs}_{2} \in \pi_{1}$;
- Unmatching error: $(x_{1}, x_{2}) \in T_{um}$ if $\mbox{obs}_{1}, \mbox{obs}_{2} \in \pi_{1}$ or $\mbox{obs}_{1}, \mbox{obs}_{2} \in \pi_{2}$.

The probability of errors are:
$$P(\mbox{Matching error}) = \int_{T_{m}} \{f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})\}p_{1}p_{2}dx_{1}dx_{2},$$
$$P(\mbox{Unmatching error}) = \int_{T_{um}} \{f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2}\}dx_{1}dx_{2}.$$
Consider the unweighted sum of those two error probabilities $P(\mbox{error}) = P(\mbox{Matching error}) + P(\mbox{Unmatching error})$. We have
$$P(\mbox{error}) = \int_{T_{m}} \big[\{f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})\}p_{1}p_{2} - \{f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2}\}\big]dx_{1}dx_{2} + C,$$
where $C$ is a constant.

The minimum of this error probability with respect to $T_{m}$ occurs when 
\begin{equation}
T_{m} = \bigg\{(x_{1}, x_{2}): \frac{[f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})]p_{1}p_{2}}{f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2}} < 1 \bigg\}.
\label{eq:Optimalrule1}
\end{equation}
This decision region is the ***optimal*** matching rule to minimize the probability of the matching errors.
Note that 
$$f_{1}(x_{1})f_{1}(x_{2})p_{1}^{2} + f_{2}(x_{1})f_{2}(x_{2})p_{2}^{2} - [f_{1}(x_{1})f_{2}(x_{2}) + f_{2}(x_{1})f_{1}(x_{2})]p_{1}p_{2} = 
\{f_{1}(x_{1})p_{1} - f_{2}(x_{1})p_{2}\}\{f_{1}(x_{2})p_{1} - f_{2}(x_{2})p_{2}\}.$$
The optimal region $T_{m}$ in (\ref{eq:Optimalrule1}) is equivalent to 
\begin{eqnarray}
\frac{f_{1}(x_{1})}{f_{2}(x_{1})} < \frac{p_{2}}{p_{1}} 
&\mbox{and}&
\frac{f_{1}(x_{2})}{f_{2}(x_{2})} < \frac{p_{2}}{p_{1}} \ \mbox{ or } \nonumber \\
\frac{f_{1}(x_{1})}{f_{2}(x_{1})} > \frac{p_{2}}{p_{1}} 
&\mbox{and}&
\frac{f_{1}(x_{2})}{f_{2}(x_{2})} > \frac{p_{2}}{p_{1}}, 
\label{eq:Optimalrule2}
\end{eqnarray}
which corresponds to the optimal classification rule. 
From (\ref{eq:Optimalrule2}), the optimal matching rule is equivalent to the optimal classification rule as long as we conclude the observations matched from one group if they are classified to the same group.

**Normal distribution.** As an example, assume $\pi_{1}$ and $\pi_{2}$ are from normal distributions with mean $\mu_{1}$ and $\mu_{2}$, and covariance $\Sigma$. 
Further assume the prioir probabilities are the same $p_{1} = p_{2} = 1 / 2$.
The optimal decision is to classify $x_{1}$ and $x_{2}$ into the same group if 
\begin{equation}
\frac{\exp\big[ \{x_{2} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \big] + \exp\big[ \{x_{1} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \big]} {1 + \exp\big[ \{x_{1} + x_{2} - (\mu_{1} + \mu_{2})\}' \Sigma^{-1} (\mu_{2} - \mu_{1})  \big]} < 1.
\label{eq:OptimalruleNormal1}
\end{equation}
It can be shown that the above inequality is equivalent to 
\begin{eqnarray}
\exp\big[ \{x_{2} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \big] < 1 
&\mbox{and}&
\exp\big[ \{x_{1} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \big] < 1 \ \mbox{ or } \nonumber \\
\exp\big[ \{x_{2} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \big] > 1 
&\mbox{and}&
\exp\big[ \{x_{1} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \big] > 1
\label{eq:OptimalruleNormal2}
\end{eqnarray}
For discriminat analysis, it is well known that the optimal classification rule under normal distribution is to classify $x_{1}$ and $x_{2}$ to $\pi_{1}$ if $\{x_{1} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \leq 0$ and $\{x_{2} - (\mu_{1} + \mu_{2}) / 2\} \Sigma^{-1} (\mu_{2} - \mu_{1}) \leq 0$ respectively, and classify them to $\pi_{2}$ if otherwise. 

**Feature difference** is a method to solve the matching problem via classification. Take $d = x_{1} - x_{2}$ as the pairwise difference between two observations. It is clear that $d \sim N(0, 2 \Sigma)$ if $x_{1}$ and $x_{2}$ are both from either $\pi_{1}$ or $\pi_{2}$, and $d \sim N(\mu_{1} - \mu_{2}, 2 \Sigma)$ or $d \sim N(\mu_{2} - \mu_{1}, 2 \Sigma)$ if $x_{1}$ and $x_{2}$ are from different groups. 
Let $f_{m}(d)$ and $f_{um}(d)$ be the density of $d$ if two observations are from the same group and different groups, respectively.
Then, $f_{m}(d)$ is the normal density with mean $0$ and covariance $2 \Sigma$, and $f_{um}(d)$ is the mixture normal $0.5 N(\mu_{1} - \mu_{2}, 2 \Sigma) + 0.5 N(\mu_{2} - \mu_{1}, 2 \Sigma)$. The optimal discriminant rule is to classify $d$ into the unmatching case if 
$$\frac{f_{um}(d)}{f_{m}(d)} = \frac{\exp\{-(d - \mu_{1} + \mu_{2})' \Sigma^{-1} (d - \mu_{1} + \mu_{2}) / 4\} + \exp\{-(d + \mu_{1} - \mu_{2})' \Sigma^{-1} (d + \mu_{1} - \mu_{2}) / 4\}}{2 \exp(-d' \Sigma^{-1} d / 4)} > \frac{p_{1}^{2} + p_{2}^{2}}{2 p_{1} p_{2}}.$$
Let $\mu_{d} = \mu_{1} - \mu_{2}$. The above inequality is equivalent to 
$$\exp(d' \Sigma^{-1} \mu_{d} / 2) + \exp( - d' \Sigma^{-1} \mu_{d} / 2) > \exp(\mu_{d}' \Sigma^{-1} \mu_{d} / 4) \frac{p_{1}^{2} + p_{2}^{2}}{p_{1}p_{2}},$$
which is approximately equivalent to 
$$|d' \Sigma^{-1} \mu_{d}| / 2 > \mu_{d}' \Sigma^{-1} \mu_{d} / 4 + \log (p_{1}^{2} + p_{2}^{2}) - \log(p_{1}p_{2}).$$

As an illustration, consider one dimensional feature space. Take $\mu_{1} = 1, \mu_{2} = -1, \Sigma = 1$, and $p_{1} = p_{2} = 0.5$. Figure 1 shows the optimal matching rule and the optimal rule based on feature difference. 
We see that in this example the matching region from the feature difference method only overlaps a small fraction of that from the optimal matching rule, and there is a missing alignment for the feature difference method in the two small triangles at the origin.
We also note that even though most of the pink area and the blue area in Figure 1 don't overlap, the probabilities that the pair of data $(x_{1}, x_{2})$ falling into those non-overlapping regions could be small, especially if the absolute value of either coordinate is large. See the contours of multivariate normal distribution in Figure 1.

```{r, echo = FALSE, message = FALSE, fig.height = 4, fig.align = 'center', fig.cap = 'Matching regions from the optimal rule (in pink) and the method based on feature difference (in blue). The contours of multivariate normal distribution with means $(1, -1)$ (unmatching case) and $(1, 1)$ (matching case) are marked in black and red, respectively, where the covariance is identity.'}
library(tidyverse)
library(mvtnorm)
data1 = data.frame(x = seq(-5, 5, 0.1), y = seq(-5, 5, 0.1))
data2 = data.frame(x = seq(-5, 0, 0.1), ymin = rep(-5, 51), ymax = rep(0, 51))
data3 = data.frame(x = seq(0, 5, 0.1), ymin = rep(0, 51), ymax = rep(5, 51))
data4 = data.frame(x = seq(-5, 5, 0.1), ymin = pmax(seq(-5, 5, 0.1) - 1 - log(2), -5), ymax = pmin(seq(-5, 5, 0.1) + 1 + log(2), 5))
mu1 = c(-1, 1); mu2 = c(1, 1); sigma = diag(c(1, 1))
data.grid1 = expand.grid(s.1 = seq(-3.1, 1.1, length.out=200), s.2 = seq(-1.1, 3.1, length.out=200))
data.grid2 = expand.grid(s.1 = seq(-1.1, 3.1, length.out=200), s.2 = seq(-1.1, 3.1, length.out=200))
q.samp1 = cbind(data.grid1, prob = mvtnorm::dmvnorm(data.grid1, mean = mu1, sigma = sigma))
q.samp2 = cbind(data.grid2, prob = mvtnorm::dmvnorm(data.grid2, mean = mu2, sigma = sigma))

data1 %>% ggplot(aes(x = x, y = y)) + xlim(-5, 5) + ylim(-5, 5) + 
  geom_ribbon(data = data2, aes(x = x, ymin = ymin, ymax = ymax), inherit.aes = FALSE, fill="#BB000033") + 
  geom_ribbon(data = data3, aes(x = x, ymin = ymin, ymax = ymax), inherit.aes = FALSE, fill="#BB000033") + 
  geom_ribbon(data = data4, aes(x = x, ymin = ymin, ymax = ymax), inherit.aes = FALSE, fill="#1A13DFD6") + 
  geom_contour(data = q.samp1, aes(x=s.1, y=s.2, z=prob), color = 'black', alpha = 0.5) + 
  geom_contour(data = q.samp2, aes(x=s.1, y=s.2, z=prob), color = 'red', alpha = 0.5) + 
  xlab("") + ylab("") + coord_fixed(ratio = 1)
```

```{r, echo = FALSE, message = FALSE, eval = FALSE}
library(randomForest)
library(MASS)
B = 100
res = matrix(0, B, 2)

for (rep in 1 : B){
  n.train = 50
  n.test = 10
  X1 = rnorm(n.train, 1, 1)
  X2 = rnorm(n.train, -1, 1)
  X1.test = rnorm(n.test, 1, 1)
  X2.test = rnorm(n.test, -1, 1)
  X = as.matrix(c(X1, X2))
  X.test = as.matrix(c(X1.test, X2.test))
  Y = as.factor(c(rep(1, n.train), rep(0, n.train)))
  Y.test = as.factor(c(rep(1, n.test), rep(0, n.test)))
  fit.lda = lda(X, Y)
  predict.lda = predict(fit.lda, X.test)$class

  m = dim(X.test)[1]
  matching.test = c()
  matching.lda = c()
  diff.test = c()
  k = 1
  for (i in 1 : m){
    for (j in i : m){
      diff.test[k] = X.test[i, 1] - X.test[j, 1]
      if (Y.test[i] == Y.test[j]) matching.test[k] = 1
      if (Y.test[i] != Y.test[j]) matching.test[k] = 0
      if (predict.lda[i] == predict.lda[j]) matching.lda[k] = 1
      if (predict.lda[i] != predict.lda[j]) matching.lda[k] = 0
      k = k + 1
    }
  }
  #mean(matching.lda == matching.test)

  matching.train = c()
  diff = c()
  k = 1
  for (i in 1 : dim(X)[1]){
    for (j in i : dim(X)[1]){
      diff[k] = X[i, 1] - X[j, 1]
      if (Y[i] == Y[j]) matching.train[k] = 1
      if (Y[i] != Y[j]) matching.train[k] = 0
      k = k + 1
    }
  }

  predict.rf = randomForest(x = as.matrix(diff), y = as.factor(matching.train), xtest = as.matrix(diff.test))
  res[rep, ] = c(mean(matching.lda == matching.test), mean(predict.rf$test$predicted == as.factor(matching.test)))
}
data.res = data.frame(optimal = res[, 1], difference.rf = res[, 2])
#write.csv(data.res, "simulation.csv")
```

**Comparison with random forest.** We also conducted a small scale simulation to compare the optimal matching rule with the random forest method applied on the feature difference. The traning data include 50 observations from $N(1, 1)$ and $N(-1, 1)$. There are additional 10 observations from each of the group serving as the testing data. We evaluate the percentage of the matching errors on the pairs of the testing data. We repeated the whole simulation 100 times. The accuracy rates are ***0.762*** and ***0.708*** for the optimal matching rule and the random forest applied on the differences of the measurements, respectively. 65\% out of the 100 repetitions, the former method has higher accuracy than the latter method.

```{r, echo = FALSE, message = FALSE, fig.width = 3}
data.res = read.csv("simulation.csv")
#colMeans(data.res[, -1])
#mean(data.res[, 2] >= data.res[, 3])
data.res %>% ggplot(aes(x = difference.rf, y = optimal)) + geom_point() + 
  geom_abline(intercept = 0, slope = 1, color="red", linetype="dashed") +
  xlab("Random forest classification") + ylab("Optimal matching rule") + coord_fixed(ratio = 1) +
  theme(panel.background = element_rect(fill = NA), 
        panel.grid.major = element_line(colour = "grey95"),
        axis.title.x = element_text(size = 8), axis.title.y = element_text(size = 8))
data.res1 = data.frame(prop = c(data.res[, 2], data.res[, 3]), method = c(rep("Optimal matching rule", dim(data.res)[1]), rep("Random forest", dim(data.res)[1])))
data.res1 %>% ggplot(aes(y = prop, x = method)) + geom_boxplot() + 
  xlab("") + ylab("Proportion of accuracy") + 
  theme(panel.background = element_rect(fill = NA), 
        panel.grid.major = element_line(colour = "grey95"),
        axis.text = element_text(size = 8), axis.title.y = element_text(size = 8))
```

### Topics needs exploration

- How to quantify the matching error rates when the training data only inlcude a small part of many potential groups?
- How training errors change as more and more features are collected (dimension $p$ increases), where only a small fraction of those features carry useful signals (feature selection).