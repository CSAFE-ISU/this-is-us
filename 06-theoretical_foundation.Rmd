# Theoretical foundations

## Common Source vs Specific Source Comparison via Information Theory

### Introduction
**Central Goals**

- continue work started by Danica and Peter Vergeer on the analysis of likelihood ratios
- study the differences between specific source (SS) and common source (CS) likelihood ratios (LRs) in an information theoretic way
- does the CS or SS LR have more "information"?
- can be the CS or SS hypotheses (prosecution or defense) be formally compared in terms of being easier to "prove" or "disprove"?

  
**General Notation**
Let $X \in \mathbb{R}^{q_x}$ and $Y \in \mathbb{R}^{q_y}$ be two random vectors with joint distribution $P$ and corresponding density $p$. 


- \textbf{Entropy}: $\mathbb{H}(X) = -\int{p(x) \log p(x) dx}$
- \textbf{Conditional Entropy}: $\mathbb{H}(X|Y) = \mathbb{E}_{Y}\left[-\int{p(x|y) \log p(x|y) dx}\right]$
- \textbf{Type II Conditional Entropy}: $\mathbb{H}_{2}(X|Y) = -\int{p(x|y) \log p(x|y) dx}$
- \textbf{Mutual Information}: $\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X|Y)$


Proof that Mutual Information is always positive:

\begin{align*}
\mathbb{I}(X;Y) &= \mathbb{H}(X) - \mathbb{H}(X|Y) \\
&= -\int{p(x) \log p(x) dx} + \int{\int{p(x|y)p(y) \log p(x|y) dx} dy} \\
&= -\int{\int{p(x,y) \log p(x) dx}dy} + \int{\int{p(x,y) \log p(x|y) dx} dy} \\
&= -\int{\int{p(x,y) \log p(x) dx}dy} + \int{\int{p(x,y) \log \frac{p(x,y)}{p(y)} dx} dy} \\
&= \int{\int{p(x,y) \log \frac{p(x,y)}{p(x)p(y)} dx}dy} \\
&= KL(P||P_{X} \times P_{Y}) \\
&\geq 0
\end{align*}
  
### Common Source vs Specific Source LR

**Basic Setup**

- $H \in \{ H_p, H_d \}$ as the random variable associated with the CS hypothesis. 
- $A$ and $B$ are discrete r.v.'s representing two "sources" of evidence
- distributions for $A$ and $B$ defined conditionally based on the hypothesis
- SS hypothesis is represented by the conditional random variable $H_p|A$
- $X$ is data coming from $A$, $Y$ is data coming from $B$
- compare information contained in $(X,Y)$ about $H_p$ and $H_p|A$
- join density can be written as $p(X,Y,A,B,H) = p(X,Y|A,B)p(B|A,H)p(A|H)p(H)$

**Is there more information in a CS or SS LR?**

Let us examine this question in two different ways. 

1. Is the posterior entropy (given $(X,Y)$) in the common source hypothesis smaller than that of the specific source hypothesis?
     + In other words, would observing the specific value of $A$ as well as the data make you _more_ certain about $H$ than just observing the data?
2. Is the posterior entropy (given $(X,Y)$) in the common source hypothesis smaller than the average (over possible values for $A$) posterior entropy of the specific source hypothesis?
     + In other words, do you expect that, on average, observing the value of $A$ as well as the data make you _more_ certain about $H$ than just observing the data?

Answering the first question/interpretation, to me, requires proving that 

\[ \mathbb{H}(H|X,Y) - \mathbb{H}_{2}(H|X,Y, A) \geq 0 \].

Answering the second question requires proving that 

\[ \mathbb{H}(H|X,Y) - \mathbb{H}(H|X,Y, A) \geq 0 \].

\noindent Luckily, the second question is true due to the fact that 

\begin{align*}
\mathbb{H}(H|X,Y) - \mathbb{H}(H|X,Y,A) &= \mathbb{E}_{(X,Y)} \left[ - \int{p(h,a|x,y) \log p(h|x,y) d(h,a)} + \int{p(h,a|x,y) \log p(h|x,y,a) d(h,a)} \right] \\
&= - \int{p(h,a|x,y)p(x,y) \log \frac{p(h,a|x,y)}{p(a|x,y)p(h|x,y)} d(h,x,y,a)} \\
&= \mathbb{E}_{(X,Y)} \left[ KL(P_{(H,A)|(X,Y)}||P_{H|(X,Y)} \times P_{A|(X,Y)}) \right] \geq 0
\end{align*}



        